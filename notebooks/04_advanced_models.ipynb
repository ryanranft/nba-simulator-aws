{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Game Prediction - Advanced Models\n",
    "\n",
    "**Purpose:** Train advanced ML models for improved prediction accuracy\n",
    "\n",
    "**Models:** XGBoost, LightGBM\n",
    "\n",
    "**Goal:** Achieve accuracy > 65% and AUC > 0.70\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Advanced models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"âœ“ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'nba-sim-raw-data-lake'\n",
    "S3_PREFIX = 'ml-features'\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_df = pd.read_parquet(f's3://{S3_BUCKET}/{S3_PREFIX}/train.parquet')\n",
    "test_df = pd.read_parquet(f's3://{S3_BUCKET}/{S3_PREFIX}/test.parquet')\n",
    "\n",
    "# Prepare features\n",
    "id_cols = ['game_id', 'game_date', 'season', 'home_team_id', 'away_team_id']\n",
    "target_col = 'home_win'\n",
    "feature_cols = [col for col in train_df.columns if col not in id_cols + [target_col]]\n",
    "\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df[target_col]\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df[target_col]\n",
    "\n",
    "print(f\"âœ“ Data loaded: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost...\\n\")\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=50\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ XGBoost trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_xgb = xgb_model.predict(X_train)\n",
    "y_test_pred_xgb = xgb_model.predict(X_test)\n",
    "y_test_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "train_acc_xgb = accuracy_score(y_train, y_train_pred_xgb)\n",
    "test_acc_xgb = accuracy_score(y_test, y_test_pred_xgb)\n",
    "test_auc_xgb = roc_auc_score(y_test, y_test_proba_xgb)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"XGBOOST PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train Accuracy: {train_acc_xgb:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc_xgb:.4f}\")\n",
    "print(f\"Test AUC:       {test_auc_xgb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_xgb, target_names=['Away Win', 'Home Win']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LightGBM...\\n\")\n",
    "\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    callbacks=[lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "print(\"\\nâœ“ LightGBM trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_train_pred_lgb = lgb_model.predict(X_train)\n",
    "y_test_pred_lgb = lgb_model.predict(X_test)\n",
    "y_test_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "train_acc_lgb = accuracy_score(y_train, y_train_pred_lgb)\n",
    "test_acc_lgb = accuracy_score(y_test, y_test_pred_lgb)\n",
    "test_auc_lgb = roc_auc_score(y_test, y_test_proba_lgb)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LIGHTGBM PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train Accuracy: {train_acc_lgb:.4f}\")\n",
    "print(f\"Test Accuracy:  {test_acc_lgb:.4f}\")\n",
    "print(f\"Test AUC:       {test_auc_lgb:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred_lgb, target_names=['Away Win', 'Home Win']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    'Model': ['XGBoost', 'LightGBM'],\n",
    "    'Train Accuracy': [train_acc_xgb, train_acc_lgb],\n",
    "    'Test Accuracy': [test_acc_xgb, test_acc_lgb],\n",
    "    'Test AUC': [test_auc_xgb, test_auc_lgb]\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"ADVANCED MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "best_idx = comparison['Test AUC'].idxmax()\n",
    "best_model = comparison.loc[best_idx, 'Model']\n",
    "best_auc = comparison.loc[best_idx, 'Test AUC']\n",
    "\n",
    "print(f\"\\nâœ“ Best Model: {best_model} (AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost feature importance\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# LightGBM feature importance\n",
    "lgb_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "axes[0].barh(range(10), xgb_importance.head(10)['importance'])\n",
    "axes[0].set_yticks(range(10))\n",
    "axes[0].set_yticklabels(xgb_importance.head(10)['feature'])\n",
    "axes[0].set_xlabel('Importance')\n",
    "axes[0].set_title('XGBoost - Top 10 Features')\n",
    "\n",
    "axes[1].barh(range(10), lgb_importance.head(10)['importance'])\n",
    "axes[1].set_yticks(range(10))\n",
    "axes[1].set_yticklabels(lgb_importance.head(10)['feature'])\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('LightGBM - Top 10 Features')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "model_prefix = 'ml-models/advanced'\n",
    "\n",
    "print(\"Saving models to S3...\")\n",
    "\n",
    "# XGBoost\n",
    "xgb_buffer = BytesIO()\n",
    "pickle.dump(xgb_model, xgb_buffer)\n",
    "xgb_buffer.seek(0)\n",
    "s3.put_object(Bucket=S3_BUCKET, Key=f'{model_prefix}/xgboost.pkl', Body=xgb_buffer.getvalue())\n",
    "print(\"  âœ“ XGBoost saved\")\n",
    "\n",
    "# LightGBM\n",
    "lgb_buffer = BytesIO()\n",
    "pickle.dump(lgb_model, lgb_buffer)\n",
    "lgb_buffer.seek(0)\n",
    "s3.put_object(Bucket=S3_BUCKET, Key=f'{model_prefix}/lightgbm.pkl', Body=lgb_buffer.getvalue())\n",
    "print(\"  âœ“ LightGBM saved\")\n",
    "\n",
    "print(f\"\\nâœ“ Models saved to s3://{S3_BUCKET}/{model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ADVANCED MODELS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nðŸ“Š Models Trained:\")\n",
    "print(f\"  1. XGBoost (200 estimators, max_depth=6)\")\n",
    "print(f\"  2. LightGBM (200 estimators, 31 leaves)\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Performance:\")\n",
    "print(f\"  XGBoost:  {test_acc_xgb:.1%} accuracy, {test_auc_xgb:.3f} AUC\")\n",
    "print(f\"  LightGBM: {test_acc_lgb:.1%} accuracy, {test_auc_lgb:.3f} AUC\")\n",
    "\n",
    "print(f\"\\nðŸ† Best Model: {best_model}\")\n",
    "\n",
    "goal_met = comparison['Test Accuracy'].max() > 0.65\n",
    "print(f\"\\n{'âœ“' if goal_met else 'âš ï¸ '} Goal {'achieved' if goal_met else 'not met'}: Accuracy > 65%\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(f\"  1. Hyperparameter tuning for optimal performance\")\n",
    "print(f\"  2. Ensemble methods (combine multiple models)\")\n",
    "print(f\"  3. Review 05_model_evaluation.ipynb for comprehensive comparison\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
