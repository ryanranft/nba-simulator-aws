{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Game Prediction - Baseline Model\n",
    "\n",
    "**Purpose:** Build baseline prediction models for NBA game outcomes\n",
    "\n",
    "**Data Source:** S3 (features from 02_feature_engineering.ipynb)\n",
    "\n",
    "**Output:** Trained baseline models and performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook trains simple baseline models:\n",
    "1. **Logistic Regression** (linear baseline)\n",
    "2. **Random Forest** (tree-based baseline)\n",
    "3. **Model evaluation** (accuracy, AUC, feature importance)\n",
    "\n",
    "**Target:** Home team win probability (binary classification)\n",
    "\n",
    "**Goal:** Establish baseline performance (accuracy > 60%)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Libraries imported\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Features from S3\n",
    "\n",
    "Load the engineered features created in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 paths\n",
    "S3_BUCKET = 'nba-sim-raw-data-lake'\n",
    "S3_PREFIX = 'ml-features'\n",
    "\n",
    "train_path = f's3://{S3_BUCKET}/{S3_PREFIX}/train.parquet'\n",
    "test_path = f's3://{S3_BUCKET}/{S3_PREFIX}/test.parquet'\n",
    "\n",
    "print(\"Loading training data...\")\n",
    "train_df = pd.read_parquet(train_path)\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_df = pd.read_parquet(test_path)\n",
    "\n",
    "print(f\"\\n‚úì Data loaded\")\n",
    "print(f\"  Train set: {len(train_df):,} rows\")\n",
    "print(f\"  Test set:  {len(test_df):,} rows\")\n",
    "print(f\"  Features:  {len(train_df.columns)} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine data\n",
    "print(\"Train data sample:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)\n",
    "\n",
    "print(\"\\nTarget distribution (train):\")\n",
    "print(train_df['home_win'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features\n",
    "\n",
    "Select features for modeling and prepare train/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (exclude identifiers and target)\n",
    "id_cols = ['game_id', 'game_date', 'season', 'home_team_id', 'away_team_id']\n",
    "target_col = 'home_win'\n",
    "\n",
    "feature_cols = [col for col in train_df.columns \n",
    "                if col not in id_cols + [target_col]]\n",
    "\n",
    "print(f\"Feature columns ({len(feature_cols)}):\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"  {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare train/test splits\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_col].copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_col].copy()\n",
    "\n",
    "print(f\"‚úì Data prepared\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_test:  {X_test.shape}\")\n",
    "print(f\"  y_test:  {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(\"Train:\", X_train.isnull().sum().sum())\n",
    "print(\"Test:\", X_test.isnull().sum().sum())\n",
    "\n",
    "if X_train.isnull().sum().sum() > 0:\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: Missing values detected\")\n",
    "    print(X_train.isnull().sum()[X_train.isnull().sum() > 0])\n",
    "else:\n",
    "    print(\"‚úì No missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling\n",
    "\n",
    "Standardize features for logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for consistency\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    "\n",
    "print(\"‚úì Features scaled\")\n",
    "print(f\"  Mean (train): {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"  Std (train):  {X_train_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Baseline Model #1: Logistic Regression\n",
    "\n",
    "Simple linear model for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"‚úì Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_scaled)\n",
    "y_test_pred_lr = lr_model.predict(X_test_scaled)\n",
    "\n",
    "y_train_proba_lr = lr_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba_lr = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "print(\"‚úì Predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate logistic regression\n",
    "print(\"=\" * 70)\n",
    "print(\"LOGISTIC REGRESSION PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train metrics\n",
    "train_acc_lr = accuracy_score(y_train, y_train_pred_lr)\n",
    "train_auc_lr = roc_auc_score(y_train, y_train_proba_lr)\n",
    "\n",
    "print(\"\\nTrain Set:\")\n",
    "print(f\"  Accuracy: {train_acc_lr:.4f}\")\n",
    "print(f\"  AUC-ROC:  {train_auc_lr:.4f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_acc_lr = accuracy_score(y_test, y_test_pred_lr)\n",
    "test_precision_lr = precision_score(y_test, y_test_pred_lr)\n",
    "test_recall_lr = recall_score(y_test, y_test_pred_lr)\n",
    "test_f1_lr = f1_score(y_test, y_test_pred_lr)\n",
    "test_auc_lr = roc_auc_score(y_test, y_test_proba_lr)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Accuracy:  {test_acc_lr:.4f}\")\n",
    "print(f\"  Precision: {test_precision_lr:.4f}\")\n",
    "print(f\"  Recall:    {test_recall_lr:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1_lr:.4f}\")\n",
    "print(f\"  AUC-ROC:   {test_auc_lr:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred_lr, \n",
    "                          target_names=['Away Win', 'Home Win']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (coefficients)\n",
    "feature_importance_lr = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'coefficient': lr_model.coef_[0]\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features (by coefficient magnitude):\")\n",
    "print(feature_importance_lr.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), feature_importance_lr.head(10)['coefficient'])\n",
    "plt.yticks(range(10), feature_importance_lr.head(10)['feature'])\n",
    "plt.xlabel('Coefficient Value')\n",
    "plt.title('Logistic Regression - Top 10 Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Model #2: Random Forest\n",
    "\n",
    "Tree-based ensemble model (no scaling needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=50,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n‚úì Model trained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "y_train_proba_rf = rf_model.predict_proba(X_train)[:, 1]\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"‚úì Predictions generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate random forest\n",
    "print(\"=\" * 70)\n",
    "print(\"RANDOM FOREST PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train metrics\n",
    "train_acc_rf = accuracy_score(y_train, y_train_pred_rf)\n",
    "train_auc_rf = roc_auc_score(y_train, y_train_proba_rf)\n",
    "\n",
    "print(\"\\nTrain Set:\")\n",
    "print(f\"  Accuracy: {train_acc_rf:.4f}\")\n",
    "print(f\"  AUC-ROC:  {train_auc_rf:.4f}\")\n",
    "\n",
    "# Test metrics\n",
    "test_acc_rf = accuracy_score(y_test, y_test_pred_rf)\n",
    "test_precision_rf = precision_score(y_test, y_test_pred_rf)\n",
    "test_recall_rf = recall_score(y_test, y_test_pred_rf)\n",
    "test_f1_rf = f1_score(y_test, y_test_pred_rf)\n",
    "test_auc_rf = roc_auc_score(y_test, y_test_proba_rf)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(f\"  Accuracy:  {test_acc_rf:.4f}\")\n",
    "print(f\"  Precision: {test_precision_rf:.4f}\")\n",
    "print(f\"  Recall:    {test_recall_rf:.4f}\")\n",
    "print(f\"  F1 Score:  {test_f1_rf:.4f}\")\n",
    "print(f\"  AUC-ROC:   {test_auc_rf:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Test):\")\n",
    "print(classification_report(y_test, y_test_pred_rf,\n",
    "                          target_names=['Away Win', 'Home Win']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_rf.head(10).to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), feature_importance_rf.head(10)['importance'])\n",
    "plt.yticks(range(10), feature_importance_rf.head(10)['feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Random Forest - Top 10 Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare baseline models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison DataFrame\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest'],\n",
    "    'Train Accuracy': [train_acc_lr, train_acc_rf],\n",
    "    'Test Accuracy': [test_acc_lr, test_acc_rf],\n",
    "    'Test Precision': [test_precision_lr, test_precision_rf],\n",
    "    'Test Recall': [test_recall_lr, test_recall_rf],\n",
    "    'Test F1': [test_f1_lr, test_f1_rf],\n",
    "    'Test AUC': [test_auc_lr, test_auc_rf]\n",
    "})\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_idx = comparison['Test AUC'].idxmax()\n",
    "best_model = comparison.loc[best_idx, 'Model']\n",
    "best_auc = comparison.loc[best_idx, 'Test AUC']\n",
    "\n",
    "print(f\"\\n‚úì Best Model: {best_model} (AUC: {best_auc:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "metrics = ['Train Accuracy', 'Test Accuracy']\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison.iloc[0][metrics], width, label='Logistic Regression')\n",
    "axes[0].bar(x + width/2, comparison.iloc[1][metrics], width, label='Random Forest')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Model Accuracy Comparison')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim([0.5, 0.8])\n",
    "\n",
    "# Test metrics comparison\n",
    "test_metrics = ['Test Precision', 'Test Recall', 'Test F1', 'Test AUC']\n",
    "x2 = np.arange(len(test_metrics))\n",
    "\n",
    "axes[1].bar(x2 - width/2, comparison.iloc[0][test_metrics], width, label='Logistic Regression')\n",
    "axes[1].bar(x2 + width/2, comparison.iloc[1][test_metrics], width, label='Random Forest')\n",
    "axes[1].set_ylabel('Score')\n",
    "axes[1].set_title('Test Set Metrics Comparison')\n",
    "axes[1].set_xticks(x2)\n",
    "axes[1].set_xticklabels(test_metrics, rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].set_ylim([0.5, 0.8])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ROC Curves\n",
    "\n",
    "Compare model discrimination ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROC curves\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_test_proba_lr)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_test_proba_rf)\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {test_auc_lr:.3f})', linewidth=2)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {test_auc_rf:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=1)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Baseline Models')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Confusion Matrices\n",
    "\n",
    "Visualize prediction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrices\n",
    "cm_lr = confusion_matrix(y_test, y_test_pred_lr)\n",
    "cm_rf = confusion_matrix(y_test, y_test_pred_rf)\n",
    "\n",
    "# Plot confusion matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Logistic Regression\n",
    "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Away Win', 'Home Win'],\n",
    "            yticklabels=['Away Win', 'Home Win'],\n",
    "            ax=axes[0])\n",
    "axes[0].set_title('Logistic Regression\\nConfusion Matrix')\n",
    "axes[0].set_ylabel('True Label')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "\n",
    "# Random Forest\n",
    "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens',\n",
    "            xticklabels=['Away Win', 'Home Win'],\n",
    "            yticklabels=['Away Win', 'Home Win'],\n",
    "            ax=axes[1])\n",
    "axes[1].set_title('Random Forest\\nConfusion Matrix')\n",
    "axes[1].set_ylabel('True Label')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Models\n",
    "\n",
    "Export trained models for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import boto3\n",
    "from io import BytesIO\n",
    "\n",
    "# Save models to S3\n",
    "s3 = boto3.client('s3')\n",
    "model_prefix = 'ml-models/baseline'\n",
    "\n",
    "print(\"Saving models to S3...\")\n",
    "\n",
    "# Save logistic regression\n",
    "lr_buffer = BytesIO()\n",
    "pickle.dump({'model': lr_model, 'scaler': scaler}, lr_buffer)\n",
    "lr_buffer.seek(0)\n",
    "s3.put_object(Bucket=S3_BUCKET, \n",
    "              Key=f'{model_prefix}/logistic_regression.pkl',\n",
    "              Body=lr_buffer.getvalue())\n",
    "print(\"  ‚úì Logistic Regression saved\")\n",
    "\n",
    "# Save random forest\n",
    "rf_buffer = BytesIO()\n",
    "pickle.dump(rf_model, rf_buffer)\n",
    "rf_buffer.seek(0)\n",
    "s3.put_object(Bucket=S3_BUCKET,\n",
    "              Key=f'{model_prefix}/random_forest.pkl',\n",
    "              Body=rf_buffer.getvalue())\n",
    "print(\"  ‚úì Random Forest saved\")\n",
    "\n",
    "# Save feature names\n",
    "feature_buffer = BytesIO()\n",
    "pickle.dump(feature_cols, feature_buffer)\n",
    "feature_buffer.seek(0)\n",
    "s3.put_object(Bucket=S3_BUCKET,\n",
    "              Key=f'{model_prefix}/feature_names.pkl',\n",
    "              Body=feature_buffer.getvalue())\n",
    "print(\"  ‚úì Feature names saved\")\n",
    "\n",
    "print(f\"\\n‚úì Models saved to s3://{S3_BUCKET}/{model_prefix}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "Baseline model results and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BASELINE MODEL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Models Trained:\")\n",
    "print(f\"  1. Logistic Regression\")\n",
    "print(f\"  2. Random Forest (100 trees, max_depth=10)\")\n",
    "\n",
    "print(f\"\\nüìà Performance (Test Set):\")\n",
    "print(f\"  Logistic Regression: {test_acc_lr:.1%} accuracy, {test_auc_lr:.3f} AUC\")\n",
    "print(f\"  Random Forest:       {test_acc_rf:.1%} accuracy, {test_auc_rf:.3f} AUC\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"  Test Accuracy: {comparison.loc[best_idx, 'Test Accuracy']:.1%}\")\n",
    "print(f\"  Test AUC:      {best_auc:.3f}\")\n",
    "\n",
    "# Goal check\n",
    "goal_met = comparison['Test Accuracy'].max() > 0.60\n",
    "if goal_met:\n",
    "    print(f\"\\n‚úì Goal achieved: Accuracy > 60%\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  Goal not met: Accuracy ‚â§ 60%\")\n",
    "    print(f\"   Consider: More features, hyperparameter tuning, advanced models\")\n",
    "\n",
    "print(f\"\\nüíæ Outputs:\")\n",
    "print(f\"  - Models saved to S3: s3://{S3_BUCKET}/ml-models/baseline/\")\n",
    "print(f\"  - Logistic Regression: logistic_regression.pkl\")\n",
    "print(f\"  - Random Forest: random_forest.pkl\")\n",
    "print(f\"  - Feature names: feature_names.pkl\")\n",
    "\n",
    "print(f\"\\nüéØ Next Steps:\")\n",
    "print(f\"  1. Review feature importance (which features matter most?)\")\n",
    "print(f\"  2. Try advanced models (XGBoost, LightGBM) in 04_advanced_models.ipynb\")\n",
    "print(f\"  3. Hyperparameter tuning for better performance\")\n",
    "print(f\"  4. Feature engineering (add more features if needed)\")\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
