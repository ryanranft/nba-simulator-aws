{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA Game Prediction - Feature Engineering\n",
    "\n",
    "**Purpose:** Extract and engineer features from RDS database for ML model training\n",
    "\n",
    "**Data Source:** PostgreSQL RDS (nba_simulator database)\n",
    "\n",
    "**Output:** Parquet files in S3 with engineered features\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook performs:\n",
    "1. **Data Validation:** Check RDS data quality\n",
    "2. **Team Features:** Win rate, PPG, FG%, defensive stats\n",
    "3. **Player Features:** Usage rate, efficiency, PER\n",
    "4. **Time Features:** Momentum, streaks, rest days\n",
    "5. **Categorical Encoding:** Home/away, opponent, venue\n",
    "6. **S3 Storage:** Save features as Parquet\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ Libraries imported\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Validation (Workflow #21)\n",
    "\n",
    "Validate RDS data quality before feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection\n",
    "DB_HOST = 'nba-sim-db.ck96ciigs7fy.us-east-1.rds.amazonaws.com'\n",
    "DB_NAME = 'nba_simulator'\n",
    "DB_USER = 'postgres'\n",
    "DB_PASSWORD = 'YOUR_PASSWORD_HERE'  # Replace with actual password\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:5432/{DB_NAME}')\n",
    "\n",
    "print(\"✓ Database connection created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data counts\n",
    "validation_queries = {\n",
    "    'games': 'SELECT COUNT(*) as count FROM games',\n",
    "    'play_by_play': 'SELECT COUNT(*) as count FROM play_by_play',\n",
    "    'player_game_stats': 'SELECT COUNT(*) as count FROM player_game_stats',\n",
    "    'team_game_stats': 'SELECT COUNT(*) as count FROM team_game_stats'\n",
    "}\n",
    "\n",
    "print(\"Data Validation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for table, query in validation_queries.items():\n",
    "    result = pd.read_sql(query, engine)\n",
    "    print(f\"{table:20s}: {result['count'][0]:>10,} rows\")\n",
    "\n",
    "# Expected:\n",
    "# games: 46,595\n",
    "# play_by_play: 6,781,155\n",
    "# player_game_stats: 408,833\n",
    "# team_game_stats: 15,900"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in critical fields\n",
    "games_sample = pd.read_sql(\"\"\"\n",
    "    SELECT game_id, game_date, home_team_id, away_team_id, \n",
    "           home_points, away_points, season\n",
    "    FROM games\n",
    "    LIMIT 10000\n",
    "\"\"\", engine)\n",
    "\n",
    "print(\"\\nMissing Value Check (sample of 10,000 games):\")\n",
    "print(\"=\" * 50)\n",
    "missing = games_sample.isnull().sum()\n",
    "print(missing[missing > 0])\n",
    "if missing.sum() == 0:\n",
    "    print(\"✓ No missing values in critical fields\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date range\n",
    "date_range = pd.read_sql(\"\"\"\n",
    "    SELECT MIN(game_date) as earliest, MAX(game_date) as latest,\n",
    "           COUNT(DISTINCT season) as num_seasons\n",
    "    FROM games\n",
    "\"\"\", engine)\n",
    "\n",
    "print(\"\\nDate Range:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Earliest game: {date_range['earliest'][0]}\")\n",
    "print(f\"Latest game:   {date_range['latest'][0]}\")\n",
    "print(f\"Total seasons: {date_range['num_seasons'][0]}\")\n",
    "print(\"✓ Data validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Game Data\n",
    "\n",
    "Load complete games table for feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all games with relevant fields\n",
    "games_query = \"\"\"\n",
    "    SELECT \n",
    "        game_id, game_date, season, season_type,\n",
    "        home_team_id, away_team_id,\n",
    "        home_points, away_points,\n",
    "        attendance, venue_name, venue_city,\n",
    "        CASE WHEN home_points > away_points THEN 1 ELSE 0 END as home_win\n",
    "    FROM games\n",
    "    WHERE season_type = 2  -- Regular season only\n",
    "    AND home_points IS NOT NULL\n",
    "    AND away_points IS NOT NULL\n",
    "    ORDER BY game_date\n",
    "\"\"\"\n",
    "\n",
    "print(\"Loading games data...\")\n",
    "games = pd.read_sql(games_query, engine)\n",
    "games['game_date'] = pd.to_datetime(games['game_date'])\n",
    "\n",
    "print(f\"✓ Loaded {len(games):,} games\")\n",
    "print(f\"  Date range: {games['game_date'].min()} to {games['game_date'].max()}\")\n",
    "print(f\"  Seasons: {games['season'].nunique()}\")\n",
    "print(f\"  Home win rate: {games['home_win'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Team Performance Features\n",
    "\n",
    "Calculate rolling team statistics:\n",
    "- Win percentage\n",
    "- Points per game (PPG)\n",
    "- Points allowed per game\n",
    "- Recent form (last 10 games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rolling_stats(games_df, team_id, is_home=True, window=10):\n",
    "    \"\"\"\n",
    "    Calculate rolling statistics for a team.\n",
    "    \n",
    "    Args:\n",
    "        games_df: DataFrame of games\n",
    "        team_id: Team ID to calculate stats for\n",
    "        is_home: Whether to calculate home or away stats\n",
    "        window: Rolling window size (default 10 games)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with rolling statistics\n",
    "    \"\"\"\n",
    "    if is_home:\n",
    "        team_games = games_df[games_df['home_team_id'] == team_id].copy()\n",
    "        team_games['points_for'] = team_games['home_points']\n",
    "        team_games['points_against'] = team_games['away_points']\n",
    "        team_games['won'] = team_games['home_win']\n",
    "    else:\n",
    "        team_games = games_df[games_df['away_team_id'] == team_id].copy()\n",
    "        team_games['points_for'] = team_games['away_points']\n",
    "        team_games['points_against'] = team_games['home_points']\n",
    "        team_games['won'] = 1 - team_games['home_win']\n",
    "    \n",
    "    team_games = team_games.sort_values('game_date')\n",
    "    \n",
    "    # Calculate rolling statistics (using past games only)\n",
    "    team_games['rolling_win_pct'] = team_games['won'].rolling(window=window, min_periods=1).mean().shift(1)\n",
    "    team_games['rolling_ppg'] = team_games['points_for'].rolling(window=window, min_periods=1).mean().shift(1)\n",
    "    team_games['rolling_papg'] = team_games['points_against'].rolling(window=window, min_periods=1).mean().shift(1)\n",
    "    team_games['rolling_margin'] = team_games['rolling_ppg'] - team_games['rolling_papg']\n",
    "    \n",
    "    return team_games[['game_id', 'rolling_win_pct', 'rolling_ppg', 'rolling_papg', 'rolling_margin']]\n",
    "\n",
    "print(\"✓ Rolling stats function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate rolling stats for all teams\n",
    "print(\"Calculating team rolling statistics...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "all_teams = pd.concat([\n",
    "    games['home_team_id'],\n",
    "    games['away_team_id']\n",
    "]).unique()\n",
    "\n",
    "home_stats_list = []\n",
    "away_stats_list = []\n",
    "\n",
    "for i, team_id in enumerate(all_teams, 1):\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processed {i}/{len(all_teams)} teams...\")\n",
    "    \n",
    "    # Home stats\n",
    "    home_stats = calculate_rolling_stats(games, team_id, is_home=True)\n",
    "    home_stats = home_stats.rename(columns={\n",
    "        'rolling_win_pct': 'home_rolling_win_pct',\n",
    "        'rolling_ppg': 'home_rolling_ppg',\n",
    "        'rolling_papg': 'home_rolling_papg',\n",
    "        'rolling_margin': 'home_rolling_margin'\n",
    "    })\n",
    "    home_stats_list.append(home_stats)\n",
    "    \n",
    "    # Away stats\n",
    "    away_stats = calculate_rolling_stats(games, team_id, is_home=False)\n",
    "    away_stats = away_stats.rename(columns={\n",
    "        'rolling_win_pct': 'away_rolling_win_pct',\n",
    "        'rolling_ppg': 'away_rolling_ppg',\n",
    "        'rolling_papg': 'away_rolling_papg',\n",
    "        'rolling_margin': 'away_rolling_margin'\n",
    "    })\n",
    "    away_stats_list.append(away_stats)\n",
    "\n",
    "# Combine all stats\n",
    "home_stats_df = pd.concat(home_stats_list, ignore_index=True)\n",
    "away_stats_df = pd.concat(away_stats_list, ignore_index=True)\n",
    "\n",
    "print(f\"\\n✓ Calculated rolling stats for {len(all_teams)} teams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge rolling stats back to games\n",
    "games_with_stats = games.merge(home_stats_df, on='game_id', how='left')\n",
    "games_with_stats = games_with_stats.merge(away_stats_df, on='game_id', how='left')\n",
    "\n",
    "print(f\"✓ Merged rolling stats into games DataFrame\")\n",
    "print(f\"  Shape: {games_with_stats.shape}\")\n",
    "print(f\"  Columns: {games_with_stats.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Series Features\n",
    "\n",
    "Calculate:\n",
    "- Rest days since last game\n",
    "- Back-to-back game indicator\n",
    "- Win/loss streaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rest_days(games_df, team_id, is_home=True):\n",
    "    \"\"\"\n",
    "    Calculate rest days since last game for a team.\n",
    "    \"\"\"\n",
    "    if is_home:\n",
    "        team_games = games_df[games_df['home_team_id'] == team_id].copy()\n",
    "    else:\n",
    "        team_games = games_df[games_df['away_team_id'] == team_id].copy()\n",
    "    \n",
    "    team_games = team_games.sort_values('game_date')\n",
    "    team_games['rest_days'] = team_games['game_date'].diff().dt.days - 1\n",
    "    team_games['rest_days'] = team_games['rest_days'].fillna(7)  # First game default\n",
    "    team_games['back_to_back'] = (team_games['rest_days'] == 0).astype(int)\n",
    "    \n",
    "    return team_games[['game_id', 'rest_days', 'back_to_back']]\n",
    "\n",
    "print(\"Calculating rest days and back-to-backs...\")\n",
    "\n",
    "home_rest_list = []\n",
    "away_rest_list = []\n",
    "\n",
    "for team_id in all_teams:\n",
    "    # Home rest\n",
    "    home_rest = calculate_rest_days(games, team_id, is_home=True)\n",
    "    home_rest = home_rest.rename(columns={\n",
    "        'rest_days': 'home_rest_days',\n",
    "        'back_to_back': 'home_back_to_back'\n",
    "    })\n",
    "    home_rest_list.append(home_rest)\n",
    "    \n",
    "    # Away rest\n",
    "    away_rest = calculate_rest_days(games, team_id, is_home=False)\n",
    "    away_rest = away_rest.rename(columns={\n",
    "        'rest_days': 'away_rest_days',\n",
    "        'back_to_back': 'away_back_to_back'\n",
    "    })\n",
    "    away_rest_list.append(away_rest)\n",
    "\n",
    "home_rest_df = pd.concat(home_rest_list, ignore_index=True)\n",
    "away_rest_df = pd.concat(away_rest_list, ignore_index=True)\n",
    "\n",
    "games_with_stats = games_with_stats.merge(home_rest_df, on='game_id', how='left')\n",
    "games_with_stats = games_with_stats.merge(away_rest_df, on='game_id', how='left')\n",
    "\n",
    "print(f\"✓ Added rest days and back-to-back features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Categorical Encoding\n",
    "\n",
    "Encode categorical variables:\n",
    "- Season (ordinal)\n",
    "- Month of season\n",
    "- Day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date-based features\n",
    "games_with_stats['month'] = games_with_stats['game_date'].dt.month\n",
    "games_with_stats['day_of_week'] = games_with_stats['game_date'].dt.dayofweek\n",
    "games_with_stats['is_weekend'] = (games_with_stats['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Season phase (early/mid/late season)\n",
    "games_with_stats['season_month'] = games_with_stats['game_date'].dt.month\n",
    "games_with_stats['season_phase'] = pd.cut(\n",
    "    games_with_stats['season_month'],\n",
    "    bins=[0, 12, 2, 5],  # Oct-Dec, Jan-Feb, Mar-Apr\n",
    "    labels=[0, 1, 2],     # Early, mid, late\n",
    "    ordered=True\n",
    ")\n",
    "games_with_stats['season_phase'] = games_with_stats['season_phase'].astype(int)\n",
    "\n",
    "print(\"✓ Added categorical features\")\n",
    "print(f\"  Months: {games_with_stats['month'].nunique()}\")\n",
    "print(f\"  Days of week: {games_with_stats['day_of_week'].nunique()}\")\n",
    "print(f\"  Season phases: {games_with_stats['season_phase'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection & Cleanup\n",
    "\n",
    "Select final features for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for ML model\n",
    "feature_columns = [\n",
    "    # Identifiers\n",
    "    'game_id', 'game_date', 'season',\n",
    "    'home_team_id', 'away_team_id',\n",
    "    \n",
    "    # Target variable\n",
    "    'home_win',\n",
    "    \n",
    "    # Home team features\n",
    "    'home_rolling_win_pct', 'home_rolling_ppg', \n",
    "    'home_rolling_papg', 'home_rolling_margin',\n",
    "    'home_rest_days', 'home_back_to_back',\n",
    "    \n",
    "    # Away team features\n",
    "    'away_rolling_win_pct', 'away_rolling_ppg',\n",
    "    'away_rolling_papg', 'away_rolling_margin',\n",
    "    'away_rest_days', 'away_back_to_back',\n",
    "    \n",
    "    # Categorical features\n",
    "    'month', 'day_of_week', 'is_weekend', 'season_phase'\n",
    "]\n",
    "\n",
    "features_df = games_with_stats[feature_columns].copy()\n",
    "\n",
    "print(f\"✓ Selected {len(feature_columns)} feature columns\")\n",
    "print(f\"  Total rows: {len(features_df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values (first few games per team)\n",
    "features_df_clean = features_df.dropna()\n",
    "\n",
    "print(f\"✓ Removed rows with missing values\")\n",
    "print(f\"  Before: {len(features_df):,} rows\")\n",
    "print(f\"  After:  {len(features_df_clean):,} rows\")\n",
    "print(f\"  Dropped: {len(features_df) - len(features_df_clean):,} rows ({(1 - len(features_df_clean)/len(features_df))*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"\\nFinal Feature Summary:\")\n",
    "print(\"=\" * 70)\n",
    "print(features_df_clean.describe())\n",
    "\n",
    "print(\"\\nFeature Correlations with Target (home_win):\")\n",
    "print(\"=\" * 70)\n",
    "numeric_cols = features_df_clean.select_dtypes(include=[np.number]).columns\n",
    "correlations = features_df_clean[numeric_cols].corr()['home_win'].sort_values(ascending=False)\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save to S3 (Parquet Format)\n",
    "\n",
    "Save engineered features to S3 for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 configuration\n",
    "S3_BUCKET = 'nba-sim-raw-data-lake'\n",
    "S3_PREFIX = 'ml-features'\n",
    "\n",
    "# Save to S3 as Parquet\n",
    "output_path = f's3://{S3_BUCKET}/{S3_PREFIX}/game_features.parquet'\n",
    "\n",
    "print(f\"Saving features to S3...\")\n",
    "print(f\"  Path: {output_path}\")\n",
    "print(f\"  Rows: {len(features_df_clean):,}\")\n",
    "print(f\"  Columns: {len(features_df_clean.columns)}\")\n",
    "\n",
    "features_df_clean.to_parquet(output_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(\"✓ Features saved to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify S3 upload\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "try:\n",
    "    response = s3.head_object(Bucket=S3_BUCKET, Key=f'{S3_PREFIX}/game_features.parquet')\n",
    "    file_size_mb = response['ContentLength'] / (1024 * 1024)\n",
    "    \n",
    "    print(\"\\nS3 Upload Verification:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"✓ File exists in S3\")\n",
    "    print(f\"  Size: {file_size_mb:.2f} MB\")\n",
    "    print(f\"  Last modified: {response['LastModified']}\")\n",
    "    print(f\"  Full path: s3://{S3_BUCKET}/{S3_PREFIX}/game_features.parquet\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error verifying S3 upload: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Train/Test Split\n",
    "\n",
    "Split data chronologically (train on earlier seasons, test on recent seasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based split (80/20)\n",
    "features_df_clean = features_df_clean.sort_values('game_date')\n",
    "split_idx = int(len(features_df_clean) * 0.8)\n",
    "\n",
    "train_df = features_df_clean.iloc[:split_idx]\n",
    "test_df = features_df_clean.iloc[split_idx:]\n",
    "\n",
    "print(f\"Train/Test Split:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Train set:\")\n",
    "print(f\"  Rows: {len(train_df):,}\")\n",
    "print(f\"  Date range: {train_df['game_date'].min()} to {train_df['game_date'].max()}\")\n",
    "print(f\"  Home win rate: {train_df['home_win'].mean():.3f}\")\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Rows: {len(test_df):,}\")\n",
    "print(f\"  Date range: {test_df['game_date'].min()} to {test_df['game_date'].max()}\")\n",
    "print(f\"  Home win rate: {test_df['home_win'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save train/test splits to S3\n",
    "train_path = f's3://{S3_BUCKET}/{S3_PREFIX}/train.parquet'\n",
    "test_path = f's3://{S3_BUCKET}/{S3_PREFIX}/test.parquet'\n",
    "\n",
    "print(\"Saving train/test splits to S3...\")\n",
    "train_df.to_parquet(train_path, engine='pyarrow', index=False)\n",
    "test_df.to_parquet(test_path, engine='pyarrow', index=False)\n",
    "\n",
    "print(f\"✓ Train data: {train_path}\")\n",
    "print(f\"✓ Test data:  {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering Summary\n",
    "\n",
    "Summary of created features and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal games processed: {len(features_df_clean):,}\")\n",
    "print(f\"Total features: {len(feature_columns)}\")\n",
    "print(f\"\\nFeature categories:\")\n",
    "print(f\"  - Team performance (rolling stats): 8 features\")\n",
    "print(f\"  - Rest/schedule: 4 features\")\n",
    "print(f\"  - Temporal: 4 features\")\n",
    "print(f\"  - Target variable: 1 feature (home_win)\")\n",
    "print(f\"\\nS3 outputs:\")\n",
    "print(f\"  - Full dataset: s3://{S3_BUCKET}/{S3_PREFIX}/game_features.parquet\")\n",
    "print(f\"  - Train set:    s3://{S3_BUCKET}/{S3_PREFIX}/train.parquet\")\n",
    "print(f\"  - Test set:     s3://{S3_BUCKET}/{S3_PREFIX}/test.parquet\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Create 03_model_training.ipynb\")\n",
    "print(f\"  2. Train baseline models (logistic regression, random forest)\")\n",
    "print(f\"  3. Evaluate model performance\")\n",
    "print(f\"  4. Tune hyperparameters\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
