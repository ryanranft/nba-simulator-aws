{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIMS Interactive Explorer\n",
    "\n",
    "**Data Inventory Management System - Interactive Analysis & Visualization**\n",
    "\n",
    "This notebook provides comprehensive tools for:\n",
    "- \ud83d\udcca Metric overview and analysis\n",
    "- \ud83d\udcc8 Historical trend visualization\n",
    "- \ud83d\udd0d Verification run exploration\n",
    "- \u2705 Approval workflow tracking\n",
    "- \ud83d\udcdd Event log analysis\n",
    "- \ud83d\udcbe Data export utilities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "\n",
    "print(\"\u2713 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project to path\n",
    "project_root = Path.home() / 'nba-simulator-aws'\n",
    "sys.path.insert(0, str(project_root / 'scripts' / 'monitoring'))\n",
    "\n",
    "# Import DIMS modules\n",
    "from dims.core import DIMSCore\n",
    "from dims.notebook_utils import DIMSNotebookHelper\n",
    "\n",
    "print(f\"\u2713 DIMS modules loaded from {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DIMS\n",
    "dims = DIMSCore(project_root=str(project_root))\n",
    "helper = DIMSNotebookHelper(dims)\n",
    "\n",
    "print(\"\u2713 DIMS initialized\")\n",
    "print(f\"  Project: {dims.config.get('project_root')}\")\n",
    "print(f\"  Database: {'Connected' if dims.database else 'Not available'}\")\n",
    "print(f\"  Total Metrics: {len(dims.metrics)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. System Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get system health metrics\n",
    "health = helper.get_system_health()\n",
    "\n",
    "print(\"\ud83d\udcca DIMS SYSTEM HEALTH\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total Metrics Defined:     {health['total_metrics']}\")\n",
    "print(f\"Metrics with Data:         {health['metrics_with_data']}\")\n",
    "print(f\"Recent Verifications (7d): {health['recent_verifications']}\")\n",
    "print(f\"Pending Approvals:         {health['pending_approvals']}\")\n",
    "print(f\"Recent Events (7d):        {health['recent_events']}\")\n",
    "print(f\"Avg Execution Time:        {health['avg_execution_time_ms']:.0f} ms\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics by category\n",
    "fig = helper.plot_metrics_overview()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latest Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all latest metrics\n",
    "df_latest = helper.get_latest_metrics()\n",
    "\n",
    "print(f\"\\n\ud83d\udcc8 LATEST METRICS ({len(df_latest)} total)\")\n",
    "print(\"=\" * 80)\n",
    "df_latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by category (interactive)\n",
    "category = 'code_base'  # Change this to explore different categories\n",
    "\n",
    "df_category = df_latest[df_latest['metric_category'] == category]\n",
    "print(f\"\\n{category.upper()} METRICS\")\n",
    "print(\"=\" * 80)\n",
    "df_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Metric Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trend for a specific metric (customize these)\n",
    "category = 'code_base'\n",
    "metric = 'python_files'\n",
    "days = 30\n",
    "\n",
    "fig = helper.plot_metric_trend(category, metric, days)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get drift summary for all metrics\n",
    "df_drift = helper.get_drift_summary(days=30)\n",
    "\n",
    "print(\"\\n\ud83d\udcc9 DRIFT ANALYSIS (30 days)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sort by absolute drift\n",
    "df_drift['abs_drift'] = df_drift['drift_pct'].abs()\n",
    "df_drift_sorted = df_drift.sort_values('abs_drift', ascending=False)\n",
    "\n",
    "df_drift_sorted.drop('abs_drift', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize drift across all metrics\n",
    "fig = helper.plot_drift_heatmap(days=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verification Run History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recent verification runs\n",
    "df_verifications = helper.get_verification_runs(days=30)\n",
    "\n",
    "print(f\"\\n\ud83d\udd0d VERIFICATION RUNS ({len(df_verifications)} runs in last 30 days)\")\n",
    "print(\"=\" * 80)\n",
    "df_verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize verification timeline\n",
    "fig = helper.plot_verification_timeline(days=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification statistics\n",
    "if not df_verifications.empty:\n",
    "    print(\"\\n\ud83d\udcca VERIFICATION STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Runs:               {len(df_verifications)}\")\n",
    "    print(f\"Runs with Drift:          {df_verifications['drift_detected'].sum()}\")\n",
    "    print(f\"Runs with Auto-Update:    {df_verifications['auto_updated'].sum()}\")\n",
    "    print(f\"Avg Metrics Verified:     {df_verifications['metrics_verified'].mean():.1f}\")\n",
    "    print(f\"Avg Execution Time:       {df_verifications['execution_time_ms'].mean()/1000:.1f}s\")\n",
    "    print(f\"Min Execution Time:       {df_verifications['execution_time_ms'].min()/1000:.1f}s\")\n",
    "    print(f\"Max Execution Time:       {df_verifications['execution_time_ms'].max()/1000:.1f}s\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Approval Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get approval history\n",
    "df_approvals = helper.get_approval_log(days=30)\n",
    "\n",
    "print(f\"\\n\u2705 APPROVAL WORKFLOW ({len(df_approvals)} requests in last 30 days)\")\n",
    "print(\"=\" * 80)\n",
    "df_approvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize approval status\n",
    "fig = helper.plot_approval_status(days=30)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approval statistics\n",
    "if not df_approvals.empty:\n",
    "    print(\"\\n\ud83d\udcca APPROVAL STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Requests:           {len(df_approvals)}\")\n",
    "    print(f\"Pending:                  {(df_approvals['status'] == 'pending').sum()}\")\n",
    "    print(f\"Approved:                 {(df_approvals['status'] == 'approved').sum()}\")\n",
    "    print(f\"Rejected:                 {(df_approvals['status'] == 'rejected').sum()}\")\n",
    "    print(f\"Avg Drift:                {df_approvals['drift_pct'].mean():.2f}%\")\n",
    "    print(f\"Max Drift:                {df_approvals['drift_pct'].max():.2f}%\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Event Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get event log\n",
    "df_events = helper.get_event_log(days=7)\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd EVENT LOG ({len(df_events)} events in last 7 days)\")\n",
    "print(\"=\" * 80)\n",
    "df_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event statistics\n",
    "if not df_events.empty:\n",
    "    print(\"\\n\ud83d\udcca EVENT STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total Events:             {len(df_events)}\")\n",
    "    print(f\"Successful:               {df_events['success'].sum()}\")\n",
    "    print(f\"Failed:                   {(~df_events['success']).sum()}\")\n",
    "    print(f\"Total Metrics Updated:    {df_events['metrics_updated'].sum()}\")\n",
    "    print(\"\\nEvents by Type:\")\n",
    "    print(df_events.groupby('event_type').size())\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Queries\n",
    "\n",
    "Use this section for ad-hoc queries and custom analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Find metrics with highest volatility\n",
    "df_drift = helper.get_drift_summary(days=30)\n",
    "\n",
    "if not df_drift.empty:\n",
    "    top_volatile = df_drift.nlargest(10, 'volatility_pct')[['category', 'metric', 'volatility_pct', 'drift_pct']]\n",
    "    \n",
    "    print(\"\\n\ud83d\udd25 TOP 10 MOST VOLATILE METRICS\")\n",
    "    print(\"=\" * 80)\n",
    "    top_volatile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Custom database query\n",
    "if dims.database:\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            metric_category,\n",
    "            COUNT(*) as metric_count,\n",
    "            AVG(numeric_value) as avg_value\n",
    "        FROM dims_metrics_latest\n",
    "        WHERE numeric_value IS NOT NULL\n",
    "        GROUP BY metric_category\n",
    "        ORDER BY metric_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    conn = dims.database.pool.getconn()\n",
    "    try:\n",
    "        df_custom = pd.read_sql_query(query, conn)\n",
    "        print(\"\\n\ud83d\udcca METRICS BY CATEGORY (with averages)\")\n",
    "        print(\"=\" * 80)\n",
    "        display(df_custom)\n",
    "    finally:\n",
    "        dims.database.pool.putconn(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Compare multiple metrics on same chart\n",
    "metrics_to_compare = [\n",
    "    ('code_base', 'python_files'),\n",
    "    ('code_base', 'test_files'),\n",
    "    ('documentation', 'markdown_files')\n",
    "]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for category, metric in metrics_to_compare:\n",
    "    df_history = helper.get_metric_history(category, metric, days=30)\n",
    "    if not df_history.empty:\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_history['recorded_at'],\n",
    "            y=df_history['numeric_value'],\n",
    "            mode='lines+markers',\n",
    "            name=f\"{category}.{metric}\"\n",
    "        ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Multi-Metric Comparison\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Value\",\n",
    "    template='plotly_white',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export latest metrics to CSV\n",
    "df_latest = helper.get_latest_metrics()\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"dims_metrics_{timestamp}.csv\"\n",
    "\n",
    "path = helper.export_to_csv(df_latest, filename)\n",
    "print(f\"\u2713 Exported to: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export comprehensive report to Excel\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"dims_report_{timestamp}.xlsx\"\n",
    "\n",
    "# Gather all data\n",
    "dataframes = {\n",
    "    'Latest Metrics': helper.get_latest_metrics(),\n",
    "    'Drift Summary': helper.get_drift_summary(days=30),\n",
    "    'Verification Runs': helper.get_verification_runs(days=30),\n",
    "    'Approvals': helper.get_approval_log(days=30),\n",
    "    'Events': helper.get_event_log(days=7)\n",
    "}\n",
    "\n",
    "# Remove empty DataFrames\n",
    "dataframes = {k: v for k, v in dataframes.items() if not v.empty}\n",
    "\n",
    "path = helper.export_to_excel(dataframes, filename)\n",
    "print(f\"\u2713 Comprehensive report exported to: {path}\")\n",
    "print(f\"  Sheets: {', '.join(dataframes.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary & Recommendations\n",
    "\n",
    "Based on the analysis above, generate insights and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate automated recommendations\n",
    "df_drift = helper.get_drift_summary(days=30)\n",
    "\n",
    "print(\"\\n\ud83c\udfaf AUTOMATED RECOMMENDATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if not df_drift.empty:\n",
    "    # High drift warnings\n",
    "    high_drift = df_drift[df_drift['drift_pct'].abs() > 15]\n",
    "    if not high_drift.empty:\n",
    "        print(\"\\n\u26a0\ufe0f  HIGH DRIFT DETECTED:\")\n",
    "        for _, row in high_drift.iterrows():\n",
    "            print(f\"   \u2022 {row['category']}.{row['metric']}: {row['drift_pct']:.1f}% drift\")\n",
    "    \n",
    "    # High volatility warnings\n",
    "    high_volatility = df_drift[df_drift['volatility_pct'] > 25]\n",
    "    if not high_volatility.empty:\n",
    "        print(\"\\n\ud83d\udcca HIGH VOLATILITY DETECTED:\")\n",
    "        for _, row in high_volatility.iterrows():\n",
    "            print(f\"   \u2022 {row['category']}.{row['metric']}: {row['volatility_pct']:.1f}% volatility\")\n",
    "    \n",
    "    # Stable metrics\n",
    "    stable = df_drift[df_drift['drift_pct'].abs() < 1]\n",
    "    if not stable.empty:\n",
    "        print(f\"\\n\u2705 STABLE METRICS: {len(stable)} metrics with <1% drift\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n\ud83d\udca1 For detailed analysis, explore specific sections above.\")\n",
    "print(\"\ud83d\udcca Use the Custom Queries section for ad-hoc investigations.\")\n",
    "print(\"\ud83d\udcbe Export data using the Export section for external analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Customize Analysis**: Modify the cells above to focus on specific metrics or time periods\n",
    "2. **Add Custom Queries**: Use Section 8 to write custom SQL queries\n",
    "3. **Export Reports**: Use Section 9 to generate reports for stakeholders\n",
    "4. **Schedule Runs**: Set up automated notebook execution for regular reporting\n",
    "\n",
    "**Documentation**: See `docs/DIMS_JUPYTER_GUIDE.md` for comprehensive guide\n",
    "\n",
    "**CLI Commands**:\n",
    "```bash\n",
    "# Launch notebook\n",
    "dims_cli.py notebook\n",
    "\n",
    "# Export notebook as HTML report\n",
    "dims_cli.py notebook export\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Import workflow integration\n",
    "from dims.workflow_integration import WorkflowIntegration\n",
    "\n",
    "workflow = WorkflowIntegration(str(project_root))\n",
    "\n",
    "print(\"\u2713 Workflow integration loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run file inventory workflow (Workflow #13)\n",
    "result = workflow.run_file_inventory(update=True)\n",
    "\n",
    "print(\"\\n\ud83d\udcc1 FILE INVENTORY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total files documented: {result['total_files']}\")\n",
    "print(f\"Last updated: {result['last_updated']}\")\n",
    "print(\"\\nCategories:\")\n",
    "for category, count in result['categories'].items():\n",
    "    print(f\"  - {category}: {count}\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run local data inventory (Workflow #45)\n",
    "result = workflow.run_local_data_inventory(mode='quick')\n",
    "\n",
    "print(\"\\n\ud83d\udcbe LOCAL DATA INVENTORY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Archives: {result['archives_size_gb']} GB\")\n",
    "print(f\"Temp data: {result['temp_size_gb']} GB\")\n",
    "print(f\"Project: {result['project_size_gb']} GB\")\n",
    "print(f\"Total: {result['archives_size_gb'] + result['temp_size_gb'] + result['project_size_gb']} GB\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run AWS inventory (Workflow #47)\n",
    "result = workflow.run_aws_data_inventory()\n",
    "\n",
    "print(\"\\n\u2601\ufe0f  AWS DATA INVENTORY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nS3:\")\n",
    "print(f\"  Objects: {result['s3_objects']:,}\")\n",
    "print(f\"  Size: {result['s3_size_gb']:.2f} GB\")\n",
    "print(f\"\\nRDS:\")\n",
    "print(f\"  Database size: {result['rds_size_gb']:.2f} GB\")\n",
    "print(f\"  Allocated storage: {result['rds_allocated_gb']} GB\")\n",
    "print(f\"\\n\ud83d\udcb0 Estimated monthly cost: ${result['estimated_cost_usd']:.2f}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize cost breakdown\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    labels=['S3 Storage', 'RDS Database'],\n",
    "    values=[result['s3_size_gb'] * 0.023, 20.0],\n",
    "    hole=0.3\n",
    ")])\n",
    "fig.update_layout(title='Monthly Cost Breakdown')\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run data gap analysis (Workflow #46)\n",
    "result = workflow.run_data_gap_analysis()\n",
    "\n",
    "print(\"\\n\ud83d\udd0d DATA GAP ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total games: {result['total_games']}\")\n",
    "print(f\"Missing box scores: {result['missing_games']}\")\n",
    "print(f\"Missing play-by-play: {result['games_without_pbp']}\")\n",
    "\n",
    "if result['missing_games'] > 0 or result['games_without_pbp'] > 0:\n",
    "    print(\"\\n\u26a0\ufe0f  Data gaps detected!\")\n",
    "else:\n",
    "    print(\"\\n\u2705 No data gaps found\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize data completeness\n",
    "if result['total_games'] > 0:\n",
    "    completeness = [\n",
    "        result['total_games'] - result['missing_games'],\n",
    "        result['total_games'] - result['games_without_pbp']\n",
    "    ]\n",
    "    labels = ['Box Scores', 'Play-by-Play']\n",
    "    \n",
    "    fig = go.Figure(data=[go.Bar(\n",
    "        x=labels,\n",
    "        y=completeness,\n",
    "        text=[f\"{(c/result['total_games']*100):.1f}%\" for c in completeness],\n",
    "        textposition='auto'\n",
    "    )])\n",
    "    fig.update_layout(\n",
    "        title='Data Completeness by Type',\n",
    "        yaxis_title='Games with Data',\n",
    "        template='plotly_white'\n",
    "    )\n",
    "    fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sync Status Dashboard\n",
    "\n",
    "Monitor synchronization between local files and S3."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run sync status check (Workflow #49)\n",
    "result = workflow.run_sync_status_check()\n",
    "\n",
    "print(\"\\n\ud83d\udd04 SYNC STATUS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"S3 files: {result['s3_files']:,}\")\n",
    "print(f\"Local files: {result['local_files']:,}\")\n",
    "print(f\"Drift: {result['drift_pct']:.1f}%\")\n",
    "print(f\"Status: {result['status'].upper()}\")\n",
    "\n",
    "if result['status'] == 'synced':\n",
    "    print(\"\\n\u2705 Local and S3 are synchronized\")\n",
    "elif result['status'] in ['minor_drift', 'moderate_drift']:\n",
    "    print(\"\\n\u26a0\ufe0f  Drift detected - consider running sync\")\n",
    "elif result['status'] == 'major_drift':\n",
    "    print(\"\\n\ud83d\udd34 MAJOR drift - URGENT sync recommended\")\n",
    "print(\"=\" * 80)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize sync status\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='S3',\n",
    "    x=['File Count'],\n",
    "    y=[result['s3_files']],\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Local',\n",
    "    x=['File Count'],\n",
    "    y=[result['local_files']],\n",
    "    marker_color='lightgreen'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='S3 vs Local File Counts',\n",
    "    yaxis_title='Number of Files',\n",
    "    barmode='group',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Status indicator\n",
    "status_colors = {\n",
    "    'synced': 'green',\n",
    "    'minor_drift': 'yellow',\n",
    "    'moderate_drift': 'orange',\n",
    "    'major_drift': 'red'\n",
    "}\n",
    "\n",
    "fig = go.Figure(go.Indicator(\n",
    "    mode='gauge+number+delta',\n",
    "    value=result['drift_pct'],\n",
    "    domain={'x': [0, 1], 'y': [0, 1]},\n",
    "    title={'text': 'Sync Drift %'},\n",
    "    delta={'reference': 0},\n",
    "    gauge={\n",
    "        'axis': {'range': [None, 50]},\n",
    "        'bar': {'color': status_colors.get(result['status'], 'gray')},\n",
    "        'steps': [\n",
    "            {'range': [0, 5], 'color': 'lightgreen'},\n",
    "            {'range': [5, 15], 'color': 'lightyellow'},\n",
    "            {'range': [15, 30], 'color': 'orange'},\n",
    "            {'range': [30, 50], 'color': 'red'}\n",
    "        ],\n",
    "        'threshold': {\n",
    "            'line': {'color': 'red', 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 30\n",
    "        }\n",
    "    }\n",
    "))\n",
    "\n",
    "fig.update_layout(height=300)\n",
    "fig.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Run all workflows and generate comprehensive report\n",
    "all_results = workflow.run_all_workflows()\n",
    "\n",
    "print(\"\\n\ud83d\udcca COMPREHENSIVE WORKFLOW REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Timestamp: {all_results['timestamp']}\")\n",
    "print(\"\\nWorkflows Run:\")\n",
    "for workflow_name in all_results['workflows'].keys():\n",
    "    print(f\"  \u2713 {workflow_name}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Export to Excel\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "filename = f\"workflow_report_{timestamp}.xlsx\"\n",
    "\n",
    "# Convert workflow results to DataFrames for export\n",
    "export_data = {}\n",
    "\n",
    "if 'file_inventory' in all_results['workflows']:\n",
    "    df_files = pd.DataFrame([all_results['workflows']['file_inventory']['categories']])\n",
    "    export_data['File Inventory'] = df_files\n",
    "\n",
    "if 'aws_inventory' in all_results['workflows']:\n",
    "    df_aws = pd.DataFrame([all_results['workflows']['aws_inventory']])\n",
    "    export_data['AWS Inventory'] = df_aws\n",
    "\n",
    "if 'data_gaps' in all_results['workflows']:\n",
    "    df_gaps = pd.DataFrame([all_results['workflows']['data_gaps']])\n",
    "    export_data['Data Gaps'] = df_gaps\n",
    "\n",
    "if 'sync_status' in all_results['workflows']:\n",
    "    df_sync = pd.DataFrame([all_results['workflows']['sync_status']])\n",
    "    export_data['Sync Status'] = df_sync\n",
    "\n",
    "if export_data:\n",
    "    path = helper.export_to_excel(export_data, filename)\n",
    "    print(f\"\\n\u2713 Workflow report exported: {path}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## 11. Workflow Integration\n\nRun existing data inventory workflows integrated with DIMS.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}