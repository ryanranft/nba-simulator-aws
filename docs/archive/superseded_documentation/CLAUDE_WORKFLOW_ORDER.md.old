# Claude Workflow Order - Priority Sequence

This document defines the order of operations Claude should follow for every session and task. Follow this sequence strictly.

---

## üöÄ Session Start Workflow (Every Session)

### 1. Initialize Session (AUTOMATIC - Don't Ask)
```bash
source scripts/shell/session_manager.sh start
```

**What this does:**
- Checks system diagnostics (hardware, software, environment)
- Reviews git status and recent commits
- Identifies documentation status (stale docs, pending tasks)
- Sources command logging functions

**Show output to user for review**

### 1.5. Standalone Health Check (OPTIONAL - Deep System Verification)

**Purpose:** Comprehensive 14-point system health verification for troubleshooting or fresh environment setup

**When to use:**
- Setting up fresh development environment
- After system updates/upgrades
- Troubleshooting environment issues
- Monthly comprehensive verification
- When `session_manager.sh` reports issues

**Script:** `scripts/shell/check_machine_health.sh`

**Usage:**
```bash
bash scripts/shell/check_machine_health.sh
```

**What this checks (14 comprehensive categories):**

#### 1Ô∏è‚É£ System Information
```bash
# Displays:
- macOS version (sw_vers)
- Device model (MacBook Pro 16-inch, 2023)
- Processor (Apple M2 Max)
- Memory (96 GB)
- Storage availability
```

#### 2Ô∏è‚É£ Disk Space
```bash
# Thresholds:
- >90% usage ‚Üí ‚ùå FAIL (critical)
- >80% usage ‚Üí ‚ö†Ô∏è  WARN (high)
- <80% usage ‚Üí ‚úÖ PASS

# Displays available space and usage percentage
```

#### 3Ô∏è‚É£ Homebrew
```bash
# Checks:
- ‚úÖ Homebrew installed
- ‚úÖ Correct location (/opt/homebrew/bin/brew for Apple Silicon)
- ‚ÑπÔ∏è  Outdated packages count
- Recommends: brew upgrade if packages outdated
```

#### 4Ô∏è‚É£ Miniconda
```bash
# Checks:
- ‚úÖ Conda installed and version
- ‚úÖ Conda base directory
- ‚úÖ nba-aws environment exists
```

#### 5Ô∏è‚É£ Python Environment (nba-aws)
```bash
# Activates nba-aws environment and checks:
- ‚úÖ Python 3.11 (required for AWS Glue 4.0)
- ‚úÖ Python location in conda env
- ‚úÖ Package count (warns if <50 packages)
```

#### 6Ô∏è‚É£ Core Python Packages
**Verifies 10 required packages:**
```python
REQUIRED_PACKAGES = [
    "boto3",          # AWS SDK
    "pandas",         # Data manipulation
    "numpy",          # Numerical computing
    "psycopg2-binary", # PostgreSQL driver
    "sqlalchemy",     # ORM
    "pytest",         # Testing
    "jupyter",        # Notebooks
    "python-dotenv",  # Environment variables
    "pyyaml",         # YAML parsing
    "tqdm"            # Progress bars
]

# Shows version for each installed package
# ‚ùå Fails if any missing
```

#### 7Ô∏è‚É£ AWS CLI
```bash
# Checks:
- ‚úÖ AWS CLI installed and version
- ‚úÖ Correct location (/opt/homebrew/bin/aws)
- ‚úÖ ~/.aws/credentials exists with permissions 600
- ‚úÖ S3 connectivity test (ls s3://nba-sim-raw-data-lake/)

# Warns if:
- Credentials file has wrong permissions
- Cannot connect to S3 bucket
```

#### 8Ô∏è‚É£ Git
```bash
# Checks:
- ‚úÖ Git installed and version
- ‚úÖ GitHub SSH authentication (ssh -T git@github.com)
- ‚úÖ Git user.name and user.email configured globally
```

#### 9Ô∏è‚É£ Project Files
**Verifies project structure:**
```bash
CRITICAL_FILES = [
    "requirements.txt",
    "Makefile",
    "CLAUDE.md",
    "PROGRESS.md",
    "README.md",
    "COMMAND_LOG.md"
]

DOC_FILES = [
    "docs/STYLE_GUIDE.md",
    "docs/TROUBLESHOOTING.md",
    "docs/SETUP.md"
]

REQUIRED_DIRS = [
    "scripts/",
    "sql/",
    "docs/",
    "config/"
]

# ‚ùå Fails if critical files missing
# ‚ö†Ô∏è  Warns if doc files or .env missing
```

#### üîü Data Directory
```bash
# Checks local data directory:
DATA_ROOT = "/Users/ryanranft/0espn/data/nba"

# Counts JSON files (may take time for 146K files)
- ‚úÖ PASS: >140,000 files
- ‚ö†Ô∏è  WARN: 100,000-140,000 files
- ‚ùå FAIL: <100,000 files

# ‚ö†Ô∏è  Warns if directory not found (okay if data in S3)
```

#### 1Ô∏è‚É£1Ô∏è‚É£ Network & AWS Connectivity
```bash
# Checks:
- ‚úÖ Internet connectivity (ping 8.8.8.8)
- ‚úÖ AWS S3 endpoint reachable (ping s3.us-east-1.amazonaws.com)
- ‚úÖ DNS resolution working (nslookup)
```

#### 1Ô∏è‚É£2Ô∏è‚É£ Security Settings
```bash
# Checks:
- ‚úÖ System Integrity Protection (SIP) enabled
- ‚úÖ FileVault disk encryption enabled
- ‚úÖ SSH private key permissions 600 (checks id_rsa or id_ed25519)
```

#### 1Ô∏è‚É£3Ô∏è‚É£ System Resources
```bash
# Monitors:
- Free memory (warns if <1 GB)
- CPU idle percentage
```

#### 1Ô∏è‚É£4Ô∏è‚É£ Optional Tools
**Checks availability (not required):**
```bash
OPTIONAL_TOOLS = [
    "jq - JSON processor",
    "psql - PostgreSQL client",
    "tree - Directory viewer",
    "tmux - Terminal multiplexer"
]

# ‚ö†Ô∏è  Shows as optional if not installed
```

**Sample Output:**
```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  NBA Simulator AWS - Machine Health Check                     ‚ïë
‚ïë  Date: 2025-10-02 19:45:23                                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

1Ô∏è‚É£  SYSTEM INFORMATION
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ProductName:		macOS
    ProductVersion:		14.6
    BuildVersion:		23G93

    Device: MacBook Pro 16-inch, 2023
    Processor: Apple M2 Max
    Memory: 96 GB
    Storage: 450 GB available of 1 TB

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

2Ô∏è‚É£  DISK SPACE
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Available: 450 GB
    Usage: 55%
‚úÖ Disk space OK

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

3Ô∏è‚É£  HOMEBREW
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Version: Homebrew 4.3.15
    Location: /opt/homebrew/bin/brew
‚úÖ Homebrew installed correctly (Apple Silicon)
‚ÑπÔ∏è  3 outdated packages (run 'brew upgrade')

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

4Ô∏è‚É£  MINICONDA
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Version: conda 24.7.1
    Base: /Users/ryanranft/miniconda3
‚úÖ Conda installed
‚úÖ nba-aws environment exists

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

5Ô∏è‚É£  PYTHON ENVIRONMENT (nba-aws)
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    Version: Python 3.11.9
    Location: /Users/ryanranft/miniconda3/envs/nba-aws/bin/python
‚úÖ Python 3.11 (required for AWS Glue 4.0)
    Packages installed: 127
‚úÖ Packages installed

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

6Ô∏è‚É£  CORE PYTHON PACKAGES
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    ‚úÖ boto3 (1.34.162)
    ‚úÖ pandas (2.2.2)
    ‚úÖ numpy (2.0.1)
    ‚úÖ psycopg2-binary (2.9.9)
    ‚úÖ sqlalchemy (2.0.32)
    ‚úÖ pytest (8.3.2)
    ‚úÖ jupyter (1.1.1)
    ‚úÖ python-dotenv (1.0.1)
    ‚úÖ pyyaml (6.0.2)
    ‚úÖ tqdm (4.66.5)

‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

[... continues through all 14 checks ...]

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  HEALTH CHECK SUMMARY                                          ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚úÖ ALL CHECKS PASSED - System ready for development

Next steps:
  1. Run: cd /Users/ryanranft/nba-simulator-aws
  2. Run: conda activate nba-aws
  3. Run: make verify-all (for additional checks)

Health check completed at 2025-10-02 19:45:45
```

**Exit Codes:**
- `0` - All checks passed (system healthy)
- `1` - One or more checks failed (review errors)

**Comparison to session_manager.sh:**

| Feature | session_manager.sh | check_machine_health.sh |
|---------|-------------------|-------------------------|
| **Scope** | Quick session start check | Comprehensive system verification |
| **Categories** | 6 quick checks | 14 detailed checks |
| **Runtime** | <2 seconds | 5-15 seconds (file counting) |
| **Package verification** | Basic (counts) | Detailed (10 required packages) |
| **Network tests** | No | Yes (ping, DNS, S3) |
| **Security checks** | Basic | Detailed (SIP, FileVault, SSH perms) |
| **Data verification** | No | Yes (counts local JSON files) |
| **Optional tools** | No | Yes (jq, psql, tree, tmux) |
| **Exit code** | Informational | Pass/Fail (usable in scripts) |
| **When to use** | Every session start | Troubleshooting, fresh setup |

**When to Use Each:**

**Use `session_manager.sh start` (Step 1):**
- ‚úÖ Every session start (automatic)
- ‚úÖ Quick orientation check
- ‚úÖ Sources command logging functions
- ‚úÖ Fast (<2 sec)

**Use `check_machine_health.sh`:**
- ‚úÖ Setting up fresh environment
- ‚úÖ After macOS/Homebrew updates
- ‚úÖ Troubleshooting "it works on my machine" issues
- ‚úÖ Monthly comprehensive verification
- ‚úÖ When session_manager reports problems
- ‚úÖ Before major deployments
- ‚úÖ After installing new packages/tools

**Troubleshooting with Health Check:**

If health check fails, common fixes:

```bash
# Missing packages
pip install -r requirements.txt

# AWS credentials issues
aws configure
chmod 600 ~/.aws/credentials

# Git configuration
git config --global user.name "Your Name"
git config --global user.email "your.email@example.com"

# Conda environment
conda env create -f environment.yml
conda activate nba-aws

# Disk space issues
# Clean conda caches:
conda clean --all

# Clean Homebrew:
brew cleanup

# Archive old data:
bash scripts/maintenance/archive_manager.sh full
```

**Integration Points:**
- **Session Start Workflow Step 1.5:** Deep verification alternative (this location)
- **Environment Setup Workflow:** Run after Step 12 to verify all components
- **Troubleshooting Workflow Step 1:** First diagnostic tool
- **Monthly Maintenance:** Part of comprehensive review checklist

**Best Practices:**
1. ‚úÖ Run before starting major work after system changes
2. ‚úÖ Run if session_manager.sh shows warnings
3. ‚úÖ Save output for comparison: `bash check_machine_health.sh > health_check.log`
4. ‚úÖ Re-run after fixing failures to verify fixes worked
5. ‚úÖ Include in onboarding checklist for new team members

### 2. Orient to Current State (AUTOMATIC - Don't Ask)
- Read `PROGRESS.md` to identify current phase and next pending task
- Review documentation status warnings from session_manager output
- Check for stale CHAT_LOG.md (>30 min old)

### 3. Ask User One Question
"Any work completed since last session that should be marked ‚úÖ COMPLETE in PROGRESS.md?"

### 4. Offer Time-Based Maintenance (Conditional - Ask Only If Applicable)

**Note:** Steps 1-3 are AUTOMATIC (don't ask). Step 4 is CONDITIONAL (ask only if condition met).

- **If Monday or 7+ days since last update:** "Would you like me to run `make update-docs` for weekly maintenance?"
- **If 7+ days since last inventory:** "Should I run `make inventory` to update file summaries?"
- **If new AWS resources may exist:** "Should I run `make sync-progress` to check PROGRESS.md matches AWS?"
- **If .md files were modified:** "After these changes, should I run `make inventory` to update FILE_INVENTORY.md?"

---

## üìù Command Logging Workflow

**Automatically log terminal commands for debugging and documentation**

### Purpose

The `log_command.sh` script provides automatic command logging with categorization, output capture, and helper functions for documentation.

**Loaded automatically by:** `session_manager.sh start` (Session Start Workflow Step 1)

### Step 1: Load Command Logging Functions (AUTOMATIC)

**Already done in Session Start Workflow:**
```bash
source scripts/shell/session_manager.sh start
```

This sources `log_command.sh` automatically, making all logging functions available.

**Verify functions loaded:**
```bash
type log_cmd
# Should output: log_cmd is a function
```

### Step 2: Use Command Logging Functions

**Available functions:**

| Function | Purpose | Example |
|----------|---------|---------|
| `log_cmd` | Log and execute command with output | `log_cmd git status` |
| `log_note` | Add note/comment to log | `log_note "Starting Phase 3 setup"` |
| `log_solution` | Document error solution | `log_solution "Fixed by installing psycopg2-binary"` |

#### Function 1: log_cmd (Execute and Log)

**Syntax:**
```bash
log_cmd <command> [arguments...]
```

**What it does:**
1. Categorizes command by type (GIT, AWS, CONDA, PYTHON, DATABASE, MAKE, BASH)
2. Executes command
3. Captures stdout and stderr
4. Records exit code
5. Appends to COMMAND_LOG.md with timestamp

**Example usage:**
```bash
# Git commands
log_cmd git status
log_cmd git diff HEAD~1

# AWS commands
log_cmd aws s3 ls s3://nba-sim-raw-data-lake/
log_cmd aws rds describe-db-instances

# Database commands
log_cmd psql -h endpoint -U admin -d nba_sim -c "SELECT COUNT(*) FROM games"

# Python scripts
log_cmd python scripts/etl/extract_data.py

# Make targets
log_cmd make inventory
log_cmd make verify-all
```

**Log entry format:**
```markdown
## [2025-10-02 18:30:45] GIT: git status

**Command:**
```bash
git status
```

**Output:**
```
On branch main
Your branch is up to date with 'origin/main'.

nothing to commit, working tree clean
```

**Exit Code:** 0
**Duration:** 0.12s
```

#### Function 2: log_note (Add Commentary)

**Syntax:**
```bash
log_note "Your note text here"
```

**Use when:**
- Starting new phase of work
- Explaining why running commands
- Adding context for future reference
- Marking significant milestones

**Example usage:**
```bash
log_note "Beginning Phase 3: RDS setup and ETL pipeline"
log_cmd aws rds create-db-instance ...

log_note "Testing connection with sample query"
log_cmd psql -h endpoint -c "SELECT 1"

log_note "Setup complete - moving to data validation"
```

**Log entry format:**
```markdown
## [2025-10-02 18:35:20] NOTE

Beginning Phase 3: RDS setup and ETL pipeline
```

#### Function 3: log_solution (Document Error Fixes)

**Syntax:**
```bash
log_solution "Brief description of error and fix"
```

**Use when:**
- Solved an error that took >5 minutes
- Found non-obvious solution
- Want to document for future reference

**Example usage:**
```bash
log_solution "psycopg2 import error - fixed by installing psycopg2-binary instead"

log_solution "AWS credentials expired - rotated keys in IAM console"

log_solution "Conda environment conflicts - created fresh environment from environment.yml"
```

**Log entry format:**
```markdown
## [2025-10-02 18:40:15] SOLUTION

**Problem:** psycopg2 import error
**Solution:** Fixed by installing psycopg2-binary instead

**Context:**
- Error occurred during: database connection testing
- Resolution time: ~15 minutes
- Related commands: pip install psycopg2-binary
```

### Command Categories

**Auto-categorized by pattern matching:**

| Category | Patterns | Examples |
|----------|----------|----------|
| **GIT** | `git`, `gh` | git status, git commit, gh pr create |
| **AWS** | `aws`, `boto3` | aws s3 ls, aws rds describe-db-instances |
| **CONDA** | `conda`, `mamba` | conda activate, conda install |
| **PYTHON** | `python`, `pip`, `pytest` | python script.py, pip install, pytest tests/ |
| **DATABASE** | `psql`, `pg_dump`, `createdb` | psql -h endpoint, pg_dump nba_sim |
| **MAKE** | `make` | make inventory, make verify-all |
| **BASH** | All others | ls, cd, grep, find |

### Log File Location

**Primary log:** `COMMAND_LOG.md` (project root)

**Archive locations:**
- After commit: `~/sports-simulator-archives/nba/<SHA>/COMMAND_LOG.md`
- Manual backups: `COMMAND_LOG.md.backup` (after sanitization)

### Viewing Logs

**Recent commands:**
```bash
tail -50 COMMAND_LOG.md
```

**Search for specific command:**
```bash
grep -A 10 "aws s3" COMMAND_LOG.md
```

**Search for errors:**
```bash
grep -i "error\|failed" COMMAND_LOG.md
```

**Count commands by category:**
```bash
grep "##.*GIT:" COMMAND_LOG.md | wc -l
grep "##.*AWS:" COMMAND_LOG.md | wc -l
```

### Best Practices

**‚úÖ DO:**
- Use `log_cmd` for all non-trivial commands
- Add `log_note` before starting new phases
- Use `log_solution` for debugging wins
- Review COMMAND_LOG.md before committing
- Keep as permanent debugging/audit trail

**‚ùå DON'T:**
- Log passwords or secrets directly (sanitization handles this)
- Delete COMMAND_LOG.md (it's archived automatically)
- Log trivial commands (ls, cd, etc.) unless relevant
- Manually edit COMMAND_LOG.md entries (breaks formatting)

### Integration with Other Workflows

**Session Start (Step 1):**
- Functions loaded automatically
- Ready to use immediately

**Git Commit Workflow (Step 0):**
- COMMAND_LOG.md automatically sanitized before commit
- Backup created at COMMAND_LOG.md.backup

**Archive Management (after commit):**
- COMMAND_LOG.md archived to `~/sports-simulator-archives/nba/<SHA>/`
- Preserved for each commit

**Error Handling Workflow (Step 4-5):**
- Use `log_solution` to document fixes
- Helps with TROUBLESHOOTING.md entries

### Troubleshooting

**Functions not available:**
```bash
# Re-source session manager
source scripts/shell/session_manager.sh start
```

**COMMAND_LOG.md missing:**
```bash
# Create empty log
touch COMMAND_LOG.md
echo "# Command Log" > COMMAND_LOG.md
```

**Log too large (>10MB):**
```bash
# Archive current log
mv COMMAND_LOG.md COMMAND_LOG_$(date +%Y%m%d).md
touch COMMAND_LOG.md
```

**Need to find old command:**
```bash
# Search archives
grep -r "command_keyword" ~/sports-simulator-archives/nba/*/COMMAND_LOG.md
```

### Manual Command Logging (Alternative)

**If functions not available, log manually:**

```bash
# Execute command
git status

# Append to log manually
cat >> COMMAND_LOG.md << EOF

## [$(date '+%Y-%m-%d %H:%M:%S')] GIT: git status

**Command:**
\`\`\`bash
git status
\`\`\`

**Output:**
\`\`\`
$(git status 2>&1)
\`\`\`

**Exit Code:** $?

EOF
```

**But prefer using `log_cmd` for consistency**

---

## üìã Decision Workflow (When User Asks to Do Something)

Follow this decision tree **before** starting any task:

### Step 1: Check PROGRESS.md
**Question:** Is this the next pending task in PROGRESS.md?
- ‚úÖ **YES** ‚Üí Proceed to Step 2
- ‚ùå **NO** ‚Üí Go to "Plan Change Protocol" below

### Step 2: Check Prerequisites
**Question:** Are all prerequisites completed?
- ‚úÖ **YES** ‚Üí Proceed to Step 3
- ‚ùå **NO** ‚Üí Warn user: "Task X requires Y to be completed first. Should we do Y first or update the plan?"

### Step 3: Check Cost Impact
**Question:** Will this create AWS resources with ongoing costs?
- ‚úÖ **YES** ‚Üí Warn user with cost estimate, wait for explicit approval
- ‚ùå **NO** ‚Üí Proceed to Step 4

### Step 4: Check Risk Level
**Question:** Could this break something or delete data?
- ‚úÖ **YES** ‚Üí Explain risk, suggest backup/test approach, wait for approval
- ‚ùå **NO** ‚Üí Proceed to execution

---

## üîÑ Plan Change Protocol

**When user asks to skip ahead or change the plan:**

1. **Stop and clarify:**
   "This isn't the next task in PROGRESS.md. Current next task is: [X]"
   "Would you like to skip ahead or should we update the plan?"

2. **If user confirms plan change:**
   - Update PROGRESS.md first (mark current task, explain change, update dependencies)
   - Show user the changes
   - Get explicit approval: "Updated plan shows: [summary]. Proceed?"
   - Only then start the new task

3. **Document the change:**
   Add note to PROGRESS.md explaining why sequence changed

---

## üíª Task Execution Workflow (During Work)

### Step 1: Execute Task
Follow the specific workflow for the task type (see sections below)

### Step 2: Document Outcome
- If created/modified code ‚Üí Document in COMMAND_LOG.md (file, purpose, outcome)
- If solved new error ‚Üí Offer to add to TROUBLESHOOTING.md
- If made architectural decision ‚Üí Offer to create ADR

### Step 3: Wait for Confirmation
- **Don't auto-mark complete** unless task is trivial (<5 min) with clear success output
- Wait for user to say: "done", "complete", "looks good", or move to next task

### Step 4: Update PROGRESS.md
- Mark task as ‚úÖ COMPLETE
- Update any time estimates or cost actuals
- Commit the progress update

### Step 5: Suggest Next Action
"Task [X] complete. Next task in PROGRESS.md is [Y]. Should we proceed?"

---

## üìù File Creation/Modification Workflow

**CRITICAL: Follow this sequence when creating or editing files**

### 1. Create/Edit Files
Write the code or documentation

### 2. Run File Inventory (AUTOMATIC - Don't Ask)
```bash
make inventory
```
This updates FILE_INVENTORY.md with new/changed files

### 3. Document Changes in COMMAND_LOG.md
**Format:**
```markdown
## [Timestamp] File Change: [Brief Description]

**Files Created/Modified:**
- `path/to/file.py` - Purpose: [What and why]

**Problem Solved:**
[Error message or requirement being addressed]

**Outcome:**
- ‚úÖ Success / ‚ùå Failed
- [Details about what worked/didn't]
```

### 4. Review Git Status
```bash
git status
```
Verify what files changed

### 5. Proceed to Git Commit Workflow (see below)

---

## üìä Quick Repository Status Check (make git-status)

**Purpose:** Fast overview of repository state combining working tree status and recent commit history

**When to use:**
- Before starting work (see what's uncommitted from last session)
- Before committing (quick status check)
- After pulling from remote (see recent changes)
- During session to check current state
- When context-switching between branches

### Single Command

```bash
make git-status
```

**What this does (2 operations):**

#### 1. Short Git Status
Shows compact view of working tree changes:

```bash
git status --short
```

**Output format:**
```
M  docs/CLAUDE_WORKFLOW_ORDER.md    # Modified file
A  scripts/new_script.sh             # Added file (staged)
?? temp_notes.md                     # Untracked file
D  old_file.md                       # Deleted file
```

**Status codes:**
- `M` = Modified (changes staged or unstaged)
- `A` = Added (new file staged)
- `D` = Deleted (file removed)
- `??` = Untracked (not in git)
- `MM` = Modified in index and working tree (staged + unstaged changes)

#### 2. Recent Commit History
Shows last 5 commits in one-line format:

```bash
git log --oneline -5
```

**Output format:**
```
3b7521c Streamline CLAUDE.md from 436 to 216 lines
029c8f4 Mark workflow consolidation project as complete
83c35f2 Add CLAUDE.md consolidation documentation files
d0badab Implement workflow #3: Automated pre-push inspection
7d73259 Consolidate archive workflows into unified archive_manager.sh
```

### Example Output

```bash
$ make git-status
M  docs/CLAUDE_WORKFLOW_ORDER.md
?? temp_analysis.md

Recent commits:
3b7521c Streamline CLAUDE.md from 436 to 216 lines
029c8f4 Mark workflow consolidation project as complete
83c35f2 Add CLAUDE.md consolidation documentation files
d0badab Implement workflow #3: Automated pre-push inspection
7d73259 Consolidate archive workflows into unified archive_manager.sh
```

**Interpretation:**
- 1 modified file ready to stage/commit
- 1 untracked file (needs `git add` or can ignore)
- Recent work shows workflow documentation improvements

### Integration with Other Workflows

**Before Git Commit:**
```bash
# 1. Quick status check
make git-status

# 2. Review what changed
git diff docs/CLAUDE_WORKFLOW_ORDER.md

# 3. Proceed with commit workflow
git add docs/CLAUDE_WORKFLOW_ORDER.md
git commit -m "Add make git-status workflow documentation"
```

**After Session Work:**
```bash
# 1. Check what you worked on
make git-status

# 2. Review changes
git diff

# 3. Run session end checklist
bash scripts/shell/session_end.sh
```

**Before Context Switch:**
```bash
# Check if current work is committed
make git-status

# If uncommitted:
# - Either commit: git add . && git commit -m "WIP: description"
# - Or stash: git stash save "description"
```

### Comparison to Full git status

| Command | Output Length | Shows Untracked? | Shows Recent Commits? | Use Case |
|---------|---------------|------------------|----------------------|----------|
| `git status` | Verbose (15+ lines) | ‚úÖ YES (detailed) | ‚ùå NO | Full details needed |
| `git status --short` | Compact (1 line per file) | ‚úÖ YES (minimal) | ‚ùå NO | Quick file list |
| `make git-status` | Compact + history | ‚úÖ YES (minimal) | ‚úÖ YES (last 5) | **Quick overview** |
| `git log --oneline` | Commit history only | ‚ùå NO | ‚úÖ YES | Review history |

### Best Practices

**Use `make git-status` when:**
- ‚úÖ Want quick overview without verbose output
- ‚úÖ Need both current state AND recent history
- ‚úÖ Starting work session (see what's uncommitted)
- ‚úÖ Ending work session (verify everything committed)
- ‚úÖ Context-switching between features/bugs

**Use full `git status` when:**
- ‚úÖ Need detailed explanations (branch status, upstream tracking)
- ‚úÖ Want git's helpful suggestions (how to unstage, discard changes)
- ‚úÖ First time using git in a project
- ‚úÖ Debugging git issues (detached HEAD, merge conflicts)

### Troubleshooting

**Problem: Shows files you expected to be committed**
```bash
# Solution: Commit them
git add <files>
git commit -m "description"

# Verify clean
make git-status  # Should show nothing
```

**Problem: Shows untracked files you want to ignore**
```bash
# Solution: Add to .gitignore
echo "temp_notes.md" >> .gitignore
git add .gitignore
git commit -m "Add temp files to gitignore"
```

**Problem: Shows modified files you didn't change**
```bash
# Solution: Check line endings or permissions
git diff <file>  # See what changed

# If just whitespace/line endings:
git config core.autocrlf true  # Fix line endings
```

**Problem: No recent commits shown**
```bash
# Solution: This is a new repository or orphan branch
git log --all --oneline -5  # Check all branches
```

### Alternative: Manual Commands

If you prefer not to use Makefile:

```bash
# Show short status
git status --short

# Show recent commits
git log --oneline -5

# Show both with separator
git status --short && echo "" && echo "Recent commits:" && git log --oneline -5
```

### Shell Alias (Optional)

Add to `~/.bashrc` or `~/.zshrc`:

```bash
# Quick git status + recent commits
alias gst='git status --short && echo "" && echo "Recent commits:" && git log --oneline -5'
```

Then use:
```bash
gst  # Same as make git-status
```

---

## üîí Git Commit Workflow

**CRITICAL: ALWAYS follow security protocol before committing**

### Step 0: Automatic Command Log Sanitization (If COMMAND_LOG.md Staged)

**Triggered automatically by pre-commit hook when COMMAND_LOG.md is in staged files**

#### What Happens Automatically:

1. **Hook detects COMMAND_LOG.md** in `git diff --cached --name-only`
2. **Runs sanitization script:**
   ```bash
   bash scripts/shell/sanitize_command_log.sh
   ```
3. **Sanitizes 9 sensitive patterns:**

| Pattern | Sanitized To | Example |
|---------|--------------|---------|
| AWS Account IDs | `************` | `123456789012` ‚Üí `************` |
| AWS Access Keys | `AWS_ACCESS_KEY****************` | `AKIAIOSFODNN7EXAMPLE` ‚Üí `AWS_ACCESS_KEY****************` |
| AWS Secret Keys | `****************************************` | Full key ‚Üí asterisks |
| GitHub Tokens | `ghp_************************************` | `ghp_1234...` ‚Üí `ghp_****...` |
| IP Addresses | `xxx.xxx.xxx.xxx` | `192.168.1.1` ‚Üí `xxx.xxx.xxx.xxx` |
| RDS Endpoints | `<database>.xxxxxxxxxx.<region>.rds.amazonaws.com` | Sanitizes instance ID |
| Database Passwords | `********` | Any password ‚Üí asterisks |
| SSH Key Paths | `~/.ssh/` | Removes specific key names |
| Session Tokens | `AWS_SESSION_TOKEN***...` | Long tokens ‚Üí truncated |

4. **Creates backup:** `COMMAND_LOG.md.backup` (in same directory)
5. **ABORTS commit** with message: "COMMAND_LOG.md has been sanitized. Please review and re-add."
6. **You review changes:**
   ```bash
   # Compare sanitized vs original
   diff COMMAND_LOG.md.backup COMMAND_LOG.md

   # If changes look correct
   git add COMMAND_LOG.md
   git commit  # Re-run commit
   ```

#### Manual Sanitization (Optional):

**Run script manually before staging:**
```bash
bash scripts/shell/sanitize_command_log.sh
```

**Review changes:**
```bash
diff COMMAND_LOG.md.backup COMMAND_LOG.md
```

**If satisfied with sanitization:**
```bash
git add COMMAND_LOG.md
# Proceed to commit
```

#### What Gets Sanitized:

‚úÖ **Safe to commit after sanitization:**
- Command outputs with redacted credentials
- AWS CLI responses with masked IDs
- Error messages with sanitized paths
- Log entries with removed IPs

‚ùå **Still review manually:**
- Comments you added (may contain context about secrets)
- File paths that might reveal architecture
- Custom notes about configuration

#### When Sanitization Fails:

**If script reports errors:**
1. Check `COMMAND_LOG.md` for syntax issues
2. Verify backup exists: `ls -la COMMAND_LOG.md.backup`
3. Manually review and fix issues
4. Re-run sanitization script

**Emergency restore:**
```bash
cp COMMAND_LOG.md.backup COMMAND_LOG.md
```

### 1. Run Security Scan (AUTOMATIC - Don't Skip)
See `docs/SECURITY_PROTOCOLS.md` for complete bash commands

**Key checks:**
- Staged files for AWS keys, secrets, passwords, IPs
- Commit message for sensitive data
- **Note:** This runs AFTER sanitization (Step 0) to catch any remaining issues

### 2. If Security Scan Flags Anything
**STOP immediately:**
- Save diff to temp file
- Show user ALL flagged lines with context
- Explain what was detected (false positive vs real secret)
- Explain if deletions (safe) or additions (review needed)
- Ask explicitly: "Do you approve bypassing pre-commit hook? [YES/NO]"
- Wait for explicit YES or NO

### 3. If Scan Passes or User Approves
Stage files and commit:
```bash
git add .
git commit -m "$(cat <<'EOF'
Brief description of changes

Detailed explanation if needed

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
EOF
)"
```

### 4. After Commit - Update Session History
```bash
bash scripts/shell/session_manager.sh start >> .session-history.md
```
Creates version snapshot for this commit

### 4.5. Check Chat Log Before Archiving (OPTIONAL - Pre-Archive Verification)

**Purpose:** Verify CHAT_LOG.md exists and check freshness before archiving conversation

**When to use:**
- Before running Step 5 (archive_chat_by_next_sha.sh)
- When uncertain if conversation was exported recently
- To verify chat log is ready for archiving

**Script:** `scripts/shell/check_chat_log.sh`

**Usage:**
```bash
bash scripts/shell/check_chat_log.sh
```

**What this does (4 checks):**

#### Check 1: File Existence
```bash
if [ -f "CHAT_LOG.md" ]; then
  echo "‚úì CHAT_LOG.md exists"
else
  echo "‚ö†Ô∏è  CHAT_LOG.md not found"
fi
```

#### Check 2: File Age Calculation
```bash
# Calculate time since last modification
file_age_seconds=$(( $(date +%s) - $(stat -f %m CHAT_LOG.md) ))
file_age_minutes=$(( file_age_seconds / 60 ))
file_age_hours=$(( file_age_minutes / 60 ))
file_age_days=$(( file_age_hours / 24 ))
```

#### Check 3: Freshness Assessment
**Color-coded age warnings:**
| Age | Color | Message |
|-----|-------|---------|
| < 1 hour | Green | "Last updated: X minutes ago" |
| 1-24 hours | Green | "Last updated: X hours ago" |
| > 24 hours | Yellow | "Last updated: X days ago" + "‚ö†Ô∏è  Chat log may be stale" |

#### Check 4: Archive Location Reminder
```bash
echo "‚úì Ready for commit"
echo ""
echo "The pre-commit hook will archive this to:"
echo "~/sports-simulator-archives/nba/<commit-hash>/CHAT_LOG_*.md"
```

**Sample Output (Recent Chat Log):**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìù CHAT LOG CHECK
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì CHAT_LOG.md exists

  File size: 45K
  Last updated: 12 minutes ago

‚úì Ready for commit

  The pre-commit hook will archive this to:
  ~/sports-simulator-archives/nba/<commit-hash>/CHAT_LOG_*.md

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Sample Output (Stale Chat Log):**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìù CHAT LOG CHECK
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì CHAT_LOG.md exists

  File size: 23K
  Last updated: 3 days ago

‚ö†Ô∏è  Chat log may be stale
   Consider exporting your latest Claude Code conversation

‚úì Ready for commit

  The pre-commit hook will archive this to:
  ~/sports-simulator-archives/nba/<commit-hash>/CHAT_LOG_*.md

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**Sample Output (Missing Chat Log):**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìù CHAT LOG CHECK
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è  CHAT_LOG.md not found

  This commit will proceed without conversation context.

  To preserve Claude Code conversations:
  1. Export conversation from Claude Code
  2. Save as: CHAT_LOG.md in project root
  3. Re-run this check before committing

üí° Tip: Export conversations after significant work sessions

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

**What Gets Checked:**
- ‚úÖ File existence (`CHAT_LOG.md` in project root)
- ‚úÖ File size (human-readable format)
- ‚úÖ Last modification time (minutes/hours/days ago)
- ‚úÖ Staleness warning (if > 24 hours old)
- ‚úÖ Archive destination reminder

**Decision Tree After Check:**

```
CHAT_LOG.md exists and recent (<24 hours)?
‚îú‚îÄ YES ‚Üí Proceed to Step 5 (archive_chat_by_next_sha.sh)
‚îî‚îÄ NO
   ‚îú‚îÄ File missing?
   ‚îÇ  ‚îú‚îÄ Export conversation from Claude Code
   ‚îÇ  ‚îú‚îÄ Save as CHAT_LOG.md
   ‚îÇ  ‚îî‚îÄ Re-run check_chat_log.sh
   ‚îî‚îÄ File stale (>24 hours)?
      ‚îú‚îÄ Option A: Export fresh conversation and replace CHAT_LOG.md
      ‚îî‚îÄ Option B: Accept stale version and proceed to Step 5
```

**Integration Points:**
- **Session End Workflow:** Run before commit to verify conversation is captured
- **Git Commit Workflow Step 4.5:** Pre-archive verification (this location)
- **Archive Workflow Prerequisite:** Verify CHAT_LOG.md before archiving

**Comparison to Related Scripts:**

| Feature | check_chat_log.sh | save_conversation.sh | archive_chat_by_next_sha.sh |
|---------|-------------------|----------------------|-----------------------------|
| **Purpose** | Verify chat log exists/fresh | Prompt Claude to save | Archive conversation by SHA |
| **Checks existence** | ‚úÖ YES | ‚ùå NO | ‚úÖ YES (aborts if missing) |
| **Checks freshness** | ‚úÖ YES (age warnings) | ‚ùå NO | ‚ùå NO |
| **Writes files** | ‚ùå NO (read-only) | ‚ùå NO (prompts Claude) | ‚úÖ YES (4 archive files) |
| **User action** | Informational check | Triggers conversation save | Archives existing chat log |
| **When to use** | Before archiving | During session (save now) | After commit (link to SHA) |

**Best Practice Workflow:**

```bash
# 1. Complete significant work
# 2. Save conversation (if not already saved)
bash scripts/shell/save_conversation.sh
# (Claude writes CHAT_LOG.md)

# 3. Commit changes
git add .
git commit -m "Implement feature X"

# 4. Check chat log before archiving
bash scripts/shell/check_chat_log.sh
# ‚úì CHAT_LOG.md exists
# ‚úì File size: 45K
# ‚úì Last updated: 2 minutes ago

# 5. Archive conversation linked to commit
bash scripts/maintenance/archive_chat_by_next_sha.sh
# Creates 4 files:
#   - chat-<SHA>-original.md
#   - chat-<SHA>-sanitized.md
#   - chat-<SHA>-spoofed.md
#   - mapping-<SHA>.txt
```

**Why This Step is Optional:**

The check is informational only - `archive_chat_by_next_sha.sh` (Step 5) will abort if CHAT_LOG.md is missing, so this verification step helps catch issues earlier but isn't strictly required.

Use this step when:
- ‚úÖ Want to verify freshness before archiving
- ‚úÖ Uncertain if conversation was exported recently
- ‚úÖ Multiple sessions worked on same commit
- ‚úÖ Debugging why archiving failed

Skip this step when:
- ‚è≠Ô∏è Just saved conversation (know it's fresh)
- ‚è≠Ô∏è Confident CHAT_LOG.md exists and is current
- ‚è≠Ô∏è Not planning to archive (routine commits)

### 5. Archive Conversation by Commit SHA (OPTIONAL - If CHAT_LOG.md Exists)

**Purpose:** Archive conversation linked to specific commit, creating 3 security-level versions with optional GitHub publishing

**When to use:**
- After completing significant feature/phase with valuable conversation
- Want conversation explicitly linked to specific commit SHA
- Planning to publish sanitized or spoofed conversation publicly

**Script:** `scripts/maintenance/archive_chat_by_next_sha.sh`

**Prerequisites:**
1. CHAT_LOG.md exists in project root (written by Claude or via `save_conversation.sh`)
2. Changes already committed (script uses current HEAD SHA)

**Usage:**
```bash
# After commit, archive conversation linked to that commit
bash scripts/maintenance/archive_chat_by_next_sha.sh
```

**What this does (7 steps):**

#### Step 1: Detect Current Commit
```bash
CURRENT_SHA=$(git rev-parse HEAD)
SHORT_SHA=$(git rev-parse --short HEAD)
COMMIT_MSG=$(git log -1 --pretty=%B)
COMMIT_DATE=$(git log -1 --format=%cd --date=iso)
```

#### Step 2: Check for CHAT_LOG.md
```bash
if [ ! -f "$PROJECT_DIR/CHAT_LOG.md" ]; then
    echo "‚ùå No CHAT_LOG.md found"
    echo "Export conversation first, then run this script"
    exit 1
fi
```

#### Step 3: Create ORIGINAL Archive
```bash
# Archive location: ~/sports-simulator-archives/nba/conversations/
# Filename: chat-<FULL_SHA>-original.md

# Adds header with:
# - Full SHA
# - Commit date
# - Author
# - Commit message
# - Full conversation content
```

**Example ORIGINAL archive header:**
```markdown
# Conversation Archive: Commit abc1234

**Full SHA:** abc1234567890abcdef1234567890abcdef1234
**Date:** 2025-10-02 19:30:45 -0700
**Author:** Developer Name <email@example.com>

## Commit Message

```
Add weekly documentation update workflow

Integrated update_docs.sh script with 7 automated steps...

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
```

## Conversation

This conversation captures the work that led to the above commit.

---

[Full conversation follows...]
```

#### Step 4: Create SANITIZED Version
**Redacts credentials only (keeps real paths):**

| Pattern | Redacted To |
|---------|-------------|
| `AWS_ACCESS_KEY[A-Z0-9]{16}` | `AWS_ACCESS_KEY[REDACTED]` |
| `aws_secret_access_key=...` | `aws_secret_access_key=[REDACTED]` |
| `aws_session_token=...` | `aws_session_token=[REDACTED]` |
| `Password: <value>` | `Password: [REDACTED]` |
| `Bearer <token>` | `Bearer [REDACTED]` |
| `ghp_<36chars>` | `ghp_[REDACTED]` |
| `github_pat_<token>` | `github_pat_[REDACTED]` |
| `postgresql://user:pass@` | `postgresql://[USER]:[REDACTED]@` |
| `postgres:pass@` | `postgres:[REDACTED]@` |

**Output:** `chat-<SHA>-sanitized.md` (safe for private repos)

#### Step 5: Create SPOOFED Version
**Generates fake paths for public sharing:**

**Path transformations:**
```python
# Real paths ‚Üí Spoofed paths
/Users/ryanranft/nba-simulator-aws/scripts/shell/update_docs.sh
  ‚Üí project/scripts/script_a3f2b891.sh

~/sports-simulator-archives/nba/conversations/chat_20251002.md
  ‚Üí /archive/external/doc_f72a89bc.md

CLAUDE.md
  ‚Üí project/doc_e4b1c332.md
```

**Content transformations:**
| Real | Spoofed |
|------|---------|
| `/Users/ryanranft/` | `/Users/developer/` |
| `~/` | `/Users/developer/` |
| `<email@domain.com>` | `<developer@example.com>` |
| `Author: Real Name` | `Author: Developer` |
| `nba-simulator-aws` | `sports-data-pipeline` |
| `nba-sim-raw-data-lake` | `sports-raw-data-bucket` |
| `nba` / `NBA` | `sport` / `SPORT` |

**Output:** `chat-<SHA>-spoofed.md` (safe for public sharing)

#### Step 6: Create Path Mapping File
**For reversing spoofed paths back to real:**

```
# Path Mapping for Commit abc1234
# Created: chat-abc1234-sanitized.md
#
# REAL PATH ‚Üí SPOOFED PATH
#
# SECURITY: This file contains real paths - NEVER commit to git!
# Location: ~/sports-simulator-archives/nba/conversations/mappings/mapping-abc1234.txt
#======================================================================

/Users/ryanranft/nba-simulator-aws/CLAUDE.md
  ‚Üí project/doc_e4b1c332.md

/Users/ryanranft/nba-simulator-aws/scripts/shell/update_docs.sh
  ‚Üí project/scripts/script_a3f2b891.sh

~/sports-simulator-archives/nba/conversations/chat_20251002.md
  ‚Üí /archive/external/doc_f72a89bc.md

Total paths spoofed: 47
```

**‚ö†Ô∏è NEVER commit mapping file - contains real paths!**

#### Step 7: Optional GitHub Publishing
**Prompts to publish SPOOFED version to public GitHub repo:**

```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üåê PUBLISH TO GITHUB
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Publish spoofed conversation to public GitHub repo? [y/N]: y

  üì§ Publishing spoofed conversation to GitHub...
  ‚úÖ Published to: https://github.com/ryanranft/nba-simulator-conversations
  üìÑ View file: https://github.com/.../chat-abc1234-spoofed.md
```

**If yes:**
- Commits spoofed file to archive repo
- Includes commit message with link to main repo commit
- Pushes to GitHub automatically

**If no:**
- Provides manual publish commands
- Files remain local-only

**Script output summary:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìù ARCHIVING CONVERSATION FOR COMMIT
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Commit: abc1234
Date:   2025-10-02 19:30:45 -0700
Author: Developer Name <email@example.com>

Message:
  Add weekly documentation update workflow

  Integrated update_docs.sh script with 7 automated steps...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  üìù Creating archive for SHA: abc1234
  üîí Creating sanitized version...
  üé≠ Creating spoofed version (public-safe)...
Spoofed 47 file paths

‚úÖ Conversation archived successfully!

üìÇ Files created:
   Original:  ~/sports-simulator-archives/nba/conversations/chat-abc1234...-original.md
   Sanitized: ~/sports-simulator-archives/nba/conversations/chat-abc1234...-sanitized.md
   Spoofed:   ~/sports-simulator-archives/nba/conversations/chat-abc1234...-spoofed.md
   Mapping:   ~/sports-simulator-archives/nba/conversations/mappings/mapping-abc1234....txt

‚ö†Ô∏è  Security Note:
   - ORIGINAL: Contains passwords - keep local only
   - SANITIZED: Safe to share or commit to private repo
   - SPOOFED: Safe to share publicly (fake paths, no credentials)
   - MAPPING: Real‚ÜíSpoofed paths - NEVER commit (local only)

üí° You can now delete CHAT_LOG.md from the project root:
   rm /Users/ryanranft/nba-simulator-aws/CHAT_LOG.md

üîç To view conversations:
   less ~/sports-simulator-archives/nba/conversations/chat-abc1234...-sanitized.md
   less ~/sports-simulator-archives/nba/conversations/chat-abc1234...-spoofed.md

üìã Archive Index:
   Total conversations archived: 12
```

**Archive file naming convention:**
```
chat-<FULL_40_CHAR_SHA>-original.md
chat-<FULL_40_CHAR_SHA>-sanitized.md
chat-<FULL_40_CHAR_SHA>-spoofed.md
mapping-<FULL_40_CHAR_SHA>.txt
```

**Finding archived conversation by commit:**
```bash
# By short SHA
ls ~/sports-simulator-archives/nba/conversations/chat-abc1234*

# By commit message keyword
grep -l "documentation update" ~/sports-simulator-archives/nba/conversations/*.md

# View specific version
less ~/sports-simulator-archives/nba/conversations/chat-<SHA>-sanitized.md
```

**Security comparison:**

| Version | Real Paths? | Credentials? | Safe for Private Repo? | Safe for Public? |
|---------|-------------|--------------|------------------------|------------------|
| **ORIGINAL** | ‚úÖ YES | ‚úÖ YES | ‚ùå NO | ‚ùå NO |
| **SANITIZED** | ‚úÖ YES | ‚ùå NO | ‚úÖ YES | ‚ùå NO |
| **SPOOFED** | ‚ùå NO (fake) | ‚ùå NO | ‚úÖ YES | ‚úÖ YES |
| **MAPPING** | ‚úÖ YES | ‚ùå NO | ‚ùå NO | ‚ùå NO |

**Best practices:**
1. ‚úÖ Use SANITIZED for team collaboration (private repos)
2. ‚úÖ Use SPOOFED for portfolios, presentations, public demos
3. ‚úÖ Keep ORIGINAL and MAPPING local-only (never push to GitHub)
4. ‚úÖ Run after commits you want publicly documented
5. ‚úÖ Delete CHAT_LOG.md after archiving (prevent accidental commit)

**Comparison to archive_manager.sh conversation mode:**

| Feature | archive_manager.sh | archive_chat_by_next_sha.sh |
|---------|-------------------|----------------------------|
| **Trigger** | Manual/automatic | Manual after commit |
| **Naming** | Timestamp-based | Commit SHA-based |
| **Filename** | `CHAT_LOG_<timestamp>.md` | `chat-<SHA>-<version>.md` |
| **Versions** | 3 (ORIGINAL/SANITIZED/SPOOFED) | 3 (ORIGINAL/SANITIZED/SPOOFED) |
| **Path mapping** | No | Yes (separate mapping file) |
| **GitHub publish** | No | Yes (interactive prompt) |
| **Commit linkage** | None | Explicit SHA linkage |
| **Use case** | General archiving | Commit-specific documentation |

**When to use each:**

**Use `archive_manager.sh conversation`:**
- ‚úÖ During session at 75%/90% context
- ‚úÖ Session end with uncommitted work
- ‚úÖ General conversation preservation

**Use `archive_chat_by_next_sha.sh`:**
- ‚úÖ After significant feature commit
- ‚úÖ Want conversation linked to specific commit
- ‚úÖ Planning to publish publicly
- ‚úÖ Need path mapping for reconstruction

### 6. Archive Management (RECOMMENDED - Run After Every Commit)

**Purpose:** Preserve operational files and conversations for each commit in dedicated archive repository

```bash
bash scripts/maintenance/archive_manager.sh full
```

**See complete Archive Management Workflow below for detailed usage**

---

## üì¶ Archive Management Workflow

**Unified archiving system for preserving operational files and conversations**

### When to Run

- ‚úÖ **After every commit** (recommended: `full` mode)
- ‚úÖ **Before session end** (if changes uncommitted)
- ‚úÖ **At 90% context** (before losing conversation)
- ‚úÖ **After significant work** (>2 hours)

### Available Modes

The `archive_manager.sh` script has 5 modes:

| Mode | Purpose | When to Use |
|------|---------|-------------|
| `gitignored` | Archive .gitignored operational files | After updating COMMAND_LOG.md, temp docs |
| `conversation` | Archive CHAT_LOG.md (3 versions) | At 75%/90% context, session end |
| `analyze` | Generate commit analysis logs | After major commits, debugging |
| `full` | Run all 3 modes in sequence | **RECOMMENDED** after every commit |
| `status` | Check archive status for commit | Verify archiving worked |

### Full Archive Workflow (Recommended)

**Step 1: Run Full Archive**

```bash
bash scripts/maintenance/archive_manager.sh full
```

**What this does:**

1. **Archives .gitignored operational files:**
   - `COMMAND_LOG.md` - Command history with outputs
   - `COMMIT_VERIFICATION.md` - Pre-commit verification results
   - `EXTRACTION_STATUS.md` - S3 data extraction status
   - `.session-history.md` - Session diagnostics snapshots
   - Any other files in `.gitignore` that are tracked locally

2. **Archives CHAT_LOG.md in 3 versions:**
   - **ORIGINAL:** `CHAT_LOG.md` - Full conversation with real paths/IPs
     - ‚ö†Ô∏è **Contains passwords/IPs** - local archive only, never push
   - **SANITIZED:** `CHAT_LOG_SANITIZED.md` - Credentials removed
     - ‚úÖ **Safe for private GitHub repos** - team sharing OK
   - **SPOOFED:** `CHAT_LOG_SPOOFED.md` - Fake paths for demonstration
     - ‚úÖ **Safe for public sharing** - portfolio/demos

3. **Generates commit analysis logs:**
   - `ERRORS_LOG.md` - All errors from COMMAND_LOG.md
   - `CHANGES_SUMMARY.md` - Git diff analysis, files changed
   - `COMMAND_HISTORY.md` - Extracted command-only list

4. **Commits to archive git repository:**
   - Archive location: `~/sports-simulator-archives/nba/<commit-SHA>/`
   - Archive repo: `~/sports-simulator-archives/nba/.git`
   - Each commit creates timestamped directory

**Step 2: Verify Archive Created**

```bash
bash scripts/maintenance/archive_manager.sh status
```

**Expected output:**
```
‚úÖ Archive exists for commit <SHA>
üìÅ Location: ~/sports-simulator-archives/nba/<SHA>/
üìÑ Files archived: 8 files
üìä Total size: 2.4 MB
```

### Individual Mode Usage

#### Mode 1: Archive Gitignored Files Only

```bash
bash scripts/maintenance/archive_manager.sh gitignored
```

**Use when:**
- Updated COMMAND_LOG.md significantly
- Changed operational documentation
- Want to preserve temp files before cleanup

**What gets archived:**
- All files matching `.gitignore` patterns
- Preserves directory structure
- Creates timestamped snapshot

#### Mode 2: Archive Conversation Only

```bash
bash scripts/maintenance/archive_manager.sh conversation
```

**Use when:**
- Approaching context limits (75%/90%)
- Ending session with valuable conversation
- Want to preserve conversation without full archive

**Creates 3 versions:**
1. **ORIGINAL** - `~/sports-simulator-archives/nba/conversations/CHAT_LOG_<timestamp>.md`
   - Full conversation, real credentials/paths
   - **NEVER commit this version to GitHub**

2. **SANITIZED** - `~/sports-simulator-archives/nba/conversations/CHAT_LOG_SANITIZED_<timestamp>.md`
   - Credentials sanitized (AWS keys ‚Üí asterisks)
   - IPs sanitized (192.168.1.1 ‚Üí xxx.xxx.xxx.xxx)
   - File paths sanitized (removes sensitive info)
   - **Safe for private repo sharing**

3. **SPOOFED** - `~/sports-simulator-archives/nba/conversations/CHAT_LOG_SPOOFED_<timestamp>.md`
   - Fake project name, fake paths
   - Demonstrates workflow without exposing details
   - **Safe for public portfolio/presentations**

#### Mode 3: Analyze Commit Only

```bash
bash scripts/maintenance/archive_manager.sh analyze
```

**Use when:**
- Debugging errors from recent work
- Need quick command history review
- Want to extract error patterns

**Generates:**
- `ERRORS_LOG.md` - Grep for "error", "failed", "ERROR" in COMMAND_LOG.md
- `CHANGES_SUMMARY.md` - Git diff stats, changed files, additions/deletions
- `COMMAND_HISTORY.md` - All commands executed (no output, just commands)

### Archive Structure

```
~/sports-simulator-archives/nba/
‚îú‚îÄ‚îÄ <commit-SHA>/                    # Full archive for each commit
‚îÇ   ‚îú‚îÄ‚îÄ COMMAND_LOG.md               # Operational command log
‚îÇ   ‚îú‚îÄ‚îÄ COMMIT_VERIFICATION.md       # Pre-commit checks
‚îÇ   ‚îú‚îÄ‚îÄ EXTRACTION_STATUS.md         # S3 extraction status
‚îÇ   ‚îú‚îÄ‚îÄ .session-history.md          # Session diagnostics
‚îÇ   ‚îú‚îÄ‚îÄ ERRORS_LOG.md                # Extracted errors
‚îÇ   ‚îú‚îÄ‚îÄ CHANGES_SUMMARY.md           # Git diff analysis
‚îÇ   ‚îî‚îÄ‚îÄ COMMAND_HISTORY.md           # Command-only list
‚îú‚îÄ‚îÄ conversations/                   # Conversation archives
‚îÇ   ‚îú‚îÄ‚îÄ CHAT_LOG_<timestamp>.md              # ORIGINAL (local only)
‚îÇ   ‚îú‚îÄ‚îÄ CHAT_LOG_SANITIZED_<timestamp>.md    # SANITIZED (private repo OK)
‚îÇ   ‚îî‚îÄ‚îÄ CHAT_LOG_SPOOFED_<timestamp>.md      # SPOOFED (public OK)
‚îú‚îÄ‚îÄ backups/                         # make backup snapshots
‚îÇ   ‚îî‚îÄ‚îÄ backup-YYYYMMDD-HHMMSS/
‚îú‚îÄ‚îÄ deleted-files-*/                 # Pre-deletion archives
‚îî‚îÄ‚îÄ pre-push-cleanup-*/              # Pre-push removed files
```

### Archive Git Repository

**The archive is also a git repository:**

```bash
cd ~/sports-simulator-archives/nba
git log --oneline | head -20
```

**Benefits:**
- Track archive history over time
- Search conversations by commit message
- Recover conversations by SHA
- See evolution of operational files

### Finding Archived Conversations

**Method 1: By date**
```bash
ls -lt ~/sports-simulator-archives/nba/conversations/ | head -10
```

**Method 2: By commit SHA**
```bash
ls -lt ~/sports-simulator-archives/nba/ | grep <partial-SHA>
```

**Method 3: By content** (git grep in archive repo)
```bash
cd ~/sports-simulator-archives/nba
git grep "<keyword>" | head -20
```

**Method 4: By archive git history**
```bash
cd ~/sports-simulator-archives/nba
git log --all --grep="<keyword>" --oneline
```

### Emergency Recovery

**If CHAT_LOG.md lost before archiving:**

1. **Check if conversation archived:**
   ```bash
   bash scripts/maintenance/archive_manager.sh status
   ```

2. **If exists, copy from archive:**
   ```bash
   cp ~/sports-simulator-archives/nba/<SHA>/CHAT_LOG.md CHAT_LOG.md
   ```

3. **If not archived, check recent conversations:**
   ```bash
   ls -lt ~/sports-simulator-archives/nba/conversations/ | head -5
   cp ~/sports-simulator-archives/nba/conversations/CHAT_LOG_<recent>.md CHAT_LOG.md
   ```

### Security Notes

**‚ö†Ô∏è CRITICAL - 3 Versions, Different Security Levels:**

| Version | Contains Secrets? | Where to Store? | Safe for GitHub? |
|---------|------------------|-----------------|------------------|
| **ORIGINAL** | ‚úÖ YES (real IPs, paths, maybe passwords) | Local archive only | ‚ùå NEVER |
| **SANITIZED** | ‚ùå NO (sanitized) | Private repos OK | ‚úÖ Private only |
| **SPOOFED** | ‚ùå NO (fake data) | Anywhere | ‚úÖ Public OK |

**Best practices:**
1. ‚úÖ Keep ORIGINAL in `~/sports-simulator-archives/` (never commit)
2. ‚úÖ Share SANITIZED with team in private repos
3. ‚úÖ Use SPOOFED for portfolios, presentations, public demos
4. ‚ùå NEVER commit ORIGINAL to any repository

### Integration with Other Workflows

**Git Commit Workflow (Step 5):**
- After commit succeeds ‚Üí Run `full` mode
- Preserves state for this commit

**Session End Workflow:**
- Before ending session ‚Üí Check if conversation archived
- If not ‚Üí Run `conversation` mode

**Context Management (At 90%):**
- Save conversation ‚Üí Run `conversation` mode
- Archive before starting new conversation

### Component Scripts (Consolidated into archive_manager.sh)

**IMPORTANT:** `archive_manager.sh` is a **unified wrapper** that consolidates 4 original standalone scripts into a single convenient interface. The original scripts still exist for backwards compatibility and specific use cases, but **you should prefer `archive_manager.sh`** for all archiving tasks.

#### Original Component Scripts:

**1. `scripts/maintenance/archive_gitignored_files.sh`** (Standalone version of `gitignored` mode)
- **Purpose:** Archives .gitignored operational files to SHA-based directory
- **Creates:** `git-info.txt`, copies COMMAND_LOG.md, EXTRACTION_STATUS.md, log files
- **Use when:** Need ONLY gitignored file archiving (no conversation, no analysis)
- **Consolidated into:** `archive_manager.sh gitignored` mode

**Direct usage** (if not using archive_manager.sh):
```bash
bash scripts/maintenance/archive_gitignored_files.sh
```

**2. `scripts/maintenance/archive_chat_log.sh`** (Standalone version of `conversation` mode)
- **Purpose:** Archives CHAT_LOG.md in 2 versions (original + sanitized)
- **Creates:** `CHAT_LOG_ORIGINAL.md` (local only), `CHAT_LOG_SANITIZED.md` (safe for sharing)
- **Use when:** Need ONLY conversation archiving (no gitignored files, no analysis)
- **Consolidated into:** `archive_manager.sh conversation` mode

**Direct usage** (if not using archive_manager.sh):
```bash
bash scripts/maintenance/archive_chat_log.sh
```

**3. `scripts/maintenance/generate_commit_logs.sh`** (Standalone version of `analyze` mode)
- **Purpose:** Generates 3 analysis logs from COMMAND_LOG.md and git history
- **Creates:** `ERRORS_LOG.md`, `CHANGES_SUMMARY.md`, `COMMAND_HISTORY.md`
- **Use when:** Need ONLY commit analysis (no archiving)
- **Consolidated into:** `archive_manager.sh analyze` mode

**Direct usage** (if not using archive_manager.sh):
```bash
bash scripts/maintenance/generate_commit_logs.sh
```

**4. `scripts/maintenance/create_sanitized_archive.sh`** (Original sanitization utility)
- **Purpose:** Creates sanitized version of any markdown file (removes credentials)
- **Use when:** Need to sanitize a specific file (not part of full archive workflow)
- **Note:** Used internally by archive_manager.sh's `sanitize_credentials()` function

**Direct usage** (if sanitizing custom files):
```bash
bash scripts/maintenance/create_sanitized_archive.sh <input.md> <output.md>
```

#### Related Scripts (Not Consolidated):

**5. `scripts/maintenance/archive_chat_by_next_sha.sh`** (Alternative conversation workflow)
- **Purpose:** Archives conversation AFTER commit, naming by the NEW commit SHA
- **Creates:** 3 versions (original, sanitized, spoofed) + path mapping file
- **Use when:** Want conversation archived with the commit SHA it describes (instead of current SHA)
- **Different from `archive_manager.sh conversation`:** Creates 3rd "spoofed" version with fake paths for public sharing

**Usage:**
```bash
# 1. Export Claude Code conversation to CHAT_LOG.md
# 2. Stage and commit your changes
# 3. Run this script AFTER commit
bash scripts/maintenance/archive_chat_by_next_sha.sh
```

**Comparison:**

| Feature | archive_manager.sh conversation | archive_chat_by_next_sha.sh |
|---------|--------------------------------|----------------------------|
| **When to run** | Before or after commit | AFTER commit only |
| **SHA used** | Current commit SHA | Next commit SHA |
| **Versions created** | 2 (original, sanitized) | 3 (original, sanitized, spoofed) |
| **Path spoofing** | No | Yes (fake paths for public sharing) |
| **Mapping file** | No | Yes (maps real paths to spoofed paths) |
| **Public sharing** | Sanitized only | Spoofed version safe for public |

**Recommendation:**
- Use `archive_manager.sh conversation` for routine archiving after every commit
- Use `archive_chat_by_next_sha.sh` when creating public portfolio/demo materials

### Why Use archive_manager.sh Instead of Individual Scripts?

**Benefits of unified wrapper:**
1. **Single command:** Run all 3 archiving operations with `full` mode
2. **Consistent interface:** One script, 5 modes, clear naming
3. **Automatic git commits:** Commits to archive repo after each mode
4. **Status checking:** Built-in `status` mode to verify archiving succeeded
5. **Error handling:** Gracefully handles missing files, continues with warnings
6. **Shared functions:** DRY code, consistent sanitization logic

**When to use individual scripts:**
- **Debugging:** Need to test specific archiving step in isolation
- **Custom workflows:** Building your own archiving automation
- **Backwards compatibility:** Existing scripts/aliases reference old script names
- **Special cases:** Need only one specific operation without others

**Example scenarios:**

**Scenario 1: After every commit (recommended)**
```bash
# Single command archives everything
bash scripts/maintenance/archive_manager.sh full
```

**Scenario 2: Quick conversation save at 90% context**
```bash
# Just archive conversation, skip gitignored files and analysis
bash scripts/maintenance/archive_manager.sh conversation
```

**Scenario 3: Debugging errors from yesterday's work**
```bash
# Generate analysis logs only (assumes archive already exists)
bash scripts/maintenance/archive_manager.sh analyze
```

**Scenario 4: Creating public demo (advanced)**
```bash
# Use standalone script for spoofed version + mapping
bash scripts/maintenance/archive_chat_by_next_sha.sh
```

---

## üöÄ Git Push Workflow - Complete Pre-Push Inspection

**CRITICAL: ALWAYS follow pre-push inspection before pushing**

### Overview: 7 Available Modes

The `pre_push_inspector.sh` script has 7 modes for different inspection needs:

| Mode | Purpose | Interactive? | When to Use |
|------|---------|--------------|-------------|
| `security-scan` | 15-point security audit | No | Quick security check only |
| `inspect-repo` | Scan for local-only files | No | Check what shouldn't be pushed |
| `recommend` | Show recommendations | No | See what needs cleanup |
| `archive-cleanup` | Archive then remove files | Yes | Preserve before deletion |
| `cleanup-repo` | Remove from git tracking | Yes | Clean repo before push |
| `full` | Complete interactive workflow | Yes | **RECOMMENDED** - all 7 steps |
| `status` | Dry-run preview | No | See what would happen |

### Recommended Workflow: Full Mode (Interactive)

**Step 1: Run Full Pre-Push Inspector**

```bash
bash scripts/shell/pre_push_inspector.sh full
```

**This runs all 7 steps automatically:**

#### **Step 1/7: Security Scan (Automatic)**

**What it checks (15 security points):**
1. Git history for AWS keys (all commits)
2. Git history for database passwords
3. Git history for private IP addresses
4. Staged files for AWS access keys
5. Staged files for AWS secret keys
6. Staged files for AWS session tokens
7. Staged files for private IPs (192.168.x.x, 10.x.x.x, 172.16-31.x.x)
8. Staged files for database connection strings
9. Staged files for SSH private keys
10. Staged files for GitHub tokens (ghp_*, github_pat_*)
11. Commit messages for sensitive data
12. `.env` files in staged changes
13. `credentials.yaml` in staged changes
14. AWS CLI config files
15. Database dump files

**If security issues found:**
- Script **STOPS immediately**
- Shows flagged lines with context
- Asks: "CRITICAL security violations found. Abort push? [Y/n]"
- **Recommend:** Fix issues, don't bypass

#### **Step 2/7: Repository Inspection (Automatic)**

**Scans for local-only files not suitable for GitHub:**

**Categories checked:**
1. **üî¥ HIGH PRIORITY** - Operational files (COMMAND_LOG.md, EXTRACTION_STATUS.md, etc.)
2. **üü° MEDIUM PRIORITY** - Log files (*.log, debug output)
3. **üü¢ LOW PRIORITY** - Temp docs (.session-history.md, temp notes)
4. **‚öôÔ∏è CONFIG** - Local config (.env, credentials)
5. **üìä DATA** - Data files (CSVs, SQLite, large files >10MB)

**Output example:**
```
üî¥ HIGH PRIORITY (operational):
  - COMMAND_LOG.md (contains command history)
  - COMMIT_VERIFICATION.md (pre-commit results)

üü° MEDIUM PRIORITY (logs):
  - error.log (may contain sensitive info)

üü¢ LOW PRIORITY (temp):
  - .session-history.md (session diagnostics)
```

#### **Step 3/7: Present Recommendations (Automatic)**

**Shows organized recommendations:**

```
RECOMMENDATIONS:

Archive before pushing:
  - COMMAND_LOG.md (operational history)
  - error.log (debugging info)

Add to .gitignore:
  - *.log
  - .session-history.md

Safe to keep (already gitignored):
  - .env
  - credentials.yaml
```

#### **Step 4/7: User Confirmation (Interactive)**

**Asks for cleanup decision:**
```
Files flagged for cleanup. Choose action:
  [1] YES - Archive and remove all flagged files
  [2] NO - Skip cleanup, push as-is
  [3] SPECIFY - Choose specific files to clean

Your choice:
```

**Options:**
- **YES** ‚Üí Proceeds to archive-cleanup (Step 5)
- **NO** ‚Üí Skips to push approval (Step 7)
- **SPECIFY** ‚Üí Prompts for file list, then archives specified files

#### **Step 5/7: Archive Before Deletion (Automatic if confirmed)**

**Creates preservation archive:**

```bash
ARCHIVE_DIR=~/sports-simulator-archives/nba/pre-push-cleanup-$(date +%Y%m%d-%H%M%S)
mkdir -p "$ARCHIVE_DIR"

# Copy all flagged files
for file in "${flagged_files[@]}"; do
  cp "$file" "$ARCHIVE_DIR/"
done

# Create cleanup record
cat > "$ARCHIVE_DIR/CLEANUP_RECORD.md" << EOF
# Pre-Push Cleanup Record

**Date:** $(date)
**Commit:** $(git rev-parse HEAD)
**Files Removed:** ${#flagged_files[@]} files
**Reason:** Pre-push repository cleanup
**Recovery:** Files archived at $ARCHIVE_DIR

## Files Removed:
$(printf '- %s\n' "${flagged_files[@]}")
EOF
```

**Archive location:** `~/sports-simulator-archives/nba/pre-push-cleanup-<timestamp>/`

#### **Step 6/7: Remove from Repository (Automatic if confirmed)**

**Removes files from git tracking:**

```bash
# Remove from git (keeps local copy)
for file in "${flagged_files[@]}"; do
  git rm --cached "$file"
  echo "$file" >> .gitignore
done

# Commit cleanup
git add .gitignore
git commit -m "Pre-push cleanup: Remove local-only files

Archived to: $ARCHIVE_DIR

Files removed from tracking:
$(printf '- %s\n' "${flagged_files[@]}")

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>"
```

**What happens:**
- Files removed from git tracking (`git rm --cached`)
- Files stay in local working directory
- Files added to .gitignore
- Cleanup commit created automatically

#### **Step 7/7: Final Push Approval (Interactive)**

**Asks for explicit push approval:**
```
Pre-push inspection complete.

Summary:
  ‚úÖ Security scan passed
  ‚úÖ 5 files archived and removed
  ‚úÖ .gitignore updated
  ‚úÖ Repository cleaned

Ready to push to GitHub? [y/N]:
```

**Options:**
- **y** ‚Üí Executes `git push`
- **N** ‚Üí Exits without pushing (default)

---

### Individual Mode Usage

#### Mode 1: Security Scan Only

```bash
bash scripts/shell/pre_push_inspector.sh security-scan
```

**Use when:**
- Quick security check before push
- Don't need repository cleanup
- Just want to verify no secrets

**What it does:**
- Runs 15-point security audit
- Reports findings
- Exits (no cleanup, no push)

**Exit codes:**
- `0` - No security issues found
- `1` - Security violations detected

#### Mode 2: Repository Inspection Only

```bash
bash scripts/shell/pre_push_inspector.sh inspect-repo
```

**Use when:**
- Want to see local-only files
- Planning cleanup strategy
- Not ready to make changes

**What it does:**
- Scans repository for local-only files
- Categorizes by priority (HIGH/MEDIUM/LOW)
- Reports findings
- Exits (no changes made)

#### Mode 3: Recommendations Only

```bash
bash scripts/shell/pre_push_inspector.sh recommend
```

**Use when:**
- Want actionable cleanup suggestions
- Planning manual cleanup
- Understanding what needs fixing

**What it does:**
- Runs inspection (Mode 2)
- Generates recommendations
- Groups by action type (archive, ignore, keep)
- Exits (no changes made)

#### Mode 4: Archive and Cleanup

```bash
bash scripts/shell/pre_push_inspector.sh archive-cleanup
```

**Use when:**
- Ready to clean repository
- Want automatic archiving
- Interactive file selection

**What it does:**
- Runs inspection
- Shows recommendations
- Asks which files to clean
- Archives selected files
- Does NOT remove from git (use Mode 5 for that)

#### Mode 5: Remove from Git Tracking

```bash
bash scripts/shell/pre_push_inspector.sh cleanup-repo
```

**Use when:**
- Files already archived (Mode 4)
- Ready to remove from git
- Want cleanup commit

**What it does:**
- Assumes files already backed up
- Runs `git rm --cached` on specified files
- Updates .gitignore
- Creates cleanup commit
- Does NOT push (manual push required)

**‚ö†Ô∏è WARNING:** This modifies git tracking. Ensure files are archived first (Mode 4).

#### Mode 6: Full Workflow (Recommended)

```bash
bash scripts/shell/pre_push_inspector.sh full
```

**Already documented above - runs all 7 steps**

#### Mode 7: Dry-Run Status Check

```bash
bash scripts/shell/pre_push_inspector.sh status
```

**Use when:**
- Want to preview full workflow
- Not ready to make changes
- Testing inspector behavior

**What it does:**
- Runs security scan (no stopping)
- Runs repository inspection
- Shows recommendations
- Shows what WOULD be archived
- Shows what WOULD be removed
- **Does NOT make any changes**
- **Does NOT prompt for input**

**Output:**
```
DRY RUN - No changes will be made

Security Scan Results:
  ‚úÖ No security violations found

Repository Inspection:
  üî¥ 5 files flagged for cleanup
  üü° 2 log files found
  üü¢ 3 temp files found

WOULD archive:
  - COMMAND_LOG.md
  - error.log
  - .session-history.md

WOULD add to .gitignore:
  - *.log
  - .session-history.md

Status check complete. Run with 'full' mode to execute.
```

---

### Workflow Decision Guide

**Choose your mode based on situation:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Are you ready to push to GitHub?               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ
        YES  ‚îÇ  NO
             ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                 ‚îÇ
    v                 v
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Run 'full'‚îÇ   ‚îÇ Just checking‚îÇ
‚îÇ mode      ‚îÇ   ‚îÇ status?      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                  YES  ‚îÇ  NO
                       ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ                   ‚îÇ
              v                   v
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ Run      ‚îÇ      ‚îÇ Need specific‚îÇ
        ‚îÇ 'status' ‚îÇ      ‚îÇ action?      ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                            ‚îÇ         ‚îÇ
                            v         v
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇsecurity-scan‚îÇ ‚îÇinspect-repo ‚îÇ
                    ‚îÇrecommend    ‚îÇ ‚îÇarchive-     ‚îÇ
                    ‚îÇ             ‚îÇ ‚îÇcleanup      ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Error Handling

**If pre-push inspector fails:**

1. **Security scan blocks:**
   - Review flagged content
   - Fix security issues
   - Re-run security-scan mode
   - Only bypass if false positives (ask user first)

2. **Archive fails:**
   - Check disk space: `df -h`
   - Check permissions: `ls -la ~/sports-simulator-archives/`
   - Create archive dir manually if needed
   - Re-run archive-cleanup mode

3. **Cleanup commit fails:**
   - Check git status: `git status`
   - Resolve any conflicts
   - Re-run cleanup-repo mode

4. **Push fails:**
   - Check network: `ping github.com`
   - Check remote: `git remote -v`
   - Check credentials: `ssh -T git@github.com`
   - Try push manually: `git push`

---

### Integration with Git Commit Workflow

**Recommended sequence:**

1. Complete work ‚Üí Document in COMMAND_LOG.md
2. Run security scan (Git Commit Workflow Step 0-1)
3. Commit changes (Git Commit Workflow Step 3)
4. Run archive manager (Git Commit Workflow Step 5)
5. **Run pre-push inspector (THIS WORKFLOW)**
6. Push to GitHub (after approval)

---

### Ask User for Push Approval (After Inspector Completes)

**NEVER push without asking:**

```
Pre-push inspection complete. Ready to push to GitHub? [y/N]
```

**If user says YES:**
```bash
git push
```

**If user says NO:**
- Exit workflow
- User can push manually later: `git push`

---

### If Pre-Push Hook Blocks (Last Resort)

**STOP immediately:**
- Show user the flagged lines from hook output
- Explain what was flagged (false positives vs real secrets)
- Show context (10 lines before/after)
- Ask: "These appear to be [false positives/real secrets]. Bypass hook and push anyway? [y/N]"
- **Only push with `--no-verify` if user explicitly approves**

**Bypass command (only if approved):**
```bash
git push --no-verify
```

**‚ö†Ô∏è CRITICAL:** NEVER bypass without showing user what was flagged and getting explicit approval.

---

## üß™ Error Handling Workflow

**When any command fails:**

### 1. Stop Immediately
Don't attempt multiple fixes automatically

### 2. Check TROUBLESHOOTING.md
Look for known solution

### 3. If Unknown Error
- Show error to user
- Ask for guidance: "This error isn't documented. How would you like to proceed?"

### 4. After Resolving Error
- Document solution in COMMAND_LOG.md
- Ask: "Should I add this solution to TROUBLESHOOTING.md?" (if meets criteria)
- Use `log_solution` helper function if applicable

### 5. Update PROGRESS.md
- Document error details
- Update time estimates if impacted
- Mark task status appropriately

---

## üí° Documentation Update Triggers

**Proactively offer to update documentation when:**

### TROUBLESHOOTING.md
**Offer when error:**
- Took >10 minutes to solve
- Has non-obvious solution
- Is environment-specific (conda, AWS, macOS)
- Recurring or has misleading error message

### ADR (Architecture Decision Record)
**Offer when decision involves:**
- Technology choices (database, framework, AWS service)
- Major trade-offs (cost vs performance)
- Decisions affecting multiple system parts
- Rejecting common approaches

### QUICKSTART.md
**Offer when:**
- New daily commands added
- File locations changed
- Workflow shortcuts discovered
- Common troubleshooting steps identified

### STYLE_GUIDE.md
**Offer when pattern:**
- Used consistently 3+ times
- Improves readability (type hints, docstrings)
- Establishes team convention

---

## üìã Generate File Inventory Workflow (make inventory)

**Purpose:** Automatically generate comprehensive FILE_INVENTORY.md with metadata and summaries of all project files

**Script:** `scripts/maintenance/generate_inventory.py` (called by `make inventory`)

**When to run:**
- After adding new scripts or documentation files
- FILE_INVENTORY.md is >7 days old (checked by `session_end.sh`)
- After major refactoring or file reorganization
- Monthly as part of documentation maintenance
- Before sharing project structure with collaborators

### Single Command

```bash
make inventory
```

**Equivalent direct call:**
```bash
python scripts/maintenance/generate_inventory.py
```

### What This Does

**4-step automated process:**

#### Step 1: Scan Project Files

Searches for all files matching these patterns:
- `scripts/**/*.py` - All Python scripts
- `scripts/**/*.sh` - All shell scripts
- `sql/**/*.sql` - SQL migration/query scripts
- `docs/**/*.md` - Documentation files
- `*.md` - Root markdown files (README, PROGRESS, etc.)
- `Makefile` - Build automation
- `.env.example` - Configuration template

**Excludes:**
- `__pycache__/` - Python cache
- `.git/` - Git metadata
- `data/` - Raw data files (too large)
- `.idea/` - IDE settings
- `backups/` - Backup directories

#### Step 2: Extract File Metadata

For each file, extracts:

**Basic metadata:**
- Relative path from project root
- File type (Python script, Bash script, etc.)
- Last modified date
- Line count
- File size

**Content analysis:**
- **Purpose** - Extracted from docstrings/comments
  - Python: Module-level `"""docstring"""`
  - Shell: First `# comment` after shebang
  - Markdown: First non-heading line
  - SQL: First `-- comment` line

- **Key Functions/Classes** - Parsed from code
  - Python: `def function_name()` and `class ClassName`
  - Bash: `function_name() { ... }`
  - Limits to top 8 functions/classes

- **Dependencies** - Identified imports
  - Python: `import` and `from` statements (external packages only)
  - Shell: Detects AWS CLI, psql, conda, git usage

- **Usage Example** - How to run the script
  - Python: Extracted from usage comments or inferred from `__main__`
  - Shell: Extracted from "Usage:" comment or inferred

#### Step 3: Organize by Category

Groups files into 9 categories:

1. **Root Documentation** - README.md, PROGRESS.md, QUICKSTART.md, etc.
2. **Scripts - AWS** - `scripts/aws/*.sh` (cost checking, resource management)
3. **Scripts - Shell Utilities** - `scripts/shell/*.sh` (session management, git hooks)
4. **Scripts - ETL** - `scripts/etl/*.{py,sh}` (data extraction, Glue jobs)
5. **Scripts - Maintenance** - `scripts/maintenance/*.{py,sh}` (archiving, inventory)
6. **Documentation** - `docs/*.md` (detailed guides, protocols)
7. **Architecture Decision Records** - `docs/adr/*.md` (numbered ADRs)
8. **SQL Scripts** - `sql/*.sql` (schema, migrations)
9. **Configuration** - Makefile, .env.example, etc.

#### Step 4: Generate Markdown Output

Creates `FILE_INVENTORY.md` with:

**File listings** for each category:
```markdown
### scripts/maintenance/generate_inventory.py

**Type:** Python script (341 lines)
**Last Modified:** 2025-10-02
**Purpose:** Automatically generate FILE_INVENTORY.md with summaries of all project files.

**Key Functions:**
- `get_file_info()`
- `extract_purpose()`
- `extract_functions()`
- `extract_dependencies()`
- `scan_project()`

**Dependencies:** subprocess

**Usage:** `python scripts/maintenance/generate_inventory.py`
```

**Summary statistics** at end:
```markdown
## Summary Statistics

**Total files documented:** 87
**Total lines of code/docs:** 45,231

**Files by type:**
- Python script: 23
- Bash script: 19
- Markdown documentation: 32
- SQL script: 8
- YAML configuration: 5
```

### Example Output

```bash
$ make inventory
üîç Scanning project files...
   Found 87 files to document

üìù Generating inventory...

üíæ Writing to /Users/ryanranft/nba-simulator-aws/FILE_INVENTORY.md...

‚úÖ FILE_INVENTORY.md generated successfully!
   Total files: 87
   Location: /Users/ryanranft/nba-simulator-aws/FILE_INVENTORY.md
```

### Integration with Other Workflows

**After adding new scripts:**
```bash
# 1. Create new script
vim scripts/etl/new_extraction_script.py

# 2. Add docstring explaining purpose
# 3. Commit changes
git add scripts/etl/new_extraction_script.py
git commit -m "Add new extraction script for team rosters"

# 4. Update inventory
make inventory

# 5. Commit updated inventory
git add FILE_INVENTORY.md
git commit -m "Update inventory with new script"
```

**Session end workflow integration:**
```bash
# Session end checklist checks if FILE_INVENTORY.md is stale
bash scripts/shell/session_end.sh

# If >7 days old:
# ‚ö†Ô∏è  FILE_INVENTORY.md is 9 days old (consider: make inventory)

# Update it:
make inventory
```

**Monthly maintenance:**
```bash
# First Monday of month
make inventory                # Refresh all metadata
git add FILE_INVENTORY.md
git commit -m "Monthly inventory refresh"
```

### File Inventory Use Cases

**1. Onboarding New Developers**
```bash
# New developer joins project
cat FILE_INVENTORY.md    # Quick overview of all files
```

**2. Finding Right Script**
```bash
# Search inventory for specific functionality
grep -i "cost" FILE_INVENTORY.md
# Finds: check_costs.sh - AWS cost reporting workflow
```

**3. Documentation Completeness Check**
```bash
# Find scripts without purpose documentation
grep "No description available" FILE_INVENTORY.md
# Returns scripts missing docstrings/comments
```

**4. Dependency Audit**
```bash
# Check which scripts use specific tools
grep -A 1 "Dependencies:" FILE_INVENTORY.md | grep "AWS CLI"
# Lists all scripts requiring AWS CLI
```

**5. Code Quality Review**
```bash
# Find large scripts that may need refactoring
grep "Python script" FILE_INVENTORY.md | grep -E "[0-9]{3,} lines"
# Finds Python scripts >100 lines
```

### Comparison to Manual Documentation

| Approach | Time Required | Accuracy | Maintenance | Use Case |
|----------|---------------|----------|-------------|----------|
| **Manual FILE_INVENTORY.md** | 2-3 hours initially | üìâ Degrades over time | ‚ö†Ô∏è Requires discipline | Legacy projects |
| **make inventory** | <10 seconds | ‚úÖ Always current | ‚úÖ Single command | **Active development** |
| **No inventory** | 0 seconds | ‚ùå N/A | ‚ùå N/A | Small projects |
| **README only** | 30 minutes | üìâ High-level only | ‚ö†Ô∏è Manual | Simple projects |

### Best Practices

**Update inventory when:**
- ‚úÖ Adding 3+ new files in one session
- ‚úÖ Renaming or reorganizing file structure
- ‚úÖ FILE_INVENTORY.md is >7 days old (per session_end.sh)
- ‚úÖ Before monthly documentation review
- ‚úÖ Before creating pull request (ensure inventory current)

**Don't worry about updating if:**
- ‚úÖ Only editing existing file contents (no new files)
- ‚úÖ Just updated yesterday (<24 hours)
- ‚úÖ Only changed data files (excluded from inventory)

### Troubleshooting

**Problem: "No such file or directory" error**
```bash
# Cause: Running from wrong directory
# Solution: Must run from project root
cd /Users/ryanranft/nba-simulator-aws
make inventory
```

**Problem: "No module named 'X'" error**
```bash
# Cause: Script depends on external Python package
# Solution: Activate conda environment first
conda activate nba-aws
make inventory
```

**Problem: FILE_INVENTORY.md missing expected files**
```bash
# Cause: Files in excluded directories or patterns
# Solution: Check script's exclude_patterns list (lines 202-208)
# Excluded: __pycache__, .git, data/, .idea, backups

# If file should be included, move out of excluded directory
```

**Problem: Purpose shows "No description available"**
```bash
# Cause: Script missing docstring/header comment
# Solution: Add documentation to script

# For Python scripts:
"""
Purpose: One-line description of what this script does
"""

# For shell scripts:
# Purpose: One-line description of what this script does
```

**Problem: Inventory shows stale "Last Modified" dates**
```bash
# Cause: FILE_INVENTORY.md not regenerated after file changes
# Solution: Run make inventory (it's fast!)
make inventory
```

### Alternative: Manual Execution

If Makefile not available:

```bash
# Direct Python execution
cd /Users/ryanranft/nba-simulator-aws
python scripts/maintenance/generate_inventory.py
```

**With virtual environment:**
```bash
conda activate nba-aws
python scripts/maintenance/generate_inventory.py
```

### Technical Details

**Performance:**
- Scans ~87 files in <10 seconds
- Uses parallel file reading (async I/O)
- Memory efficient (processes one file at a time)

**Output format:**
- GitHub-flavored Markdown
- UTF-8 encoded
- Consistent heading structure (searchable)
- Auto-generated timestamp at top

**Limitations:**
- Python/Bash-centric (other languages need custom parsers)
- Docstring extraction uses regex (may miss complex formats)
- Function count limited to 8 per file (prevents clutter)
- Dependency detection is heuristic (not AST-based)

**Future enhancements (potential):**
- Add file complexity metrics (cyclomatic complexity)
- Detect unused scripts (no imports, no git history)
- Generate dependency graph (which scripts call which)
- Include test coverage % for each script

---

## üéØ Session End Workflow

**Purpose:** Comprehensive checklist to ensure work is saved, documented, and ready for next session

**When to use:** Before closing Claude Code session after any significant work

### Automated Session End Checklist (Recommended)

**Script:** `scripts/shell/session_end.sh`

**Usage:**
```bash
bash scripts/shell/session_end.sh
```

**What this does (4-part checklist):**

#### Part 1: Git Status Check

**Checks for uncommitted changes:**
```bash
git status --porcelain
```

**Output if changes exist:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã 1. GIT STATUS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è  You have uncommitted changes:

 M CLAUDE.md
 M docs/CLAUDE_WORKFLOW_ORDER.md
 M scripts/shell/session_end.sh
   ... (15 total files changed)

üí° Consider:
   - Review changes: git diff
   - Commit work: git add . && git commit
   - Or stash: git stash save "WIP: session end 2025-10-02"
```

**Output if clean:**
```
‚úì No uncommitted changes
```

#### Part 2: Claude Code Conversation Check

**Checks if CHAT_LOG.md exists and freshness:**

**Scenario A: CHAT_LOG.md exists and recent (<1 hour)**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üí¨ 2. CLAUDE CODE CONVERSATION
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì CHAT_LOG.md exists (updated 12 minutes ago)

üí° Workflow:
   1. Commit your changes: git add . && git commit -m 'message'
   2. Archive conversation: bash scripts/maintenance/archive_chat_by_next_sha.sh
   3. This saves the chat with the commit SHA it produced
```

**Scenario B: CHAT_LOG.md exists but stale (>1 hour)**
```
‚ö†Ô∏è  CHAT_LOG.md is 5 hours old

üìù ACTION NEEDED:
   1. Export your latest Claude Code conversation
   2. Save/overwrite as: CHAT_LOG.md
   3. Commit changes, then archive conversation
```

**Scenario C: CHAT_LOG.md missing**
```
‚ö†Ô∏è  CHAT_LOG.md not found

üìù RECOMMENDED WORKFLOW:
   1. Export Claude Code conversation ‚Üí Save as CHAT_LOG.md
   2. Stage and commit changes ‚Üí git add . && git commit
   3. Archive conversation ‚Üí bash scripts/maintenance/archive_chat_by_next_sha.sh
   4. This saves chat as: chat-<SHA>-sanitized.md

üí° Why this matters:
   - Each conversation linked to the commit it produced
   - Future LLMs can trace: 'What work led to commit X?'
   - Preserves full context for reproducing this pipeline
   - Archive: ~/sports-simulator-archives/nba/conversations/
```

#### Part 3: Documentation Status Check

**Checks 3 key documentation files:**

1. **COMMAND_LOG.md** - Updated today?
2. **PROGRESS.md** - Updated today?
3. **FILE_INVENTORY.md** - Updated within 7 days?

**Sample output:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìö 3. DOCUMENTATION
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úì COMMAND_LOG.md updated today
‚úì PROGRESS.md updated today
‚ö†Ô∏è  FILE_INVENTORY.md is 9 days old (consider: make inventory)

üí° Consider updating documentation before next session
```

#### Part 4: Next Session Preview

**Shows next pending task from PROGRESS.md:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üöÄ 4. NEXT SESSION
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Next pending task in PROGRESS.md:
  ‚è∏Ô∏è Phase 2.2: Create year-based Glue Crawlers for manageable data...

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  ‚úì Session end checklist complete                            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

üëã Session ended at 2025-10-02 19:45:23
```

### Manual Session End Checklist (Alternative)

**If session_end.sh not available, manually check:**

**1. Git Status:**
```bash
git status
```
- ‚úÖ Commit work or stash if needed

**2. Conversation Export:**
- ‚úÖ Export Claude Code conversation to CHAT_LOG.md
- ‚úÖ Check freshness: `ls -lh CHAT_LOG.md`

**3. Documentation:**
```bash
# Check if files modified today
ls -lt COMMAND_LOG.md PROGRESS.md FILE_INVENTORY.md
```
- ‚úÖ Update COMMAND_LOG.md if needed
- ‚úÖ Update PROGRESS.md if tasks completed
- ‚úÖ Run `make inventory` if files changed

**4. Next Session:**
- ‚úÖ Review next pending task in PROGRESS.md
- ‚úÖ Note any blockers or prerequisites

### Shell Alias for Quick Access

**Add to ~/.bashrc or ~/.zshrc:**
```bash
alias end-session='bash /Users/ryanranft/nba-simulator-aws/scripts/shell/session_end.sh'
```

**Then simply run:**
```bash
end-session
```

### Integration with Other Workflows

**Session End Workflow integrates with:**
- **Git Commit Workflow:** Checks for uncommitted changes before ending
- **Archive Management:** Prompts to archive CHAT_LOG.md after commit
- **Context Management:** Ensures conversation saved before ending session
- **Session Start Workflow:** Shows next pending task for continuity

### Best Practices

1. ‚úÖ **Run before every session end** (even short sessions)
2. ‚úÖ **Always export conversation** if significant work done (>30 min)
3. ‚úÖ **Commit changes** before ending session (avoid WIP accumulation)
4. ‚úÖ **Archive conversation** after commit (links chat to specific SHA)
5. ‚úÖ **Review next task** to prepare for next session

### What Gets Checked

| Check | File/Command | Warning If | Action |
|-------|--------------|------------|--------|
| **Git Status** | `git status --porcelain` | Uncommitted changes exist | Commit or stash |
| **Conversation** | `CHAT_LOG.md` age | Missing or >1 hour old | Export conversation |
| **Command Log** | `COMMAND_LOG.md` date | Not updated today | Review session commands |
| **Progress** | `PROGRESS.md` date | Not updated today | Update task status |
| **Inventory** | `FILE_INVENTORY.md` age | >7 days old | Run `make inventory` |
| **Next Task** | `grep ‚è∏Ô∏è PROGRESS.md` | None | Review PROGRESS.md |

### Troubleshooting

**Script not found:**
```bash
# Verify script exists
ls -l scripts/shell/session_end.sh

# If missing, check git status
git status scripts/shell/session_end.sh

# Restore if needed
git checkout scripts/shell/session_end.sh
```

**CHAT_LOG.md detection issues:**
```bash
# Check if file exists
ls -lh CHAT_LOG.md

# Check last modification time (macOS)
stat -f %m CHAT_LOG.md

# Check last modification time (Linux)
stat -c %Y CHAT_LOG.md
```

**Documentation date checks failing:**
```bash
# Manually check when files last modified
ls -lt COMMAND_LOG.md PROGRESS.md FILE_INVENTORY.md

# Update if needed
make inventory          # Updates FILE_INVENTORY.md
make update-docs        # Updates all documentation
```

### Claude's Session End Reminders (Manual)

**If script not used, Claude should remind user:**
- ‚úÖ "COMMAND_LOG.md modified - review for sensitive data before committing"
- ‚úÖ "Multiple files changed - consider running `make backup`"
- ‚úÖ "Documentation changed - consider running `make inventory`"
- ‚úÖ "Phase complete - update PROGRESS.md status to ‚úÖ COMPLETE and run `make sync-progress`"
- ‚úÖ "Conversation not saved - export to CHAT_LOG.md before ending session"

---

## üìä Context Management Workflow

**Monitor context usage throughout session:**

### At 75% Context (~150K tokens)
**AUTOMATIC - Don't ask:**
- Save conversation verbatim to CHAT_LOG.md
- Notify user: "Context at 75% (~150K tokens) - auto-saved conversation to CHAT_LOG.md"

### At 90% Context (~180K tokens)
**URGENT - Strongly recommend:**
"Context at 90% (~180K tokens) - strongly urge committing changes NOW before context limit (200K)"

### User Says "Save This Conversation"
**AUTOMATIC - Don't ask:**
- Write verbatim transcript to CHAT_LOG.md
- Include timestamp and session context

### Manual Save with Script (Alternative Method)

**When to use:** User wants to trigger conversation save via script

**Script:** `scripts/shell/save_conversation.sh`

**What it does:**
1. Displays save conversation prompt
2. Shows what will be saved (verbatim transcript format)
3. Asks for confirmation
4. Outputs instructions for Claude to save conversation

**Usage:**
```bash
bash scripts/shell/save_conversation.sh
```

**Script output:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üíæ SAVE CONVERSATION TO CHAT_LOG.md
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

This will trigger Claude to write the conversation transcript.

Claude will write:
  - Full verbatim conversation (your messages + Claude's responses)
  - All code snippets and commands
  - Timestamp and session info

Destination: /Users/ryanranft/nba-simulator-aws/CHAT_LOG.md

Continue? [y/N] y

üìù Prompting Claude to save conversation...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üëâ CLAUDE: Please write the full verbatim conversation to CHAT_LOG.md

   Include:
   - Every user message (exact text)
   - Every assistant response (complete text)
   - All code blocks, commands, and tool outputs
   - Timestamp header with session date/time

   Format:
   # Claude Code Conversation - [Date/Time]

   User: [exact message]

   Assistant: [complete response]

   [Continue for entire conversation...]

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚è≥ Waiting for Claude to write CHAT_LOG.md...
```

**When Claude saves conversation:**

Claude will write to `CHAT_LOG.md` with format:
```markdown
# Claude Code Conversation - 2025-10-02 18:45:23

**Session Info:**
- Date: 2025-10-02
- Time: 18:45:23
- Working Directory: /Users/ryanranft/nba-simulator-aws
- Context: [current task/phase]

---

## Conversation

**User:** [First user message]

**Assistant:** [First assistant response with complete text, code blocks, etc.]

**User:** [Second user message]

**Assistant:** [Second assistant response...]

[Continue for entire conversation...]

---

**End of Conversation**
```

**After save completes:**
- Review CHAT_LOG.md for completeness
- Archive conversation: `bash scripts/maintenance/archive_manager.sh conversation`
- Commit if needed: `git add CHAT_LOG.md && git commit -m "Save conversation log"`

**Note:** The script itself doesn't write CHAT_LOG.md - it prompts Claude to do so. This is a workflow trigger, not an automated writer.

### Context Tracking Notes
- **Total context limit:** 200K tokens
- **75% threshold:** 150K tokens (auto-save trigger)
- **90% threshold:** 180K tokens (urgent warning)
- **CHAT_LOG.md location:** Project root
- **Archive location:** `~/sports-simulator-archives/nba/chat-logs/`

### Integration with Archive Management

**After saving conversation to CHAT_LOG.md:**

1. **Archive immediately** (creates 3 versions):
   ```bash
   bash scripts/maintenance/archive_manager.sh conversation
   ```

2. **This creates:**
   - **ORIGINAL:** `~/sports-simulator-archives/nba/conversations/CHAT_LOG_<timestamp>.md`
     - Full conversation with real paths/IPs
     - **NEVER commit to GitHub**
   - **SANITIZED:** `~/sports-simulator-archives/nba/conversations/CHAT_LOG_SANITIZED_<timestamp>.md`
     - Credentials removed, IPs masked
     - **Safe for private repos**
   - **SPOOFED:** `~/sports-simulator-archives/nba/conversations/CHAT_LOG_SPOOFED_<timestamp>.md`
     - Fake paths for demonstration
     - **Safe for public sharing**

3. **Clear CHAT_LOG.md** (optional, to start fresh):
   ```bash
   # After archiving, optionally clear for next session
   > CHAT_LOG.md  # Clear contents
   echo "# Chat Log" > CHAT_LOG.md  # Or add header
   ```

**See Archive Management Workflow for complete archiving procedures**

---

## üß™ Testing Workflows

**Follow test-driven development approach for all code**

### Before Writing Code
1. **Write test cases first** (if applicable)
   ```bash
   # Create test file
   touch tests/test_<module_name>.py
   ```
2. **Create test fixtures**
   - Sample data files
   - Mock AWS responses
   - Expected outputs
3. **Define success criteria**
   - What should pass?
   - What should fail?
   - Edge cases to cover

### While Writing Code
1. **Run tests frequently** (after each function/class)
   ```bash
   pytest tests/test_<module_name>.py -v
   ```
2. **Test-driven development cycle:**
   - Write failing test ‚Üí Write code ‚Üí Test passes ‚Üí Refactor ‚Üí Repeat
3. **Check coverage** (aim for >80%)
   ```bash
   pytest --cov=scripts tests/
   ```

### Before Committing Code
1. **Run ALL tests**
   ```bash
   pytest tests/ -v
   ```
2. **Check for test failures**
   - Fix all failures before commit
   - Document any expected failures
3. **Verify code coverage**
   ```bash
   pytest --cov=scripts --cov-report=html tests/
   ```
4. **Review test output** with user before commit

### Before AWS Deployment (Integration Testing)
1. **Run integration tests with sample data**
   ```bash
   python scripts/test_integration.py
   ```
2. **Verify AWS connectivity**
   - S3 bucket access
   - RDS connection (if applicable)
   - Glue job submission (if applicable)
3. **Manual validation checklist:**
   - ‚úÖ Sample data processed correctly
   - ‚úÖ Output format matches expectations
   - ‚úÖ Error handling works (test with bad data)
   - ‚úÖ Logging captures all operations
   - ‚úÖ Performance acceptable (timing checks)

**See `docs/TESTING.md` for complete testing procedures and examples**

---

## üîß Environment Setup & Verification Workflows

### Fresh Environment Setup (12-Step Process)

**Use when setting up new development machine or recovering from system failure**

1. **Install system dependencies** (Homebrew, Xcode tools)
2. **Install Miniconda** (Python environment management)
3. **Install AWS CLI** (v2)
4. **Configure AWS credentials** (`~/.aws/credentials`)
5. **Set up SSH keys** (GitHub authentication)
6. **Clone repository** (via SSH)
7. **Install git hooks** (pre-commit, pre-push, commit template)

   **Script:** `scripts/shell/setup_git_hooks.sh`

   **Purpose:** Automates installation of Git hooks for session history tracking and commit automation

   **Usage:**
   ```bash
   bash scripts/shell/setup_git_hooks.sh
   ```

   **What this installs:**

   #### Hook 1: post-commit (Automatic Session History Logging)

   **Location:** `.git/hooks/post-commit`

   **What it does:**
   - Automatically runs after every `git commit`
   - Executes `session_startup.sh` to capture environment snapshot
   - Appends snapshot to `.session-history.md`
   - Enables correlation of commits with exact software versions

   **Information captured per commit:**
   - Hardware specs (MacBook model, chip, memory)
   - macOS version and build
   - Python version and location
   - Conda environment and package count
   - Git status (branch, recent commits)
   - Timestamp of commit

   **Hook script contents:**
   ```bash
   #!/bin/bash
   # Post-commit hook: Automatically log session snapshot after every commit

   # Change to project root directory
   cd "$(git rev-parse --show-toplevel)"

   # Append session snapshot to .session-history.md
   bash scripts/shell/session_startup.sh >> .session-history.md

   # Silent success
   exit 0
   ```

   **Script output during installation:**
   ```
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   NBA Simulator AWS - Git Hooks Setup
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

   Project root: /Users/ryanranft/nba-simulator-aws

   Installing Git hooks...

   Installing post-commit hook...
   ‚úÖ post-commit installed successfully

   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   Summary:
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

   ‚úÖ Hooks installed: 1

   Installed hooks in .git/hooks:
     post-commit

   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   What these hooks do:
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

   post-commit:
     - Automatically appends environment snapshot to .session-history.md
     - Logs: hardware, Python version, conda env, git status, packages
     - Enables correlation of commits with exact software versions

   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
   Testing:
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

   To test the post-commit hook:
     1. Make a test commit:
        touch test.txt
        git add test.txt
        git commit -m "Test post-commit hook"

     2. Verify .session-history.md was updated:
        tail -20 .session-history.md

     3. Clean up:
        git rm test.txt
        git commit -m "Remove test file"

   ‚úì All hooks installed successfully!
   ```

   **Testing the installation:**
   ```bash
   # 1. Make a test commit
   touch test.txt
   git add test.txt
   git commit -m "Test post-commit hook"

   # 2. Verify .session-history.md was updated
   tail -20 .session-history.md
   # Should show new snapshot with current timestamp

   # 3. Clean up
   git rm test.txt
   git commit -m "Remove test file"
   ```

   **Expected .session-history.md format:**
   ```markdown
   # Session History

   This file is automatically updated by the post-commit hook.

   ---

   ## Session: 2025-10-02 19:30:45

   **Commit:** abc1234 - "Add feature X"

   **Hardware:**
   - Device: MacBook Pro 16-inch, 2023
   - Chip: Apple M2 Max
   - Memory: 96 GB

   **Software:**
   - macOS: 14.6 (23G93)
   - Python: 3.11.9
   - Conda: 24.7.1
   - Environment: nba-aws (127 packages)

   **Git Status:**
   - Branch: main
   - Last 3 commits:
     abc1234 - Add feature X
     def5678 - Update docs
     ghi9012 - Fix bug Y

   ---
   ```

   **Why this is useful:**
   - **Debugging:** "This worked in commit abc123 but broke in def456 - what changed?"
   - **Environment tracking:** Compare exact Python/package versions between commits
   - **Onboarding:** New team members see complete environment evolution
   - **Troubleshooting:** "Code worked on commit X with Python 3.11.8, now fails on 3.11.9"
   - **Audit trail:** Complete record of software versions for compliance

   **What gets logged automatically:**
   - ‚úÖ Every commit triggers post-commit hook
   - ‚úÖ Environment snapshot appended to `.session-history.md`
   - ‚úÖ No manual intervention required
   - ‚úÖ Zero impact on commit speed (<1 second overhead)

   **Hook management commands:**
   ```bash
   # List installed hooks
   ls -lh .git/hooks/

   # View post-commit hook contents
   cat .git/hooks/post-commit

   # Temporarily disable hook (remove execute permission)
   chmod -x .git/hooks/post-commit

   # Re-enable hook
   chmod +x .git/hooks/post-commit

   # Remove hook completely
   rm .git/hooks/post-commit

   # Reinstall all hooks
   bash scripts/shell/setup_git_hooks.sh
   ```

   **Troubleshooting:**

   **Hook not running:**
   ```bash
   # Check if hook is executable
   ls -l .git/hooks/post-commit
   # Should show: -rwxr-xr-x

   # If not executable, make it so
   chmod +x .git/hooks/post-commit
   ```

   **session_startup.sh not found:**
   ```bash
   # Verify script exists
   ls -l scripts/shell/session_startup.sh

   # If missing, check git status
   git status

   # Script should be in repo - restore if needed
   git checkout scripts/shell/session_startup.sh
   ```

   **.session-history.md not updating:**
   ```bash
   # Test hook manually
   bash .git/hooks/post-commit

   # Check for errors in output
   # Verify .session-history.md was modified
   git status
   ```

   **Integration with other workflows:**
   - **Git Commit Workflow Step 4:** Post-commit hook runs automatically after commit
   - **Session Start Workflow:** `session_manager.sh` reads `.session-history.md` to show recent snapshots
   - **Archive Management:** `.session-history.md` archived with each commit
   - **Troubleshooting:** Use `.session-history.md` to correlate issues with environment changes

   **Best practices:**
   1. ‚úÖ Run setup script once during initial environment setup (Step 7)
   2. ‚úÖ Verify hooks installed: `ls .git/hooks/post-commit`
   3. ‚úÖ Test with dummy commit after installation
   4. ‚úÖ Keep `.session-history.md` in `.gitignore` (already configured)
   5. ‚úÖ Archive `.session-history.md` regularly (automatic via archive_manager.sh)
   6. ‚úÖ Review `.session-history.md` when debugging environment issues

   **Future hooks (not yet implemented):**
   The script is designed to easily add more hooks in the future:
   - `pre-commit`: Security scanning, linting, formatting
   - `pre-push`: Run tests, check for TODOs, validate documentation
   - `commit-msg`: Enforce commit message format, add issue tracking links

   To add a new hook, edit `setup_git_hooks.sh` and add a new case in the `create_hook()` function.

8. **Verify .gitignore security settings** (NEW - security hardening)
   ```bash
   bash scripts/shell/verify_gitignore.sh
   ```

   **What this checks:**
   - ‚úÖ `.env$` pattern exists (environment files)
   - ‚úÖ `*.key` pattern exists (private keys)
   - ‚úÖ `*.pem` pattern exists (PEM certificates)
   - ‚úÖ `credentials.yaml` pattern exists (credential files)
   - ‚úÖ `*credentials*` pattern exists (any credential files)
   - ‚úÖ `.aws/` directory pattern exists (AWS config)
   - ‚úÖ `*.log` pattern exists (log files with potential secrets)

   **If verification fails:**
   - Script will show missing patterns
   - Add missing patterns to .gitignore:
     ```bash
     echo "<missing_pattern>" >> .gitignore
     ```
   - Re-run verification until all checks pass
   - Commit updated .gitignore:
     ```bash
     git add .gitignore
     git commit -m "Add missing .gitignore security patterns"
     ```

9. **Create conda environment** (`conda env create -f environment.yml`)
10. **Install Python dependencies** (boto3, pandas, psycopg2, etc.)
11. **Configure .env file** (project-specific settings)
12. **Verify S3 access** (list bucket contents)

**Complete setup instructions:** See `docs/SETUP.md` lines 1-558

### Environment Verification Workflow

**Run whenever environment issues suspected:**

```bash
# Comprehensive verification
make verify-all

# Individual checks
make verify-env    # Conda environment
make verify-aws    # AWS credentials and connectivity
make verify-files  # Check expected files exist
```

**Health check script:**
```bash
source scripts/shell/session_manager.sh start
```

**What it verifies:**
- Hardware specs (model, chip, cores, memory)
- System versions (macOS, Homebrew)
- Conda environment (version, active env, packages)
- Python environment (version, location, key packages)
- AWS CLI (version, credentials)
- Git status (branch, commits, modified files)
- Documentation status (inventory age, stale docs)

**If verification fails:**
1. Check error message for specific issue
2. Consult `TROUBLESHOOTING.md` for common environment issues
3. Re-run specific verification (e.g., `make verify-aws`)
4. If persistent, consider fresh setup (see above)

---

## üí∞ Cost Management Workflows

### Automated Cost Reporting (Recommended)

**Purpose:** Get real-time AWS cost breakdown with budget alerts and optimization recommendations

**Script:** `scripts/aws/check_costs.sh`

**Usage:**
```bash
bash scripts/aws/check_costs.sh

# Or via Makefile:
make check-costs
```

**What this report includes:**

#### 1. Total Cost (Month-to-Date)
Shows current month spending from start of month to today:
```
üí∞ Total Cost (Month-to-Date):
   $12.34 USD
```

#### 2. Cost by Service (Top 10)
Breaks down spending by AWS service, sorted by cost:
```
üìä Cost by Service:
   Amazon Simple Storage Service (S3)    2.74 USD
   Amazon Relational Database Service    8.50 USD
   AWS Glue                              1.10 USD
```

**Key services monitored:**
- **S3:** Storage costs (currently $2.74/month for 146K files)
- **RDS:** Database instance hours (~$29/month for db.t3.small)
- **Glue:** ETL job DPU-hours (~$0.44/DPU-hour)
- **EC2:** Compute instances (~$5-15/month for t2.micro)
- **SageMaker:** Notebook instances (~$50/month for ml.t3.medium)

#### 3. S3 Storage Analysis
Calculates S3 bucket size and estimated monthly cost:
```
üóÑÔ∏è  S3 Storage:
   Size: 119.23 GB
   Estimated monthly cost: $2.74
```

**Cost calculation:** `Size (GB) √ó $0.023/GB` (S3 Standard storage)

#### 4. RDS Status
Shows RDS instance status and estimated cost:
```
üóÉÔ∏è  RDS Status:
   Status: available
   Instance: db.t3.small
   Estimated cost: ~$29/month
```

**Possible statuses:**
- `available` - Running (incurring costs)
- `stopped` - Stopped (minimal costs)
- `Not created yet (‚è∏Ô∏è Pending)` - No RDS resources

#### 5. EC2 Instances
Lists EC2 instances tagged with Project=nba-simulator:
```
üñ•Ô∏è  EC2 Instances:
   i-0123456789abcdef0: running (t2.micro)
```

**Note:** Only shows instances tagged properly. Untagged instances won't appear.

#### 6. Glue Jobs
Lists Glue ETL jobs:
```
‚öôÔ∏è  Glue Jobs:
   Jobs: extract-schedule extract-pbp process-game-data
```

**Status:** Shows "Not created yet (‚è∏Ô∏è Pending)" if Phase 2.2 ETL not started

#### 7. Cost Forecast
Projects end-of-month total based on current spending rate:
```
üìà Cost Forecast:
   Projected month-end total: $18.67 USD
```

**Uses:** AWS Cost Explorer forecast API to predict month-end total

#### 8. Budget Alert
Warns if spending exceeds budget threshold:
```
‚ö†Ô∏è  WARNING: Month-to-date costs exceed budget threshold of $150
```

**Budget threshold:** $150/month (configured in script line 148)

#### 9. Cost Summary & Recommendations
Provides context-aware optimization tips:

**Low costs (<$10/month):**
```
‚úÖ Costs are low (Phase 1: S3 only)
```

**Moderate costs ($10-50/month):**
```
‚úÖ Costs are within expected range (S3 + minimal services)
```

**High costs ($50-100/month):**
```
‚ö†Ô∏è  Costs are moderate - monitor RDS/EC2 usage
```

**Very high costs (>$100/month):**
```
‚ö†Ô∏è  Costs are high - review running services
   Consider stopping unused RDS/EC2 instances
```

#### 10. Cost Optimization Tips
Always displays practical optimization strategies:
```
üí° Cost Optimization Tips:
   1. Stop RDS when not in use (saves ~$29-60/month)
   2. Stop EC2 when not in use (saves ~$5-15/month)
   3. Use Spot Instances for non-critical workloads
   4. Run Glue ETL monthly instead of daily
   5. Monitor with: aws ce get-cost-and-usage --help
```

### When to Run Cost Report

**Run `check_costs.sh` when:**

1. **Before creating new AWS resources** (RDS, EC2, Glue jobs, SageMaker)
   - Establishes baseline before change
   - Helps calculate marginal cost increase

2. **Weekly (every Monday)** as part of maintenance routine
   - Track spending trends
   - Catch unexpected cost increases early

3. **After deploying new infrastructure** (same day and next day)
   - Verify expected cost increase
   - Detect provisioning errors (e.g., wrong instance size)

4. **When approaching end of month** (last week)
   - Check if forecast exceeds budget
   - Time to optimize if needed

5. **After receiving AWS cost alert emails**
   - Investigate which service caused spike
   - Take immediate action if needed

### Integration with Other Workflows

**Before Creating AWS Resources Workflow:**
```bash
# Step 1: Check current costs
bash scripts/aws/check_costs.sh

# Step 2: Review PROGRESS.md cost estimates for new resource
# Step 3: Calculate monthly impact
# Step 4: Warn user with explicit approval
# Step 5: Create resource
# Step 6: Run check_costs.sh again to verify
```

**Weekly Maintenance Workflow (update_docs.sh):**
The weekly documentation update script also fetches costs and updates PROGRESS.md automatically. The `check_costs.sh` script provides the detailed breakdown.

**Session End Workflow (session_end.sh):**
Consider running cost check at end of sessions where AWS resources were created/modified.

### Cost Tracking Workflow (Weekly)

**Run every Monday or after creating resources:**

1. **Check AWS Cost Explorer (detailed breakdown)**
   ```bash
   bash scripts/aws/check_costs.sh
   ```

2. **Review by service:**
   - S3 storage costs (from report section 3)
   - RDS instance hours (from report section 4)
   - Glue job DPU-hours (from report section 6)
   - EC2 instance hours (from report section 5)
   - SageMaker notebook hours (not yet tracked)

3. **Update PROGRESS.md** with actual costs (lines 1145-1206)
   ```bash
   # Compare actual costs from report to estimates in PROGRESS.md
   # Update if variance >20%
   ```

4. **Compare actual vs. estimated:**
   - If >20% over estimate ‚Üí investigate why
   - If consistently under ‚Üí update estimates

### Cost Optimization Workflow

**Run when approaching budget limit:**

1. **Identify highest costs** (Cost Explorer breakdown)
2. **Optimization strategies:**
   - **RDS:** Stop when not in use, reduce instance size
   - **Glue:** Reduce DPUs (2 minimum), optimize job runtime
   - **EC2:** Stop instances, use smaller instance types
   - **SageMaker:** Stop notebooks when not in use
   - **S3:** Review Intelligent-Tiering, lifecycle policies
3. **Calculate savings:** Before vs. after optimization
4. **Update PROGRESS.md** with new estimates
5. **Document optimization** in COMMAND_LOG.md

**Cost estimate reference (from PROGRESS.md):**
- S3 storage: $2.74/month (current)
- RDS db.t3.micro: ~$29/month
- Glue jobs (2 DPU): ~$13/month
- EC2 t2.micro: ~$8-15/month
- SageMaker ml.t3.medium: ~$50/month
- **Total full deployment:** $95-130/month
- **Budget target:** $150/month

---

## üíæ Backup & Recovery Workflows

### Regular Backup Workflow

**Run after significant work sessions or before risky operations:**

```bash
make backup
```

**What this backs up:**
- All uncommitted changes (git diff)
- Untracked files (git ls-files --others)
- Current branch state (git branch status)
- Working tree snapshot (tar archive)

**Backup location:** `~/sports-simulator-archives/nba/backups/backup-YYYYMMDD-HHMMSS/`

**When to run backup:**
- ‚úÖ Before destructive operations (database drops, file deletions)
- ‚úÖ After multiple hours of work (>2 hours without commit)
- ‚úÖ Before risky refactoring
- ‚úÖ After 5+ commits
- ‚úÖ End of productive session (if uncommitted changes exist)

### File Deletion Protocol

**CRITICAL: ALWAYS archive before deleting files**

1. **Identify file(s) to delete**
2. **Check if tracked by git:**
   ```bash
   git ls-files | grep <filename>
   ```
3. **Archive the file(s):**
   ```bash
   # Create archive directory
   ARCHIVE_DIR=~/sports-simulator-archives/nba/deleted-files-$(date +%Y%m%d-%H%M%S)
   mkdir -p "$ARCHIVE_DIR"

   # Copy file(s) to archive
   cp <file> "$ARCHIVE_DIR/"
   ```
4. **Create deletion record:**
   ```bash
   cat > "$ARCHIVE_DIR/DELETION_RECORD.md" << EOF
   # File Deletion Record

   **Date:** $(date)
   **Files Deleted:** <list>
   **Reason:** <why deleted>
   **Recovery:** Archive at $ARCHIVE_DIR
   EOF
   ```
5. **Remove from git tracking:**
   ```bash
   git rm --cached <file>
   echo "<file>" >> .gitignore
   ```
6. **Commit removal:**
   ```bash
   git add .gitignore
   git commit -m "Remove <file> - archived to $ARCHIVE_DIR"
   ```
7. **Delete local file** (optional - only if not needed)
   ```bash
   rm <file>
   ```

### Archive Search Workflow

**When you need to find past conversations or deleted files:**

**Method 1: Directory Structure Search**
```bash
# Find by date
ls -lt ~/sports-simulator-archives/nba/ | head -20

# Find chat logs
find ~/sports-simulator-archives/nba/chat-logs -name "*.md"

# Find deleted files
find ~/sports-simulator-archives/nba/deleted-files* -type f
```

**Method 2: Git-Based Search**
```bash
cd ~/sports-simulator-archives/nba
git log --all --oneline | grep <keyword>
git show <commit-hash>:path/to/file
```

**Archive categories:**
- `chat-logs/` - Saved conversations
- `backups/` - Working tree snapshots
- `deleted-files-*/` - Pre-deletion archives
- `pre-push-cleanup-*/` - Files removed before GitHub push

---

## üîÑ Maintenance Schedule Workflows

### Weekly Maintenance (Monday or 7+ Days)

**Automatically offered during session start (Step 4)**

```bash
make update-docs
# OR run directly:
bash scripts/maintenance/update_docs.sh
```

**What this does (7 automated steps):**

#### Step 1: Update QUICKSTART.md Costs
```bash
# Automatically fetches current month AWS costs via Cost Explorer
aws ce get-cost-and-usage \
  --time-period Start=2025-10-01,End=2025-10-02 \
  --granularity MONTHLY \
  --metrics BlendedCost

# Updates cost line in QUICKSTART.md:
# "**Current:** ~$2.74/month" ‚Üí "**Current:** ~$[ACTUAL]/month"
```

#### Step 2: Update ADR Index
```bash
# Counts all ADR files in docs/adr/
ADR_COUNT=$(find docs/adr -name "[0-9]*.md" -type f | wc -l)

# Updates docs/adr/README.md:
# "**Total ADRs:** 5" ‚Üí "**Total ADRs:** [COUNT]"
```

#### Step 3: Update PROGRESS.md Statistics
```bash
# Gathers current project state:
- Git commit count: git rev-list --count HEAD
- S3 object count: aws s3 ls s3://nba-sim-raw-data-lake/ --recursive --summarize
- RDS status: aws rds describe-db-instances --db-instance-identifier nba-sim-db
- Glue status: aws glue get-crawler --name nba-data-crawler

# Displays summary:
# "Stats: 87 commits, 146115 S3 objects"
# "RDS: not-created, Glue: not-created"
```

#### Step 4: Update Last Updated Timestamps
```bash
CURRENT_DATE=$(date +%Y-%m-%d)

# Updates "**Last Updated:** YYYY-MM-DD" in:
- QUICKSTART.md
- docs/STYLE_GUIDE.md
- docs/TESTING.md
- docs/TROUBLESHOOTING.md
- docs/adr/README.md
```

#### Step 5: Generate Project Statistics
```bash
# Counts:
- Lines of code (Python, Bash, SQL): find . -name "*.py" -o -name "*.sh" -o -name "*.sql"
- Documentation lines: find docs -name "*.md"
- Test files: find tests -name "test_*.py"

# Displays comprehensive report:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Project Statistics:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Lines of code: 15430
Documentation lines: 8924
Test files: 12
Git commits: 87
S3 objects: 146115
ADRs: 5
Current AWS cost: $2.74/month
```

#### Step 6: Check for Stale Documentation
```bash
# Finds docs not modified in 30+ days:
find docs -name "*.md" -mtime +30

# Warns about stale files:
# "‚ö†Ô∏è  Stale (30+ days): docs/OLD_SETUP.md"
```

#### Step 7: Validate Internal Documentation Links
```bash
# Extracts all markdown links: [text](link)
# Skips external links (http/https)
# Checks if local file exists

# Reports broken links:
# "‚ö†Ô∏è  Broken link in docs/SETUP.md: missing-file.md"
# "‚úÖ All internal links valid" (if no issues)
```

**Script output summary:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Documentation Auto-Update
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

1. Updating QUICKSTART.md...
   ‚úÖ Updated current costs to $2.74/month
2. Updating ADR index...
   ‚úÖ Updated ADR count to 5
3. Updating PROGRESS.md statistics...
   ‚úÖ Stats: 87 commits, 146115 S3 objects
   ‚úÖ RDS: not-created, Glue: not-created
4. Updating 'Last Updated' timestamps...
   ‚úÖ Updated timestamp in QUICKSTART.md
   ‚úÖ Updated timestamp in docs/STYLE_GUIDE.md
   [... other files ...]
5. Generating statistics...
   [Statistics table displayed]
6. Checking for stale documentation...
   ‚ö†Ô∏è  Stale (30+ days): docs/OLD_NOTES.md
7. Validating internal links...
   ‚úÖ All internal links valid

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Documentation update complete!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Next steps:
  1. Review changes: git diff
  2. Commit if satisfied: git add -u && git commit -m 'Update documentation'
  3. Run this script weekly to keep docs current
```

**When to run:**
- ‚úÖ Monday morning (start of week)
- ‚úÖ 7+ days since last run
- ‚úÖ After major documentation changes
- ‚úÖ Before starting new phase
- ‚úÖ After creating/modifying AWS resources (updates costs and status)

**What gets updated:**
- ‚úÖ QUICKSTART.md - Current AWS costs (live from Cost Explorer)
- ‚úÖ docs/adr/README.md - ADR count
- ‚úÖ PROGRESS.md - Project statistics (commits, S3, RDS, Glue)
- ‚úÖ Multiple docs - "Last Updated" timestamps
- ‚úÖ Console output - Full project statistics report
- ‚úÖ Console warnings - Stale docs (30+ days) and broken links

**Integration with other workflows:**
- **Session Start Workflow Step 4:** Automatically prompts to run if 7+ days since last run
- **Git Commit Workflow:** Run before committing documentation changes to ensure consistency
- **Monthly Maintenance:** Part of comprehensive monthly review checklist

### Monthly Maintenance (First Monday of Month)

**5-step review checklist:**

1. **Review PROGRESS.md**
   - Mark completed tasks ‚úÖ COMPLETE
   - Update time estimates based on actuals
   - Adjust remaining phase estimates
   - Update cost actuals from AWS billing

2. **Review documentation health**
   ```bash
   make stats  # Show project statistics
   ```
   - Check for outdated docs (>30 days)
   - Review TROUBLESHOOTING.md coverage
   - Verify ADRs are up to date
   - Check FILE_INVENTORY.md accuracy

3. **Review AWS resources**
   ```bash
   make sync-progress  # Compare PROGRESS.md vs. actual AWS
   ```
   - Verify all resources documented
   - Check for orphaned resources (not in PROGRESS.md)
   - Review resource costs (Cost Explorer)
   - Stop unused resources

4. **Review security**
   - Check credential ages (90-day rotation)
   - Review .gitignore effectiveness
   - Verify git hooks still installed
   - Check for exposed secrets (GitHub secret scanning)

5. **Create archive checkpoint**
   ```bash
   make backup  # Monthly backup
   ```
   - Tag significant commits
   - Archive completed phase documentation
   - Clean up old archives (>90 days)

### Inventory Update Workflow

**Run automatically after file creation/modification (File Creation Workflow Step 2)**

```bash
make inventory
```

**Manual run when:**
- ‚úÖ After creating multiple files
- ‚úÖ After moving/renaming files
- ‚úÖ After deleting files
- ‚úÖ Before important commits
- ‚úÖ 7+ days since last run

**What it updates:**
- FILE_INVENTORY.md (file list with descriptions)
- File modification timestamps
- File size statistics
- Directory structure overview

### üìä Sync PROGRESS.md with AWS Resources (make sync-progress)

**Purpose:** Automatically detect AWS resource states and suggest PROGRESS.md updates

**Script:** `scripts/maintenance/sync_progress.py` (called by `make sync-progress`)

**When to run:**
- After creating RDS instance
- After creating Glue jobs/crawlers
- After launching EC2 instances
- After setting up SageMaker notebooks
- Weekly (Monday maintenance routine)
- When unsure if PROGRESS.md reflects reality
- Before monthly documentation review

### Single Command

```bash
make sync-progress
```

**Equivalent direct call:**
```bash
python scripts/maintenance/sync_progress.py
```

**Preview mode (no changes):**
```bash
python scripts/maintenance/sync_progress.py --dry-run
```

### What This Does

**4-step automated detection process:**

#### Step 1: Check S3 Data Lake Status

Verifies if S3 bucket exists and contains expected data:

```bash
aws s3 ls s3://nba-sim-raw-data-lake/ --recursive --summarize
```

**Detection logic:**
- ‚úÖ **complete** - Bucket exists AND contains 146,115 objects
- ‚è∏Ô∏è **pending** - Bucket missing OR object count doesn't match

**Mapped to:** Phase 1 - S3 Data Lake Setup

#### Step 2: Check RDS Database Status

Verifies if RDS instance exists:

```bash
aws rds describe-db-instances --db-instance-identifier nba-sim-db
```

**Detection logic:**
- ‚úÖ **complete** - Instance exists with "DBInstanceStatus" in response
- ‚è∏Ô∏è **pending** - Instance not found OR error response

**Mapped to:** Phase 3.1 - RDS Database Setup

#### Step 3: Check Glue Crawler Status

Verifies if Glue crawler exists:

```bash
aws glue get-crawler --name nba-data-crawler
```

**Detection logic:**
- ‚úÖ **complete** - Crawler exists with "Crawler" in response
- ‚è∏Ô∏è **pending** - Crawler not found OR error response

**Mapped to:** Phase 2.1 - Glue Crawler Setup

#### Step 4: Check Glue ETL Job Status

Verifies if Glue ETL job exists:

```bash
aws glue get-job --job-name nba-etl-job
```

**Detection logic:**
- ‚úÖ **complete** - Job exists with "Job" in response
- ‚è∏Ô∏è **pending** - Job not found OR error response

**Mapped to:** Phase 2.2 - Glue ETL Job Setup

### Example Output

```bash
$ make sync-progress
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
PROGRESS.md Synchronization
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üîç Resource Detection:
   S3 Data Lake: complete
   RDS Database: pending
   Glue Crawler: pending
   Glue ETL Job: pending

üìä Project Summary:
============================================================
Git commits: 158
Documentation files: 32
Architecture Decision Records: 12
Python files: 23
Test files: 5
============================================================

üí° Suggested Updates:
   ‚úÖ Phase 1 (S3 Data Lake) appears complete
   ‚è∏Ô∏è Phase 2.1 (Glue Crawler) pending
   ‚è∏Ô∏è Phase 3.1 (RDS Database) pending
   ‚è∏Ô∏è Phase 2.2 (Glue ETL Job) pending

üíæ To apply updates, manually edit PROGRESS.md
   This script provides detection only for now

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ Sync complete!
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
```

### How to Interpret Results

**Scenario 1: All resources exist**
```
üîç Resource Detection:
   S3 Data Lake: complete
   RDS Database: complete
   Glue Crawler: complete
   Glue ETL Job: complete

üí° Suggested Updates:
   ‚úÖ Phase 1 (S3 Data Lake) appears complete
   ‚úÖ Phase 2.1 (Glue Crawler) appears complete
   ‚úÖ Phase 3.1 (RDS Database) appears complete
   ‚úÖ Phase 2.2 (Glue ETL Job) appears complete
```

**Action:** Verify PROGRESS.md shows all phases as `‚úÖ COMPLETED`

**Scenario 2: RDS not created yet**
```
üîç Resource Detection:
   S3 Data Lake: complete
   RDS Database: pending
   Glue Crawler: pending
   Glue ETL Job: pending

üí° Suggested Updates:
   ‚úÖ Phase 1 (S3 Data Lake) appears complete
   ‚è∏Ô∏è Phase 3.1 (RDS Database) pending
```

**Action:** This matches expected state - RDS is Phase 3 (comes after Phase 2 Glue setup)

**Scenario 3: Mismatch detected**
```
PROGRESS.md shows: Phase 2.1 Glue Crawler = ‚úÖ COMPLETED
make sync-progress shows: Glue Crawler: pending

üí° Suggested Updates:
   ‚è∏Ô∏è Phase 2.1 (Glue Crawler) pending
```

**Action:** Either:
- PROGRESS.md is wrong (update to ‚è∏Ô∏è PENDING)
- OR crawler was deleted (recreate crawler)

### Integration with Other Workflows

**After creating AWS resources:**
```bash
# 1. Create RDS database
aws rds create-db-instance --db-instance-identifier nba-sim-db [...]

# 2. Wait for creation
aws rds wait db-instance-available --db-instance-identifier nba-sim-db

# 3. Verify detection
make sync-progress
# Shows: RDS Database: complete

# 4. Update PROGRESS.md manually
vim PROGRESS.md
# Change Phase 3.1 from ‚è∏Ô∏è PENDING to ‚úÖ COMPLETED
```

**Weekly maintenance routine:**
```bash
# Every Monday morning
make sync-progress           # Check actual AWS state
make check-costs            # Check monthly spending
make inventory              # Update FILE_INVENTORY.md

# Review output and update PROGRESS.md if needed
```

**Before documentation review:**
```bash
# Ensure documentation matches reality
make sync-progress

# If mismatches found, decide:
# - Update PROGRESS.md to match AWS (documentation wrong)
# - OR create missing AWS resources (AWS wrong)
# - OR delete extra AWS resources (AWS has extras)
```

**Troubleshooting phase completion:**
```bash
# User claims Phase 2 complete, but checklist shows pending
make sync-progress

# Check which specific resource is missing:
# ‚è∏Ô∏è Phase 2.1 (Glue Crawler) pending
# ‚è∏Ô∏è Phase 2.2 (Glue ETL Job) pending

# Go back and complete those tasks
```

### What This Script Does NOT Do

**‚ùå Does NOT automatically update PROGRESS.md**
- Only provides detection and suggestions
- User must manually edit PROGRESS.md
- Future enhancement: could auto-update with `--apply` flag

**‚ùå Does NOT check resource configurations**
- Only checks existence (not settings)
- Example: RDS exists, but doesn't verify instance class

**‚ùå Does NOT validate data quality**
- S3 check only counts objects (not content validity)
- Use `check_data_availability.py` for data quality checks

**‚ùå Does NOT check costs**
- Use `make check-costs` for cost verification
- This script only checks resource existence

### Project Summary Statistics

The script also generates helpful project statistics:

**Git Statistics:**
- Total commit count

**Documentation Statistics:**
- Total documentation files (`.md` in `docs/`)
- Architecture Decision Records count (`docs/adr/[0-9]*.md`)

**Code Statistics:**
- Total Python files (`.py` anywhere in project)
- Test files count (`tests/test_*.py`)

**Example:**
```
üìä Project Summary:
============================================================
Git commits: 158
Documentation files: 32
Architecture Decision Records: 12
Python files: 23
Test files: 5
============================================================
```

### Comparison to Manual Verification

| Approach | Time Required | Accuracy | Use Case |
|----------|---------------|----------|----------|
| **Manual AWS Console** | 10-15 minutes | üìâ Human error | Visual verification |
| **Manual AWS CLI** | 5-10 minutes | ‚úÖ Accurate | One-off checks |
| **make sync-progress** | <10 seconds | ‚úÖ Automated | **Weekly maintenance** |
| **No verification** | 0 seconds | ‚ùå Drift guaranteed | Never recommended |

### Best Practices

**Run sync-progress when:**
- ‚úÖ Just created new AWS resources (immediate verification)
- ‚úÖ Weekly on Monday (maintenance routine)
- ‚úÖ Before monthly documentation review
- ‚úÖ After long break from project (verify nothing changed)
- ‚úÖ When PROGRESS.md status unclear

**Don't need to run if:**
- ‚úÖ No AWS changes made (just code/docs)
- ‚úÖ Already ran today (<24 hours ago)
- ‚úÖ Only local development work (no AWS interaction)

### Troubleshooting

**Problem: "aws: command not found"**
```bash
# Cause: AWS CLI not installed or not in PATH
# Solution: Install AWS CLI
brew install awscli  # macOS
# Or follow: https://aws.amazon.com/cli/

# Verify installation
aws --version
```

**Problem: "Unable to locate credentials"**
```bash
# Cause: AWS credentials not configured
# Solution: Configure credentials
aws configure

# Or verify credentials exist
cat ~/.aws/credentials
```

**Problem: "S3 Data Lake: pending" but bucket exists**
```bash
# Cause: Object count doesn't match expected 146,115
# Solution: Check actual count
aws s3 ls s3://nba-sim-raw-data-lake/ --recursive --summarize | grep "Total Objects"

# If different count, bucket incomplete
# Re-run data extraction or verify upload
```

**Problem: Script says "complete" but PROGRESS.md shows "pending"**
```bash
# Cause: PROGRESS.md is outdated
# Solution: Update PROGRESS.md manually
vim PROGRESS.md
# Find the phase (e.g., Phase 3.1 RDS Database)
# Change: ‚è∏Ô∏è PENDING ‚Üí ‚úÖ COMPLETED

# Commit the update
git add PROGRESS.md
git commit -m "Update PROGRESS.md: Phase 3.1 complete"
```

**Problem: Script says "pending" but resource exists in AWS Console**
```bash
# Cause: Resource name doesn't match expected name
# Solution: Check actual resource name in AWS

# Expected names (hardcoded in script):
# - S3: nba-sim-raw-data-lake
# - RDS: nba-sim-db
# - Glue Crawler: nba-data-crawler
# - Glue ETL Job: nba-etl-job

# If using different names, edit sync_progress.py line 44, 57, 66, 75
```

**Problem: Timeout errors**
```bash
# Cause: AWS CLI commands taking >30 seconds
# Solution: Check AWS region, network, or increase timeout

# Edit sync_progress.py line 36:
timeout=30  # Increase to 60 or 120
```

### Future Enhancements (Potential)

**Auto-update mode:**
```bash
# Not yet implemented - would auto-update PROGRESS.md
make sync-progress --apply
```

**Configuration checks:**
```bash
# Not yet implemented - would verify settings, not just existence
# Example: RDS instance class, Glue DPU count, etc.
```

**Cost correlation:**
```bash
# Not yet implemented - would link resource detection to cost data
# Example: "RDS exists ‚Üí Expected cost $29/month ‚Üí Actual $28.47"
```

**Resource dependency graph:**
```bash
# Not yet implemented - would show which resources depend on others
# Example: "Glue Crawler depends on S3 bucket (dependency met ‚úÖ)"
```

---

## üìä Data Validation Workflows

### üîç Check Data Availability (check_data_availability.py)

**Purpose:** Analyze which JSON files contain actual game data vs. just ESPN metadata

**Script:** `scripts/analysis/check_data_availability.py`

**When to run:**
- **Before ETL planning** - Understand which files have usable data
- After data extraction - Verify extraction quality
- When investigating data gaps or issues
- Before estimating storage/compute costs for ETL
- Monthly as part of data quality monitoring

### Why This Matters

**Critical problem:** Not all JSON files in S3 contain usable game data!

**Example issues:**
- ‚ùå Some files are just ESPN API metadata (no plays, no stats)
- ‚ùå Empty playoff games (game scheduled but never played)
- ‚ùå Malformed JSON (extraction errors)
- ‚ùå Files exist but critical sections missing

**Without this check:**
- ‚ö†Ô∏è ETL processes ALL 146K files (wasted compute time)
- ‚ö†Ô∏è Database filled with empty records
- ‚ö†Ô∏è Simulation breaks on missing data
- ‚ö†Ô∏è Cost estimates wrong (paying to process empty files)

**With this check:**
- ‚úÖ Filter files BEFORE ETL processing
- ‚úÖ Document which game IDs/years have valid data
- ‚úÖ Set file size thresholds for quick filtering
- ‚úÖ Accurate cost estimates (only process valid files)

### Single Command

```bash
python scripts/analysis/check_data_availability.py
```

**From project root:**
```bash
cd /Users/ryanranft/nba-simulator-aws
conda activate nba-aws
python scripts/analysis/check_data_availability.py
```

### What This Does

**4-step automated analysis:**

#### Step 1: Scan Local Data Directories

Searches for JSON files in 4 data type directories:

- `data/nba_pbp/` - Play-by-play files (~36,000 files)
- `data/nba_box_score/` - Box score files (~36,000 files)
- `data/nba_schedule_json/` - Schedule files (~33 files, 1 per year)
- `data/nba_team_stats/` - Team statistics files (~36,000 files)

**Total files scanned:** ~146,000 files

#### Step 2: Sample Files for Analysis

For each data type:
- **Sample size:** 200 random files (statistically significant)
- **Why sample?** Analyzing 146K files would take hours
- **Accuracy:** 200-file sample gives ¬±7% margin of error at 95% confidence

#### Step 3: Check Each File for Valid Data

**For play-by-play files:**
```python
# Looks for: gamepackageJSON.shtChrt.plays[]
# Valid if: len(plays) > 0
# Returns: play_count, file_size
```

**For box score files:**
```python
# Looks for: gamepackageJSON.boxscore.players[].statistics[].athletes[]
# Valid if: player_count > 0
# Returns: player_count, file_size
```

**For schedule files:**
```python
# Looks for: content.sbData.events[]
# Valid if: game_count > 0
# Returns: game_count, file_size
```

**For team stats files:**
```python
# Looks for: gamepackageJSON.boxscore.teams[]
# Valid if: team_count > 0
# Returns: team_count, file_size
```

#### Step 4: Generate Analysis Report

**For each data type, reports:**

1. **Sample Results** - Valid/Empty/Error percentages from sample
2. **Estimated Full Dataset** - Extrapolates to all files
3. **File Size Ranges** - Average, min, max for valid and empty files
4. **Sample Files** - Shows 3 valid and 3 empty file examples
5. **Errors** - Shows any JSON parsing errors encountered

**Final summary:**
- Total files across all types
- Estimated valid files
- Storage impact (wasted GB on empty files)
- ETL implications and recommendations

### Example Output

```bash
$ python scripts/analysis/check_data_availability.py

======================================================================
NBA Data Availability Analysis
======================================================================

Checking which files contain actual game data vs ESPN metadata...

======================================================================
Analyzing play-by-play files in: nba_pbp/
======================================================================
Total files: 36,542
Sampling: 200 files

üìä Sample Results:
  ‚úÖ Valid:  147 / 200 (73.5%)
  ‚ùå Empty:   48 / 200 (24.0%)
  ‚ö†Ô∏è  Errors:   5 / 200 ( 2.5%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~26,858 files
  ‚ùå Empty:  ~8,770 files
  ‚ö†Ô∏è  Errors: ~914 files

üì¶ Valid File Sizes:
  Average: 245.3 KB
  Range:   12.4 KB - 1,892.7 KB

üì¶ Empty File Sizes:
  Average: 8.2 KB
  Range:   1.1 KB - 15.3 KB

üîç Sample Valid Files:
  401584849.json - 487 plays, 312.5 KB
  401161551.json - 523 plays, 289.4 KB
  401359963.json - 445 plays, 276.1 KB

üîç Sample Empty Files:
  400989234.json - 8.7 KB
  401234561.json - 7.9 KB
  401345678.json - 8.4 KB

======================================================================
Analyzing box_scores files in: nba_box_score/
======================================================================
Total files: 36,542
Sampling: 200 files

üìä Sample Results:
  ‚úÖ Valid:  152 / 200 (76.0%)
  ‚ùå Empty:   43 / 200 (21.5%)
  ‚ö†Ô∏è  Errors:   5 / 200 ( 2.5%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~27,772 files
  ‚ùå Empty:  ~7,856 files
  ‚ö†Ô∏è  Errors: ~914 files

üì¶ Valid File Sizes:
  Average: 187.6 KB
  Range:   34.2 KB - 567.8 KB

üîç Sample Valid Files:
  401584849.json - 28 players, 198.3 KB
  401161551.json - 26 players, 183.2 KB
  401359963.json - 27 players, 192.4 KB

======================================================================
Analyzing schedule files in: nba_schedule_json/
======================================================================
Total files: 33
Sampling: 33 files

üìä Sample Results:
  ‚úÖ Valid:   33 / 33 (100.0%)
  ‚ùå Empty:    0 / 33 (  0.0%)
  ‚ö†Ô∏è  Errors:   0 / 33 (  0.0%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~33 files
  ‚ùå Empty:  ~0 files

üîç Sample Valid Files:
  2023.json - 1,312 games, 2,456.7 KB
  2022.json - 1,298 games, 2,398.1 KB
  2021.json - 1,080 games, 2,134.6 KB

======================================================================
Analyzing team_stats files in: nba_team_stats/
======================================================================
Total files: 36,542
Sampling: 200 files

üìä Sample Results:
  ‚úÖ Valid:  149 / 200 (74.5%)
  ‚ùå Empty:   46 / 200 (23.0%)
  ‚ö†Ô∏è  Errors:   5 / 200 ( 2.5%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~27,224 files
  ‚ùå Empty:  ~8,405 files
  ‚ö†Ô∏è  Errors: ~913 files

======================================================================
SUMMARY: Data Availability Across All File Types
======================================================================

üìä Overall Statistics:
  Total files:        146,115
  Est. valid files:   81,887 (56.0%)
  Est. empty files:   64,228 (44.0%)

üíæ Storage Impact:
  Current S3:         119 GB (all files)
  Est. valid data:    ~66.6 GB
  Est. waste:         ~52.4 GB (44% of storage)

‚ö†Ô∏è  ETL IMPLICATIONS:
  - Not all files contain usable game data
  - Filter files BEFORE processing to save compute time
  - Consider file size thresholds for quick filtering
  - Document which game IDs/patterns have valid data

======================================================================
```

### How to Use Results

**Scenario 1: Planning ETL pipeline**
```
Analysis shows: 56% valid files (81,887 out of 146,115)

Action:
1. Add file size filter to ETL: Skip files < 20 KB (likely empty)
2. Expected Glue processing time: 56% less than estimated
3. Expected Glue cost: 56% less than estimated
4. Update PROGRESS.md cost estimates accordingly
```

**Scenario 2: Investigating data gaps**
```
Analysis shows: 2.5% files have JSON errors

Action:
1. Check error_files list for patterns
2. Re-extract problematic game IDs from ESPN API
3. Document known bad game IDs in TROUBLESHOOTING.md
```

**Scenario 3: Optimizing storage costs**
```
Analysis shows: ~52.4 GB wasted on empty files (44% of bucket)

Action:
1. Option A: Delete empty files (save $1.20/month in S3)
2. Option B: Keep for audit trail (ESPN metadata useful)
3. Decision: Keep (cost low, metadata valuable for debugging)
```

**Scenario 4: Setting up file filters**
```
Analysis shows:
  Valid files: Average 245 KB, Range 12-1,892 KB
  Empty files: Average 8 KB, Range 1-15 KB

Action:
1. Set ETL filter: file_size > 20 KB (catches 95%+ of valid files)
2. Test filter on sample: python scripts/etl/test_file_filter.py --min-size 20000
3. Update Glue job with filter logic
```

### Integration with Other Workflows

**Before ETL planning (Phase 2):**
```bash
# 1. Check data availability
python scripts/analysis/check_data_availability.py > data_availability_report.txt

# 2. Review results
less data_availability_report.txt

# 3. Update PROGRESS.md Phase 2 estimates with actual percentages
vim PROGRESS.md  # Adjust Glue job cost estimates

# 4. Commit findings
git add data_availability_report.txt PROGRESS.md
git commit -m "Add data availability analysis, adjust Phase 2 estimates"
```

**After data extraction:**
```bash
# 1. Extract new data
python scripts/etl/extract_schedule_local.py --year 2025

# 2. Verify extraction quality
python scripts/analysis/check_data_availability.py

# 3. Check if new data shows in valid files count
# Expected: Valid file count increases by ~1,200 (new season games)
```

**Monthly data quality check:**
```bash
# First Monday of month
python scripts/analysis/check_data_availability.py > monthly_checks/data_quality_$(date +%Y%m).txt

# Compare to last month
diff monthly_checks/data_quality_202410.txt monthly_checks/data_quality_202411.txt

# If significant changes (¬±5%), investigate why
```

### Comparison to Manual Checks

| Approach | Time Required | Coverage | Accuracy | Use Case |
|----------|---------------|----------|----------|----------|
| **Manual file inspection** | 1-2 hours | 10-20 files | üìâ Limited sample | Quick spot check |
| **check_data_availability.py** | <2 minutes | 800 files (sample) | ‚úÖ Statistically significant | **ETL planning** |
| **Comprehensive scan** | 2-3 hours | All 146K files | ‚úÖ 100% accurate | One-time deep audit |
| **No validation** | 0 seconds | None | ‚ùå Blind guess | Never recommended |

### Best Practices

**Run analysis when:**
- ‚úÖ Before Phase 2 ETL planning (get accurate cost estimates)
- ‚úÖ After any new data extraction (verify quality)
- ‚úÖ When investigating data issues (identify patterns)
- ‚úÖ Monthly as part of data quality monitoring
- ‚úÖ Before major ETL refactoring (understand current state)

**Don't need to run if:**
- ‚úÖ No new data added (results unchanged)
- ‚úÖ Just ran yesterday (<24 hours ago)
- ‚úÖ Only working on simulation logic (not ETL)

### Troubleshooting

**Problem: "No such file or directory: data/nba_pbp"**
```bash
# Cause: Running from wrong directory or data not extracted yet
# Solution: Ensure you're in project root with data directories

cd /Users/ryanranft/nba-simulator-aws
ls -la data/  # Should show nba_pbp, nba_box_score, etc.

# If data directories missing, extract first:
python scripts/etl/extract_pbp_local.py
```

**Problem: "No module named 'json'"**
```bash
# Cause: Python environment issue
# Solution: Use Python 3 with standard library

python3 scripts/analysis/check_data_availability.py

# Or activate conda environment:
conda activate nba-aws
python scripts/analysis/check_data_availability.py
```

**Problem: Analysis takes >5 minutes**
```bash
# Cause: Analyzing too many files or slow disk
# Solution: Reduce sample size

# Edit script line 321-333:
sample_size=50  # Instead of 200

# Or run on SSD instead of network drive
```

**Problem: "KeyError: 'gamepackageJSON'" errors**
```bash
# Cause: Files have different JSON structure (ESPN API changes over time)
# Solution: This is expected - script counts these as errors

# Check error percentage:
# < 5% errors: Normal, ignore
# > 10% errors: Investigate, ESPN may have changed API format
```

**Problem: Sample results vary between runs**
```bash
# Cause: Random sampling (expected behavior)
# Solution: This is normal - sample is random for speed

# For consistent results, increase sample size:
sample_size=500  # More stable percentages

# Or run comprehensive scan (slow but deterministic):
# Modify script to check ALL files instead of sampling
```

### Technical Details

**Sampling methodology:**
- Uses Python's `random.sample()` for unbiased selection
- Sample size: 200 files per data type (800 total)
- Margin of error: ¬±7% at 95% confidence level
- Runtime: <2 minutes for 800 files

**JSON parsing:**
- Uses standard `json` library (no external dependencies)
- Handles malformed JSON gracefully (counts as error)
- UTF-8 encoding assumed (ESPN standard)

**File size heuristics:**
- Valid files: Typically 50-500 KB (actual game data)
- Empty files: Typically 1-20 KB (just ESPN metadata)
- Threshold: 20 KB cutoff catches ~95% of valid files

**Data structure assumptions:**
- Play-by-play: `gamepackageJSON.shtChrt.plays[]`
- Box scores: `gamepackageJSON.boxscore.players[].statistics[].athletes[]`
- Schedule: `content.sbData.events[]`
- Team stats: `gamepackageJSON.boxscore.teams[]`

**Limitations:**
- Assumes local `data/` directory exists (not checking S3)
- Sample-based (not 100% comprehensive)
- Doesn't validate data correctness (only presence/absence)
- ESPN API structure may change over time (script needs updates)

### Future Enhancements (Potential)

**S3-based analysis:**
```bash
# Not yet implemented - would analyze S3 bucket directly
python scripts/analysis/check_data_availability.py --source s3
```

**Per-year breakdown:**
```bash
# Not yet implemented - would show which years have best data quality
python scripts/analysis/check_data_availability.py --by-year
```

**Data quality scoring:**
```bash
# Not yet implemented - would score each file 0-100
# Based on: completeness, format, size, key fields present
```

**Automated filtering:**
```bash
# Not yet implemented - would generate file lists for ETL
python scripts/analysis/check_data_availability.py --generate-filter > valid_files.txt
```

---

### üìä Comprehensive Data Quality Analysis (comprehensive_data_analysis.py)

**Purpose:** Complete quality analysis across ALL 4 data types with storage/cost impact estimates

**Script:** `scripts/analysis/comprehensive_data_analysis.py`

**Location:** Data Validation Workflows

---

#### üéØ Why This Matters

**Problem:** `check_data_availability.py` gives per-type analysis, but you need **holistic project overview**:

- **Strategic planning:** Total valid files across entire dataset (146K files)
- **Cost forecasting:** ETL compute savings from pre-filtering (~44% waste)
- **Storage optimization:** Identify total GB wasted on empty files (~52.4 GB)
- **Architecture decisions:** Inform Glue job design (filter criteria per data type)

**Without comprehensive analysis:**
- ‚ö†Ô∏è Siloed understanding (per-type stats don't reveal big picture)
- ‚ö†Ô∏è Missed optimization opportunities (don't see cumulative waste)
- ‚ö†Ô∏è Inaccurate cost estimates (extrapolation errors)

**With comprehensive analysis:**
- ‚úÖ Complete project overview (all 4 data types in single report)
- ‚úÖ Accurate cost/storage forecasts (validated sampling methodology)
- ‚úÖ Data-driven ETL design (filter criteria backed by statistics)
- ‚úÖ Executive summary (ready for stakeholder presentation)

---

#### üìã What This Does (4-Phase Analysis)

**Sample size:** 300 files per data type (1,200 total samples)
**Runtime:** ~3-5 minutes for complete analysis
**Accuracy:** ¬±5.7% margin of error at 95% confidence

##### Phase 1: Play-by-Play Analysis (1/4)
**Directory:** `data/nba_pbp/` (~36,000 files)
**Check:** Does file contain `playGrps[]` with actual plays AND `shtChrt.plays[]` for shot chart?

**Valid file indicators:**
```json
{
  "page": {
    "content": {
      "gamepackage": {
        "pbp": {
          "playGrps": [
            [...],  // Period 1 plays
            [...]   // Period 2 plays
          ]
        },
        "shtChrt": {
          "plays": [...]  // Shot chart data
        }
      }
    }
  }
}
```

**Metrics extracted:**
- `plays`: Total play count across all periods
- `shots`: Shot chart play count
- `has_gmInfo`: Game metadata present?
- `size_kb`: File size in KB

**Empty file pattern:**
```json
{
  "page": {
    "content": {
      "gamepackage": {
        "pbp": {},        // Empty
        "shtChrt": {}     // Empty
      }
    }
  }
}
```

##### Phase 2: Box Score Analysis (2/4)
**Directory:** `data/nba_box_score/` (~36,000 files)
**Check:** Does file contain `boxscore.players[].statistics[].athletes[]` with player stats?

**Valid file structure:**
```json
{
  "page": {
    "content": {
      "gamepackage": {
        "boxscore": {
          "players": [
            {
              "team": {...},
              "statistics": [
                {
                  "name": "starters",
                  "athletes": [
                    {"athlete": {...}, "stats": [...]}
                  ]
                }
              ]
            }
          ]
        }
      }
    }
  }
}
```

**Metrics extracted:**
- `players`: Total player count (starters + bench)
- `has_boxscore`: Boxscore section exists?
- `size_kb`: File size in KB

##### Phase 3: Team Stats Analysis (3/4)
**Directory:** `data/nba_team_stats/` (~36,000 files)
**Check:** Does file contain `boxscore.teams[]` with team statistics?

**Valid file structure:**
```json
{
  "page": {
    "content": {
      "gamepackage": {
        "boxscore": {
          "teams": [
            {
              "team": {...},
              "statistics": [
                {"name": "fieldGoalPct", "displayValue": "45.2%"},
                {"name": "rebounds", "displayValue": "42"}
              ]
            }
          ]
        }
      }
    }
  }
}
```

**Metrics extracted:**
- `teams`: Team count (should be 2 for valid games)
- `has_stats`: Team statistics arrays present?
- `size_kb`: File size in KB

##### Phase 4: Schedule Analysis (4/4)
**Directory:** `data/nba_schedule_json/` (~33 files)
**Check:** Does file contain `sbData.events[]` with game information?

**Valid file structure:**
```json
{
  "page": {
    "content": {
      "sbData": {
        "events": [
          {
            "id": "401359841",
            "date": "2021-10-19T23:00Z",
            "name": "Lakers @ Warriors",
            "competitions": [...]
          }
        ]
      }
    }
  }
}
```

**Metrics extracted:**
- `games`: Game count (typically 1,230 games per season)
- `has_sbData`: Scoreboard data section exists?
- `size_kb`: File size in KB

---

#### üìä Example Output

```bash
$ python scripts/analysis/comprehensive_data_analysis.py

======================================================================
COMPREHENSIVE NBA DATA QUALITY ANALYSIS
======================================================================

======================================================================
1/4: PLAY-BY-PLAY DATA
======================================================================
Total files: 36,028
Sampling: 300 files
  Processed 100/300...
  Processed 200/300...
  Processed 300/300...

üìä Results:
  ‚úÖ Valid:   167 / 300 (55.7%)
  ‚ùå Empty:   131 / 300 (43.7%)
  ‚ö†Ô∏è  Errors:   2 / 300 ( 0.7%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~20,056 files (55.7%)
  ‚ùå Empty:  ~15,972 files (43.7%)

‚úÖ Sample Valid Files:
  401359841.json: plays=412, shots=87, has_gmInfo=True, size_kb=842.3
  401359842.json: plays=398, shots=79, has_gmInfo=True, size_kb=821.7
  401359843.json: plays=405, shots=82, has_gmInfo=True, size_kb=835.1

‚ùå Sample Empty Files:
  401544521.json: size=18.2 KB
  401544522.json: size=19.1 KB
  401544523.json: size=17.8 KB

======================================================================
2/4: BOX SCORE DATA
======================================================================
Total files: 36,028
Sampling: 300 files

üìä Results:
  ‚úÖ Valid:   169 / 300 (56.3%)
  ‚ùå Empty:   129 / 300 (43.0%)
  ‚ö†Ô∏è  Errors:   2 / 300 ( 0.7%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~20,284 files (56.3%)
  ‚ùå Empty:  ~15,744 files (43.0%)

‚úÖ Sample Valid Files:
  401359841.json: players=28, has_boxscore=True, size_kb=487.2
  401359842.json: players=27, has_boxscore=True, size_kb=472.6
  401359843.json: players=29, has_boxscore=True, size_kb=493.8

======================================================================
3/4: TEAM STATS DATA
======================================================================
Total files: 36,028
Sampling: 300 files

üìä Results:
  ‚úÖ Valid:   170 / 300 (56.7%)
  ‚ùå Empty:   128 / 300 (42.7%)
  ‚ö†Ô∏è  Errors:   2 / 300 ( 0.7%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~20,428 files (56.7%)
  ‚ùå Empty:  ~15,600 files (42.7%)

‚úÖ Sample Valid Files:
  401359841.json: teams=2, has_stats=True, size_kb=487.2
  401359842.json: teams=2, has_stats=True, size_kb=472.6
  401359843.json: teams=2, has_stats=True, size_kb=493.8

======================================================================
4/4: SCHEDULE DATA
======================================================================
Total files: 33
Sampling: 33 files

üìä Results:
  ‚úÖ Valid:    33 / 33 (100.0%)
  ‚ùå Empty:     0 / 33 (  0.0%)
  ‚ö†Ô∏è  Errors:   0 / 33 (  0.0%)

üìà Estimated Full Dataset:
  ‚úÖ Valid:  ~33 files (100.0%)
  ‚ùå Empty:  ~0 files (0.0%)

‚úÖ Sample Valid Files:
  nba_schedule_2021.json: games=1230, has_sbData=True, size_kb=1847.3
  nba_schedule_2020.json: games=1059, has_sbData=True, size_kb=1612.8
  nba_schedule_2019.json: games=1230, has_sbData=True, size_kb=1851.2

======================================================================
OVERALL SUMMARY
======================================================================

üìä Across All Data Types:
  Total files:      108,117
  Est. valid:       60,801 (56.2%)
  Est. empty:       47,316 (43.8%)

üìÅ By Data Type:
  PBP         :  20,056 / 36,028 valid (55.7%)
  BOX         :  20,284 / 36,028 valid (56.3%)
  TEAM        :  20,428 / 36,028 valid (56.7%)
  SCHEDULE    :     33 /     33 valid (100.0%)

üíæ Storage Impact:
  Total S3 storage: ~84.4 GB
  Usable data:      ~50.6 GB (56.2%)
  Waste:            ~33.8 GB (43.8%)

‚ö° ETL Impact:
  Files to skip:    47,316 (43.8%)
  Compute savings:  ~$5.69/month by pre-filtering
  Runtime savings:  ~44% faster ETL

üéØ Recommendations:
  1. Pre-filter ALL file types in Glue ETL before processing
  2. Expected to reduce processing from 108,117 ‚Üí 60,801 files
  3. Document filter criteria for each data type
  4. Consider creating 'valid file manifest' for future runs

======================================================================
```

---

#### üîß How to Use Results

##### Scenario 1: Planning Glue ETL Job Design
**When:** Before writing Glue PySpark scripts in Phase 2

**Use analysis to determine:**
1. **Filter criteria per data type:**
   - PBP: `file_size > 20 KB` (catches ~95% valid files)
   - Box scores: `file_size > 15 KB` (catches ~94% valid files)
   - Team stats: `file_size > 15 KB` (catches ~94% valid files)
   - Schedule: No filter needed (100% valid)

2. **DynamicFrame filtering:**
```python
# In Glue ETL script
pbp_frame = glueContext.create_dynamic_frame.from_catalog(
    database="nba_data_catalog",
    table_name="pbp"
)

# Pre-filter based on comprehensive analysis
valid_pbp = Filter.apply(
    frame=pbp_frame,
    f=lambda x: x["file_size"] > 20480  # 20 KB threshold
)
```

3. **Expected outcomes:**
   - Processing time: 44% faster (~6 hours ‚Üí ~3.4 hours)
   - Glue DPU costs: 44% reduction (~$13/month ‚Üí ~$7.30/month)
   - Database records: Only valid games (no empty placeholders)

##### Scenario 2: Validating Extraction Completeness
**When:** After running local extraction scripts, before ETL

**Compare analysis results against expected dataset:**
```bash
# Expected for complete NBA dataset (2001-2024)
# - 24 seasons √ó ~1,230 games/season = ~29,520 regular season games
# - Plus ~400 playoff games per year = ~9,600 playoff games
# - Total: ~39,120 games

# Comprehensive analysis shows: ~60,801 valid files across 4 types
# Expected: ~39,120 games √ó 3 file types (pbp, box, team) = ~117,360 files
# BUT: 43.8% are empty (playoff games that didn't happen)
# Result: ~60,801 valid files ‚âà expected (~56% of theoretical max)
```

**Validation checklist:**
- ‚úÖ Total files (~108K) matches S3 bucket count
- ‚úÖ Valid percentage (~56%) consistent with known playoff structure
- ‚úÖ Schedule files (33) match years extracted (2001-2024 = 24 years)
- ‚úÖ Empty files (~44%) align with ESPN API behavior (all playoff slots created)

##### Scenario 3: Presenting to Stakeholders
**When:** Weekly status meetings, project reviews

**Use overall summary section:**
- **Storage impact:** "We're storing ~33.8 GB of empty files (44% waste)"
- **Cost optimization:** "Pre-filtering will save ~$5.69/month in Glue costs"
- **Runtime improvement:** "ETL will run 44% faster by skipping empty files"
- **Data quality:** "56.2% of files contain actual game data (rest are ESPN placeholders)"

**Action items from analysis:**
1. Implement file size filtering in Glue ETL scripts
2. Document filter criteria in `docs/DATA_STRUCTURE_GUIDE.md`
3. Consider S3 lifecycle policy to archive empty files to Glacier (cheaper storage)
4. Add data quality metrics to weekly progress reports

##### Scenario 4: Debugging Data Gaps
**When:** Simulation fails due to missing game data

**Use sample file lists to investigate:**
```bash
# Analysis shows which specific files are empty
# Example: 401544521.json is empty (18.2 KB)

# Investigate that game ID
python scripts/utilities/game_id_decoder.py 401544521
# Output: 2022 Playoff Game (Round 1, Game 7 - series ended in 6 games)

# Conclusion: Empty file is expected (game never played)
```

**Debugging workflow:**
1. Check if missing data is in "empty files" list from analysis
2. Use `game_id_decoder.py` to understand game context
3. Determine if gap is expected (unplayed playoff game) or actual missing data
4. If actual gap: Re-run extraction for that specific game ID

---

#### üîÑ Integration with Other Workflows

##### Integration 1: After Local Extraction (Phase 1 Complete)
**Trigger:** After running all local extraction scripts

**Workflow:**
```bash
# Step 1: Verify S3 upload complete
aws s3 ls s3://nba-sim-raw-data-lake/raw-data/ --recursive --summarize
# Expected: 146,115 objects, ~119 GB

# Step 2: Run comprehensive analysis on local mirror
python scripts/analysis/comprehensive_data_analysis.py > analysis_report.txt
# Runtime: ~3-5 minutes

# Step 3: Validate results match expectations
grep "Total files:" analysis_report.txt
# Expected: ~108,117 files (local mirror slightly smaller than S3)

# Step 4: Document filter criteria for ETL
# Save "Recommendations" section to docs/DATA_STRUCTURE_GUIDE.md

# Step 5: Update PROGRESS.md with data quality metrics
echo "Data Quality: 56.2% valid files (60,801 / 108,117)" >> PROGRESS.md
```

**Why this order?**
- ‚úÖ Validates extraction completeness before ETL design
- ‚úÖ Provides data for informed ETL architecture decisions
- ‚úÖ Documents baseline metrics before transformation

##### Integration 2: Before Phase 2 Planning (Glue ETL Design)
**Trigger:** Before writing Glue PySpark scripts

**Workflow:**
```bash
# Step 1: Run comprehensive analysis (if not already done)
python scripts/analysis/comprehensive_data_analysis.py

# Step 2: Extract filter criteria
# From output: "file_size > 20 KB" for PBP (95% accuracy)

# Step 3: Design Glue ETL with pre-filtering
# Create scripts/etl/glue_etl_extract_pbp.py with:
valid_pbp = Filter.apply(
    frame=pbp_frame,
    f=lambda x: x["file_size"] > 20480
)

# Step 4: Update cost estimates in PROGRESS.md
# Original estimate: $13/month for Glue jobs
# With pre-filtering: $7.30/month (44% reduction)

# Step 5: Document ETL design decisions in ADR
echo "ADR: Pre-filter files based on comprehensive data analysis" > docs/adr/005-etl-prefiltering.md
```

**Why this matters?**
- ‚úÖ Prevents wasted compute on empty files (44% savings)
- ‚úÖ Reduces ETL runtime (44% faster)
- ‚úÖ Improves data quality (no empty records in database)

##### Integration 3: Monthly Data Quality Checks
**Trigger:** First Monday of each month (maintenance routine)

**Workflow:**
```bash
# Step 1: Re-run comprehensive analysis
python scripts/analysis/comprehensive_data_analysis.py > monthly_analysis_$(date +%Y%m).txt

# Step 2: Compare to baseline (from Phase 1)
diff analysis_report.txt monthly_analysis_$(date +%Y%m).txt
# Expected: No differences (dataset is static post-extraction)

# Step 3: If differences detected, investigate
# Possible causes:
#   - New data extracted (unexpected)
#   - File corruption (re-download from S3)
#   - Analysis script changes (validate logic)

# Step 4: Archive analysis report
mv monthly_analysis_$(date +%Y%m).txt ~/sports-simulator-archives/nba/analysis/

# Step 5: Update documentation if needed
# If filter criteria changed, update docs/DATA_STRUCTURE_GUIDE.md
```

**Why monthly?**
- ‚úÖ Detects data drift (should be none for static dataset)
- ‚úÖ Validates data integrity (no corruption)
- ‚úÖ Documents historical quality metrics

---

#### üìä Comparison: Comprehensive vs Per-Type Analysis

| Aspect | comprehensive_data_analysis.py | check_data_availability.py | Manual Analysis |
|--------|-------------------------------|---------------------------|----------------|
| **Scope** | All 4 data types | Single data type | 1 file at a time |
| **Sample size** | 300 per type (1,200 total) | 200 per type | Variable |
| **Runtime** | ~3-5 minutes | ~2 minutes per type (~8 min total) | Hours/days |
| **Output** | Overall summary + per-type | Per-type detailed | No structure |
| **Storage impact** | YES (total GB waste) | NO | NO |
| **Cost estimates** | YES (ETL savings) | NO | NO |
| **Filter criteria** | YES (per-type thresholds) | NO | Manual inference |
| **Use case** | Project planning, stakeholder reporting | Deep-dive per data type | Ad-hoc investigation |
| **Accuracy** | ¬±5.7% (larger sample) | ¬±7% (smaller sample) | Variable |
| **Integration** | Monthly checks, ETL design | Initial discovery | One-off |

**When to use each:**

**Use `comprehensive_data_analysis.py` when:**
- ‚úÖ Planning Phase 2 ETL job design (need overall picture)
- ‚úÖ Preparing stakeholder reports (executive summary)
- ‚úÖ Estimating costs (need cumulative savings)
- ‚úÖ Monthly data quality checks (trend analysis)
- ‚úÖ Validating extraction completeness (all types at once)

**Use `check_data_availability.py` when:**
- ‚úÖ Investigating specific data type (e.g., "why are PBP files different?")
- ‚úÖ Debugging extraction issues (focused analysis)
- ‚úÖ Testing new extraction logic (before/after comparison)
- ‚úÖ Need more detailed per-file examples (shows individual file characteristics)

**Use manual analysis when:**
- ‚úÖ Debugging a single file (e.g., "why is 401359841.json empty?")
- ‚úÖ Understanding JSON structure (learning API format)
- ‚úÖ Validating script logic (spot-checking automation)

---

#### üí° Best Practices

1. **Run comprehensive analysis AFTER local extraction completes:**
   ```bash
   # In session_end.sh or manual workflow
   if [ -d "data/nba_pbp" ] && [ -d "data/nba_box_score" ]; then
       echo "Running comprehensive data quality analysis..."
       python scripts/analysis/comprehensive_data_analysis.py > data_quality_report.txt
   fi
   ```

2. **Save analysis output to file (for historical reference):**
   ```bash
   python scripts/analysis/comprehensive_data_analysis.py | tee analysis_$(date +%Y%m%d).txt
   # Archive to: ~/sports-simulator-archives/nba/analysis/
   ```

3. **Use analysis to inform ETL design (evidence-based decisions):**
   ```python
   # In Glue ETL script comments
   # Pre-filter based on comprehensive_data_analysis.py results (2024-02-15):
   # - PBP: 55.7% valid, threshold 20 KB
   # - Box: 56.3% valid, threshold 15 KB
   # - Team: 56.7% valid, threshold 15 KB
   ```

4. **Document filter criteria in DATA_STRUCTURE_GUIDE.md:**
   ```markdown
   ## ETL Filter Criteria (from comprehensive_data_analysis.py)
   - Play-by-play: file_size > 20 KB (95% accuracy)
   - Box scores: file_size > 15 KB (94% accuracy)
   - Team stats: file_size > 15 KB (94% accuracy)
   - Schedule: No filter (100% valid)
   ```

5. **Re-run monthly to detect data drift:**
   ```bash
   # First Monday of each month
   make monthly-analysis  # (add to Makefile)
   ```

---

#### üêõ Troubleshooting

##### Problem 1: "Directory not found" errors
**Symptom:**
```
‚ùå Directory not found: /Users/ryanranft/nba-simulator-aws/data/nba_pbp
```

**Causes:**
- Local data not extracted yet (extraction scripts not run)
- Directory path wrong (script expects project_root/data/)
- Data in different location (custom ARCHIVE_BASE)

**Solutions:**
```bash
# Check if local data exists
ls -la data/
# Expected: nba_pbp/ nba_box_score/ nba_team_stats/ nba_schedule_json/

# If missing, verify S3 bucket has data
aws s3 ls s3://nba-sim-raw-data-lake/raw-data/ --summarize
# Expected: 146,115 objects

# Download sample for testing
aws s3 sync s3://nba-sim-raw-data-lake/raw-data/nba_pbp/ data/nba_pbp/ --dryrun
# Remove --dryrun to actually download

# Or: Run extraction scripts
python scripts/etl/extract_pbp_local.py
```

##### Problem 2: Analysis shows 0% valid files
**Symptom:**
```
üìä Results:
  ‚úÖ Valid:     0 / 300 (0.0%)
  ‚ùå Empty:   300 / 300 (100.0%)
```

**Causes:**
- JSON structure changed (ESPN API update)
- Script checking wrong JSON paths
- Files corrupted during download

**Solutions:**
```bash
# Inspect a sample file manually
cat data/nba_pbp/401359841.json | jq . | head -50
# Look for structure differences

# Check if file has data but script doesn't detect it
python -c "
import json
from pathlib import Path

with open('data/nba_pbp/401359841.json') as f:
    data = json.load(f)

# Check expected path
pkg = data.get('page', {}).get('content', {}).get('gamepackage', {})
print('pbp' in pkg)  # Should be True
print('shtChrt' in pkg)  # Should be True
"

# If structure changed, update check functions in script
# Edit scripts/analysis/comprehensive_data_analysis.py
```

##### Problem 3: Script runs very slowly (>10 minutes)
**Symptom:**
```
Analyzing PLAY-BY-PLAY DATA
Total files: 36,028
Sampling: 300 files
  Processed 100/300...  [10 minutes elapsed]
```

**Causes:**
- Large files (>5 MB) slow down JSON parsing
- Disk I/O bottleneck (reading from network drive)
- Python memory issues (loading entire files)

**Solutions:**
```bash
# Reduce sample size for faster testing
python scripts/analysis/comprehensive_data_analysis.py --sample-size 100
# (Requires script modification to accept CLI arg)

# Check file sizes
find data/nba_pbp -name "*.json" -exec ls -lh {} \; | sort -k5 -hr | head -10
# If any >5 MB, investigate why (should be ~800 KB average)

# Run on local SSD instead of network drive
# Copy data to local disk first
cp -r data/ /tmp/data_local/
# Update script to use /tmp/data_local/
```

##### Problem 4: Error percentage is high (>5%)
**Symptom:**
```
üìä Results:
  ‚úÖ Valid:   150 / 300 (50.0%)
  ‚ùå Empty:   120 / 300 (40.0%)
  ‚ö†Ô∏è  Errors:  30 / 300 (10.0%)  ‚Üê HIGH!
```

**Causes:**
- Malformed JSON files (incomplete downloads)
- Encoding issues (UTF-8 vs Latin-1)
- Permission errors (can't read files)

**Solutions:**
```bash
# List error files from script output
# (Script prints error files with error messages)

# Check one error file manually
cat data/nba_pbp/401359841.json
# Look for JSON syntax errors

# Validate JSON
jq empty data/nba_pbp/401359841.json
# If error: "parse error: ..."

# Check encoding
file data/nba_pbp/401359841.json
# Expected: "JSON data"

# Re-download corrupted files from S3
aws s3 cp s3://nba-sim-raw-data-lake/raw-data/nba_pbp/401359841.json data/nba_pbp/401359841.json
```

##### Problem 5: Extrapolated estimates don't match S3 bucket count
**Symptom:**
```
üìä Across All Data Types:
  Total files:      108,117  ‚Üê Should be ~146,115
```

**Causes:**
- Local data incomplete (not all files downloaded)
- Analysis missing data directories
- S3 bucket has files not in local mirror

**Solutions:**
```bash
# Compare local to S3
LOCAL_COUNT=$(find data/ -name "*.json" | wc -l)
S3_COUNT=$(aws s3 ls s3://nba-sim-raw-data-lake/raw-data/ --recursive | grep ".json" | wc -l)

echo "Local: $LOCAL_COUNT"
echo "S3: $S3_COUNT"
# If different, local is incomplete

# Sync missing files from S3
aws s3 sync s3://nba-sim-raw-data-lake/raw-data/ data/ --exclude "*" --include "*.json"

# Re-run analysis after sync
python scripts/analysis/comprehensive_data_analysis.py
```

---

#### üîç Technical Details

**Script structure:**
```python
# 4 check functions (one per data type)
check_pbp_data()         # Returns: (has_data: bool, info: Dict)
check_box_score_data()   # Returns: (has_data: bool, info: Dict)
check_team_stats_data()  # Returns: (has_data: bool, info: Dict)
check_schedule_data()    # Returns: (has_data: bool, info: Dict)

# 1 analysis orchestrator
analyze_directory(dir_path, check_func, data_type, sample_size=300)
  # Samples files, runs check_func on each, calculates statistics

# 1 main coordinator
main()
  # Runs analyze_directory for all 4 types, generates overall summary
```

**Sampling methodology:**
- **Method:** `random.sample()` (unbiased, no replacement)
- **Sample size:** 300 per data type (1,200 total)
- **Confidence:** 95% (standard for statistical analysis)
- **Margin of error:** ¬±5.7% for 300-sample (vs ¬±7% for 200-sample in check_data_availability.py)
- **Extrapolation:** Linear scaling (valid_count / sample_size √ó total_files)

**JSON parsing approach:**
```python
# Safe nested dictionary access (no exceptions)
pkg = data.get('page', {}).get('content', {}).get('gamepackage', {})

# Check for data existence
plays = pkg.get('pbp', {}).get('playGrps', [])
has_data = len(plays) > 0

# Extract metrics
info = {
    'plays': play_count,
    'size_kb': file_path.stat().st_size / 1024,
    'error': None  # Or exception message
}
```

**Performance characteristics:**
- **Disk reads:** 1,200 files (300 per type √ó 4 types)
- **Average file size:** ~800 KB
- **Total data read:** ~960 MB
- **JSON parsing:** ~1,200 `json.load()` calls
- **Expected runtime:** 3-5 minutes on SSD, 10-15 minutes on HDD

**Limitations:**
1. **Local data only** - Doesn't analyze S3 bucket directly (requires download)
2. **Sample-based** - Not exhaustive (¬±5.7% error margin)
3. **File size estimates** - Storage calculations use averages (not precise per-file)
4. **Static thresholds** - File size cutoffs (20 KB, 15 KB) may need adjustment if ESPN API changes
5. **No historical tracking** - Doesn't compare to previous runs (manual diff required)

---

#### üöÄ Future Enhancements

**1. S3-based analysis (no local download required):**
```python
# Use boto3 to stream files from S3
s3 = boto3.client('s3')
obj = s3.get_object(Bucket='nba-sim-raw-data-lake', Key='raw-data/nba_pbp/401359841.json')
data = json.loads(obj['Body'].read())
```

**2. Historical trend tracking:**
```python
# Save results to database, track over time
results['timestamp'] = datetime.now()
save_to_db(results)

# Generate trend chart
plot_data_quality_over_time()
```

**3. Automated filter generation:**
```python
# Output Glue ETL filter code automatically
generate_glue_filter_code(results)
# Creates: scripts/etl/filters_generated.py
```

**4. Data quality scoring:**
```python
# Assign quality score per file (0-100)
quality_score = calculate_quality(
    has_plays=True,
    play_count=412,
    file_size_kb=842,
    has_errors=False
)
# Score: 95 (excellent)
```

**5. Integration with data catalog:**
```python
# Update Glue Data Catalog with quality metrics
glue.update_table(
    DatabaseName='nba_data_catalog',
    TableInput={
        'Name': 'pbp',
        'Parameters': {
            'data_quality_pct': '55.7',
            'last_analyzed': '2024-02-15'
        }
    }
)
```

---

### Pre-ETL Validation (Before Running Glue Jobs)

**7-step checklist:**

1. ‚úÖ **Verify S3 bucket structure**
   ```bash
   aws s3 ls s3://nba-sim-raw-data-lake/raw-data/ --recursive | head -20
   ```
2. ‚úÖ **Verify partition structure** (year-based)
   ```bash
   bash scripts/shell/check_partition_status.sh
   ```
3. ‚úÖ **Test with sample data** (1-2 files)
   ```python
   python scripts/etl/test_sample_extraction.py
   ```
4. ‚úÖ **Verify Glue Crawler configuration**
   ```bash
   aws glue get-crawler --name nba-sim-crawler
   ```
5. ‚úÖ **Check RDS connectivity**
   ```bash
   psql -h <endpoint> -U <user> -d nba_sim -c "SELECT 1"
   ```
6. ‚úÖ **Verify disk space** (local and RDS)
   ```bash
   df -h
   aws rds describe-db-instances --query 'DBInstances[0].AllocatedStorage'
   ```
7. ‚úÖ **Review ETL script logs** (check for errors)
   ```bash
   tail -100 logs/etl_execution.log
   ```

### Pre-Simulation Validation (Before Running Simulation)

**5-step checklist:**

1. ‚úÖ **Verify database schema**
   ```sql
   \dt  -- List tables
   SELECT COUNT(*) FROM games;  -- Check row counts
   ```
2. ‚úÖ **Test simulation with small sample** (1 season)
   ```python
   python scripts/simulation/test_single_season.py
   ```
3. ‚úÖ **Verify randomization** (run twice, different results)
4. ‚úÖ **Check output format** (matches expectations)
5. ‚úÖ **Review performance** (timing acceptable for full run)

### Pre-ML Training Validation (Before Training Models)

**5-step checklist:**

1. ‚úÖ **Verify training data quality**
   ```python
   python scripts/ml/validate_training_data.py
   ```
2. ‚úÖ **Check for missing values** (imputation strategy)
3. ‚úÖ **Verify feature engineering** (test transforms)
4. ‚úÖ **Split train/test/validation sets** (70/20/10)
5. ‚úÖ **Test model training** (small epoch count first)

**Complete validation procedures:** See `docs/TESTING.md` lines 615-639

---

## üîç Systematic Troubleshooting Protocol

**When any operation fails or behaves unexpectedly:**

### Step 1: Run Health Check
```bash
source scripts/shell/session_manager.sh start
```
**Review output for:**
- Environment issues (conda, Python, packages)
- AWS credential issues
- Git state problems
- Disk space issues

### Step 2: Check Documentation
```bash
# Search TROUBLESHOOTING.md for keywords
grep -i "<error_keyword>" docs/TROUBLESHOOTING.md

# Search COMMAND_LOG.md for similar errors
grep -i "<error_keyword>" COMMAND_LOG.md
```

### Step 3: Enable Debug Logging
```python
# Add to Python scripts
import logging
logging.basicConfig(level=logging.DEBUG)

# For AWS operations
import boto3
boto3.set_stream_logger('boto3.resources', logging.DEBUG)
```

```bash
# For bash scripts
set -x  # Enable verbose output
```

### Step 4: Isolate the Problem
1. **Reproduce with minimal example**
2. **Test components individually**
3. **Check intermediate outputs**
4. **Verify assumptions** (file exists, network connectivity, permissions)

### Step 5: Document and Resolve
1. **Document in COMMAND_LOG.md:**
   - Error message (exact text)
   - Steps to reproduce
   - What you tried
   - What worked
2. **If took >10 min to solve:** Offer to add to TROUBLESHOOTING.md
3. **Use `log_solution` helper:**
   ```bash
   log_solution "Brief description of error and fix"
   ```

### Common Troubleshooting Commands

**Environment issues:**
```bash
conda info --envs
conda list boto3
which python
pip show psycopg2-binary
```

**AWS issues:**
```bash
aws sts get-caller-identity  # Verify credentials
aws s3 ls  # Test S3 access
aws rds describe-db-instances  # Check RDS status
```

**Git issues:**
```bash
git status
git log --oneline -5
git remote -v
git config --list | grep user
```

**Disk space issues:**
```bash
df -h  # Check disk usage
du -sh ~/sports-simulator-archives/nba/*  # Archive sizes
find . -type f -size +100M  # Large files
```

**See `docs/TROUBLESHOOTING.md` for complete error catalog**

---

## üîê Credential Rotation Workflow

**Follow 90-day rotation schedule for security**

### Rotation Schedule

| Credential Type | Frequency | Next Rotation | Priority |
|-----------------|-----------|---------------|----------|
| AWS Access Keys | 90 days | Check IAM | üî¥ High |
| AWS Secret Keys | 90 days | Check IAM | üî¥ High |
| SSH Keys | 365 days | Check GitHub | üü° Medium |
| RDS Passwords | 90 days | After RDS created | üî¥ High |
| API Tokens | 90 days | Service-specific | üü° Medium |

### AWS Credential Rotation Procedure

1. **Check current key age:**
   ```bash
   aws iam get-credential-report
   # Look for "access_key_1_last_rotated"
   ```

2. **Generate new key:**
   - AWS Console ‚Üí IAM ‚Üí Users ‚Üí Security Credentials
   - Click "Create access key"
   - Save new key securely (DO NOT commit)

3. **Update local credentials:**
   ```bash
   # Edit ~/.aws/credentials
   [default]
   aws_access_key_id = <NEW_KEY>
   aws_secret_access_key = <NEW_SECRET>
   ```

4. **Test new credentials:**
   ```bash
   aws sts get-caller-identity
   aws s3 ls  # Verify S3 access
   ```

5. **Deactivate old key:**
   - AWS Console ‚Üí IAM ‚Üí Users ‚Üí Security Credentials
   - Click "Make inactive" on old key

6. **Wait 24 hours** (verify no issues)

7. **Delete old key:**
   - AWS Console ‚Üí IAM ‚Üí Users ‚Üí Security Credentials
   - Click "Delete" on inactive key

8. **Document rotation:**
   ```bash
   echo "AWS credentials rotated on $(date)" >> MACHINE_SPECS.md
   ```

### Emergency Rotation (If Compromised)

**If credentials exposed (committed to GitHub, logged, etc.):**

1. **IMMEDIATELY deactivate compromised credential**
2. **Generate new credential**
3. **Update all systems using old credential**
4. **Delete compromised credential**
5. **Review CloudTrail logs** for unauthorized access:
   ```bash
   aws cloudtrail lookup-events --max-results 50
   ```
6. **Document incident** in TROUBLESHOOTING.md
7. **Run security audit:**
   ```bash
   bash scripts/shell/security_scan.sh
   ```

**Set calendar reminders:** 85 days after each rotation

---

## ‚òÅÔ∏è AWS Resource Setup Workflows

**Phase-specific workflows for provisioning AWS resources**

### General AWS Resource Creation Pattern

**Follow this pattern for ALL AWS resource creation:**

1. **Review PROGRESS.md** for resource specifications
2. **Estimate monthly cost** (see Cost Management Workflows)
3. **Warn user with cost estimate** and get explicit approval
4. **Check prerequisites** (VPC, security groups, IAM roles)
5. **Create resource** via AWS Console or CLI
6. **Verify creation** (describe command, check status)
7. **Test connectivity** (if applicable)
8. **Document endpoint/ARN** in PROGRESS.md
9. **Update cost actuals** in PROGRESS.md
10. **Run `make sync-progress`** to verify documentation matches reality

### Phase 2: Year-Based ETL Pipeline Workflow (Phase 2.2)

**Purpose:** Process 146K+ S3 files efficiently using year-based partitioning and crawlers

**Challenge:** Traditional single-crawler approach overwhelms Glue with 146K files. Solution: Partition by year (33 years), create year-specific crawlers, process incrementally.

**Complete pipeline:** 6 scripts working in sequence

---

#### üìã Local ETL Alternative: Schedule Data Extraction (extract_schedule_local.py)

**Script location:** `scripts/etl/extract_schedule_local.py` (567 lines)

##### üéØ Why This Matters

**Purpose:** Local ETL alternative to AWS Glue for extracting schedule data from S3 ‚Üí RDS PostgreSQL

**When to use this instead of Glue:**
- **Development/testing:** Test ETL logic locally before deploying to AWS Glue
- **Cost savings:** Avoid Glue DPU charges (~$0.44/hour) for small year ranges
- **Debugging:** Easier to debug locally with print statements and breakpoints
- **Quick extractions:** Single year or small range (<5 years) faster locally
- **No AWS Glue setup:** RDS exists but Glue jobs not yet configured

**When NOT to use (use Glue instead):**
- **Full historical load:** 1993-2025 (33 years) better suited for parallel Glue jobs
- **Production pipelines:** Automated recurring jobs should use Glue
- **Large-scale processing:** >10 years at once (local processing slow)
- **Team collaboration:** Shared Glue jobs provide better visibility

**Key difference from Glue:**
- **Glue:** Distributed processing, auto-scaling, integrated with Data Catalog, production-ready
- **Local:** Single-threaded, runs on laptop, direct S3/RDS access, development-friendly

---

##### üìã What This Does (5-Step Process)

**High-level flow:**
```
S3 (schedule/*.json) ‚Üí Parse ESPN JSON ‚Üí Extract 53 fields ‚Üí Deduplicate ‚Üí Batch insert to RDS
```

**Sample size:** Varies by year (1993 has ~1,200 games, 2024 has ~1,230 games)

**Runtime:** ~2-5 minutes per year (depends on file count and network speed)

**Accuracy:** 100% of games in valid JSON files (no sampling)

---

###### Step 1: Environment Validation

**Check:** Verifies all required environment variables are set

**Required variables:**
```bash
DB_HOST=nba-simulator-db.xxxx.us-east-1.rds.amazonaws.com
DB_NAME=nba_simulator
DB_USER=nba_admin
DB_PASSWORD=<secure_password>
AWS_DEFAULT_REGION=us-east-1
```

**Source file:** `/Users/ryanranft/nba-sim-credentials.env`

**Error handling:**
```python
def validate_environment():
    required_vars = ['DB_HOST', 'DB_NAME', 'DB_USER', 'DB_PASSWORD']
    missing = [var for var in required_vars if not os.environ.get(var)]
    if missing:
        print("‚ùå ERROR: Required environment variables not set:")
        for var in missing:
            print(f"  - {var}")
        print("\nüí° Run: source /Users/ryanranft/nba-sim-credentials.env")
        sys.exit(1)
```

**Output (if missing variables):**
```
‚ùå ERROR: Required environment variables not set:
  - DB_HOST
  - DB_USER
  - DB_PASSWORD

üí° Run: source /Users/ryanranft/nba-sim-credentials.env
```

---

###### Step 2: S3 File Discovery

**Check:** Lists all schedule JSON files for specified year(s) from S3

**S3 structure:**
```
s3://nba-sim-raw-data-lake/schedule/
‚îú‚îÄ‚îÄ 19931105.json (Nov 5, 1993)
‚îú‚îÄ‚îÄ 19931106.json (Nov 6, 1993)
‚îú‚îÄ‚îÄ 19931123.json (Nov 23, 1993)
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ 20240115.json (Jan 15, 2024)
‚îú‚îÄ‚îÄ 20240116.json (Jan 16, 2024)
‚îî‚îÄ‚îÄ 20250410.json (Apr 10, 2025)
```

**File naming:** `YYYYMMDD.json` (8-digit date format)

**Discovery logic:**
```python
def get_s3_files_for_year(year: int) -> List[str]:
    s3 = boto3.client('s3')
    prefix = 'schedule/'
    paginator = s3.get_paginator('list_objects_v2')
    pages = paginator.paginate(Bucket='nba-sim-raw-data-lake', Prefix=prefix)

    files = []
    year_str = str(year)  # "1993"

    for page in pages:
        for obj in page.get('Contents', []):
            key = obj['Key']
            filename = key.split('/')[-1]
            # Filter: "19931105.json" starts with "1993"
            if filename.startswith(year_str) and filename.endswith('.json'):
                files.append(key)

    return sorted(files)
```

**Example output (1993):**
```
üìÇ Found 365 schedule files for year 1993
   schedule/19931101.json
   schedule/19931102.json
   ...
   schedule/19931231.json
```

---

###### Step 3: JSON Parsing & Game Extraction

**Check:** Parses ESPN JSON structure and extracts comprehensive game data (53 fields)

**ESPN JSON structure (varies by year):**

**Format 1 (newer files):**
```json
{
  "page": {
    "content": {
      "events": [
        {
          "id": "401468215",
          "date": "2023-10-24T23:30Z",
          "competitions": [
            {
              "competitors": [
                {
                  "homeAway": "home",
                  "team": {
                    "id": "16",
                    "abbrev": "LAL",
                    "displayName": "Los Angeles Lakers",
                    "logo": "https://...",
                    "teamColor": "552583"
                  },
                  "score": "95",
                  "winner": false,
                  "leaders": [
                    {
                      "name": "points",
                      "displayName": "Points",
                      "leaders": [
                        {
                          "athlete": {"displayName": "LeBron James"},
                          "value": 21
                        }
                      ]
                    }
                  ]
                }
              ],
              "venue": {
                "fullName": "Crypto.com Arena",
                "address": {
                  "city": "Los Angeles",
                  "state": "CA"
                }
              },
              "status": {
                "type": {
                  "completed": true,
                  "description": "Final"
                }
              },
              "broadcasts": [
                {
                  "market": "national",
                  "names": ["ESPN"]
                }
              ]
            }
          ]
        }
      ]
    }
  }
}
```

**Format 2 (older files):**
```json
{
  "page": {
    "content": {
      "schedule": {
        "1993-11-05": [
          {
            "id": "100000001",
            "date": "1993-11-05T19:30Z",
            "competitions": [...]
          }
        ],
        "1993-11-06": [...]
      }
    }
  }
}
```

**Extraction logic (handles both formats):**
```python
def extract_game_data(json_content: dict) -> List[Dict]:
    page = json_content.get('page', {})

    # Handle both JSON structures
    events = None
    if 'content' in page and 'schedule' in page['content']:
        events = page['content']['schedule']  # Format 2 (older)
    elif 'content' in page and 'events' in page['content']:
        events = page['content']['events']    # Format 1 (newer)

    # Extract ALL games from ALL dates
    all_games = []
    if isinstance(events, dict):
        # Format 2: {"1993-11-05": [...], "1993-11-06": [...]}
        for date_key, date_games in events.items():
            if isinstance(date_games, list):
                all_games.extend(date_games)
    elif isinstance(events, list):
        # Format 1: [game1, game2, ...]
        all_games = events

    # Extract 53 fields per game
    games_list = []
    for game in all_games:
        competition = game.get('competitions', [{}])[0]
        competitors = competition.get('competitors', [])

        # Identify home/away teams
        home_team = next((c for c in competitors if c.get('homeAway') == 'home'), {})
        away_team = next((c for c in competitors if c.get('homeAway') == 'away'), {})

        # Build comprehensive record
        game_record = {
            # Primary identifiers (4 fields)
            'game_id': str(game.get('id')),
            'game_date': game_date.split('T')[0],
            'game_time': game_date.split('T')[1].replace('Z', '') if 'T' in game_date else None,
            'season': f"{season_year}-{str(season_year + 1)[2:]}",

            # Home team (19 fields)
            'home_team_id': str(home_team.get('team', {}).get('id')),
            'home_team_abbrev': home_team.get('team', {}).get('abbrev'),
            'home_team_name': home_team.get('team', {}).get('displayName'),
            'home_team_logo': home_team.get('team', {}).get('logo'),
            'home_team_color': home_team.get('team', {}).get('teamColor'),
            'home_score': int(home_team['score']) if 'score' in home_team else None,
            'home_team_is_winner': home_team.get('winner', False),
            'home_team_leader_name': None,  # Extracted from leaders array
            'home_team_leader_points': None,
            'home_team_leader_rebounds': None,
            'home_team_leader_assists': None,
            # ... 8 more home fields

            # Away team (19 fields)
            'away_team_id': str(away_team.get('team', {}).get('id')),
            'away_team_abbrev': away_team.get('team', {}).get('abbrev'),
            # ... 17 more away fields

            # Venue (6 fields)
            'venue_name': venue.get('fullName'),
            'venue_city': venue.get('address', {}).get('city'),
            'venue_state': venue.get('address', {}).get('state'),
            'venue_capacity': venue.get('capacity'),
            'venue_indoor': venue.get('indoor'),
            'venue_id': str(venue.get('id')),

            # Status (4 fields)
            'status_completed': status_type.get('completed', False),
            'status_description': status_type.get('description'),
            'status_detail': status_type.get('detail'),
            'status_state': status_type.get('state'),

            # Broadcast (4 fields)
            'broadcast_market': broadcast.get('market'),
            'broadcast_names': ','.join(broadcast.get('names', [])),
            'has_national_broadcast': any(b.get('market') == 'national' for b in broadcasts),
            'has_tickets': competition.get('tickets') is not None,

            # Metadata (7 fields)
            'attendance': competition.get('attendance'),
            'neutral_site': competition.get('neutralSite', False),
            'playoff_game': competition.get('type', {}).get('abbreviation') == 'PST',
            'conference_competition': competition.get('conferenceCompetition', False),
            'recent': competition.get('recent', False),
            'created_at': 'CURRENT_TIMESTAMP',
            'updated_at': 'CURRENT_TIMESTAMP'
        }

        games_list.append(game_record)

    return games_list
```

**Output:**
```
üìä Extracted 1,230 games from 365 JSON files (year 1993)
```

---

###### Step 4: Deduplication & Team Validation

**Check:** Remove duplicate game_ids and ensure all referenced teams exist in teams table

**Deduplication logic:**
```python
def process_year(year: int, dry_run: bool = False):
    # ... extraction logic ...

    # Remove duplicates (keep first occurrence)
    seen_ids = set()
    unique_games = []
    for game in games_to_insert:
        if game['game_id'] not in seen_ids:
            seen_ids.add(game['game_id'])
            unique_games.append(game)

    duplicates_removed = len(games_to_insert) - len(unique_games)
    if duplicates_removed > 0:
        print(f"  ‚ö†Ô∏è  Removed {duplicates_removed} duplicate game_ids")

    games_to_insert = unique_games
```

**Auto-insert missing teams (foreign key constraint):**
```python
# Collect all unique team IDs from games
team_ids = set()
for game in games_to_insert:
    if game['home_team_id']:
        team_ids.add(game['home_team_id'])
    if game['away_team_id']:
        team_ids.add(game['away_team_id'])

# Insert missing teams (avoid foreign key violations)
for team_id in team_ids:
    cursor.execute("""
        INSERT INTO teams (team_id, team_name, team_abbreviation)
        VALUES (%s, %s, %s)
        ON CONFLICT (team_id) DO NOTHING
    """, (team_id, f"Team {team_id}", f"T{team_id}"))

conn.commit()
print(f"  ‚úÖ Ensured {len(team_ids)} teams exist in database")
```

**Output:**
```
  ‚ö†Ô∏è  Removed 15 duplicate game_ids
  ‚úÖ Ensured 30 teams exist in database
```

---

###### Step 5: Batch Insert to RDS

**Check:** Insert all games using batch `execute_values()` with conflict handling

**Batch insert logic (idempotent):**
```python
# Build values list for batch insert
values = []
for game in games_to_insert:
    values.append((
        game['game_id'],
        game['game_date'],
        game['game_time'],
        game['season'],
        # ... 49 more fields
    ))

# Batch insert with conflict handling (ON CONFLICT DO UPDATE)
insert_query = """
    INSERT INTO games (
        game_id, game_date, game_time, season,
        home_team_id, home_team_abbrev, home_team_name, home_team_logo,
        home_team_color, home_score, home_team_is_winner,
        home_team_leader_name, home_team_leader_points,
        home_team_leader_rebounds, home_team_leader_assists,
        away_team_id, away_team_abbrev, away_team_name, away_team_logo,
        away_team_color, away_score, away_team_is_winner,
        away_team_leader_name, away_team_leader_points,
        away_team_leader_rebounds, away_team_leader_assists,
        venue_name, venue_city, venue_state, venue_capacity,
        venue_indoor, venue_id,
        status_completed, status_description, status_detail, status_state,
        broadcast_market, broadcast_names, has_national_broadcast,
        has_tickets, attendance, neutral_site, playoff_game,
        conference_competition, recent, created_at, updated_at
    )
    VALUES %s
    ON CONFLICT (game_id) DO UPDATE SET
        game_date = EXCLUDED.game_date,
        home_score = EXCLUDED.home_score,
        away_score = EXCLUDED.away_score,
        status_completed = EXCLUDED.status_completed,
        status_description = EXCLUDED.status_description,
        updated_at = CURRENT_TIMESTAMP
"""

# Batch insert using psycopg2.extras.execute_values (fast)
from psycopg2.extras import execute_values
execute_values(cursor, insert_query, values)
conn.commit()

print(f"  ‚úÖ Inserted/updated {len(values)} games in database")
```

**Performance optimization:**
- **Batch insert:** Single query for all games (vs. 1,230 individual INSERTs)
- **execute_values():** PostgreSQL-optimized batch insertion
- **Idempotent:** Re-running script updates existing games (safe to retry)

**Output (success):**
```
  ‚úÖ Inserted/updated 1,230 games in database
  ‚è±Ô∏è  Total runtime: 2m 15s
```

---

##### üéÆ How to Use This Script

**Prerequisites:**
1. ‚úÖ RDS PostgreSQL instance running
2. ‚úÖ `games` table created (run `scripts/db_schema.sql`)
3. ‚úÖ Credentials file sourced: `source /Users/ryanranft/nba-sim-credentials.env`
4. ‚úÖ AWS credentials configured (for S3 access)
5. ‚úÖ Python environment activated: `conda activate nba-aws`

---

###### Usage 1: Single Year Extraction

**Command:**
```bash
python scripts/etl/extract_schedule_local.py --year 1993
```

**Output:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üèÄ NBA Schedule Extractor (Local ETL)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Environment validated
‚úÖ Database connection established

üìÇ Processing year: 1993
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  üìÅ Listing S3 files for 1993...
  üìÇ Found 365 schedule files

  üì• Downloading and parsing JSON files...
  ‚è±Ô∏è  Progress: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 365/365 (100%)

  üìä Extracted 1,230 games from 365 JSON files

  üîç Deduplicating games...
  ‚ö†Ô∏è  Removed 15 duplicate game_ids
  ‚úÖ 1,215 unique games ready for insert

  üîß Ensuring teams exist in database...
  ‚úÖ Ensured 30 teams exist in database

  üíæ Batch inserting games to RDS...
  ‚úÖ Inserted/updated 1,215 games in database

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ SUCCESS
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìä Summary:
  - Year: 1993
  - Files processed: 365
  - Games extracted: 1,230
  - Duplicates removed: 15
  - Games inserted: 1,215
  - Runtime: 2m 15s

üí° Next steps:
  - Verify data: SELECT COUNT(*) FROM games WHERE season = '1993-94';
  - Check quality: SELECT * FROM games WHERE home_score IS NULL LIMIT 10;
```

**Use case:** Test ETL logic, load single historical year, backfill missing year

---

###### Usage 2: Year Range Extraction

**Command:**
```bash
python scripts/etl/extract_schedule_local.py --year-range 1993-1995
```

**Output:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üèÄ NBA Schedule Extractor (Local ETL) - Year Range Mode
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ Processing 3 years: 1993, 1994, 1995

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÇ Year 1/3: 1993
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  ... [same output as single year] ...
  ‚úÖ 1,215 games inserted

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÇ Year 2/3: 1994
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  ... [processing 1994] ...
  ‚úÖ 1,189 games inserted

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÇ Year 3/3: 1995
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  ... [processing 1995] ...
  ‚úÖ 1,178 games inserted

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ ALL YEARS COMPLETED
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

üìä Overall Summary:
  - Years processed: 3 (1993-1995)
  - Total files: 1,095
  - Total games: 3,582
  - Total duplicates removed: 47
  - Total games inserted: 3,535
  - Total runtime: 6m 42s
  - Average: 2m 14s per year
```

**Use case:** Load multiple consecutive years, backfill year ranges, incremental loading

---

###### Usage 3: Dry Run (Preview)

**Command:**
```bash
python scripts/etl/extract_schedule_local.py --year 1993 --dry-run
```

**Output:**
```
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üèÄ DRY RUN MODE (No database changes)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

  üìÅ Would process 365 S3 files for year 1993
  üìä Would extract ~1,230 games (estimated)
  üîç Would deduplicate ~15 games (estimated)
  üíæ Would insert/update ~1,215 games

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
Sample games that WOULD be inserted:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Game ID: 100000001
  Date: 1993-11-05
  Season: 1993-94
  Home: Los Angeles Lakers (LAL) - 95 pts
  Away: Portland Trail Blazers (POR) - 102 pts ‚úÖ Winner
  Venue: Crypto.com Arena, Los Angeles, CA
  Status: Final
  Broadcast: ESPN, TNT (National)

Game ID: 100000002
  Date: 1993-11-05
  Season: 1993-94
  Home: Chicago Bulls (CHI) - 108 pts ‚úÖ Winner
  Away: New York Knicks (NYK) - 96 pts
  Venue: United Center, Chicago, IL
  Status: Final
  Broadcast: WGN (Local)

... [showing 10 sample games] ...

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üí° To execute this for real:
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

python scripts/etl/extract_schedule_local.py --year 1993
```

**Use case:** Preview before committing, test S3 access, validate JSON parsing, estimate runtime

---

##### üìä 53 Fields Extracted Per Game

**Field categories:**

**1. Primary Identifiers (4 fields):**
- `game_id` - Unique ESPN game identifier
- `game_date` - YYYY-MM-DD format
- `game_time` - HH:MM:SS format (UTC)
- `season` - "1993-94" format

**2. Home Team (19 fields):**
- `home_team_id`, `home_team_abbrev`, `home_team_name`, `home_team_logo`, `home_team_color`
- `home_score`, `home_team_is_winner`
- `home_team_leader_name`, `home_team_leader_points`, `home_team_leader_rebounds`, `home_team_leader_assists`
- `home_team_leader_steals`, `home_team_leader_blocks`, `home_team_leader_turnovers`
- `home_team_leader_fg_pct`, `home_team_leader_3pt_pct`, `home_team_leader_ft_pct`
- `home_team_leader_plus_minus`, `home_team_leader_minutes`

**3. Away Team (19 fields):**
- Same structure as home team

**4. Venue (6 fields):**
- `venue_name`, `venue_city`, `venue_state`, `venue_capacity`, `venue_indoor`, `venue_id`

**5. Status (4 fields):**
- `status_completed` (boolean)
- `status_description` ("Final", "In Progress", "Scheduled")
- `status_detail` ("Final Score", "End of 4th Quarter")
- `status_state` ("post", "in", "pre")

**6. Broadcast (4 fields):**
- `broadcast_market` ("national", "home", "away")
- `broadcast_names` (comma-separated: "ESPN,TNT,ABC")
- `has_national_broadcast` (boolean)
- `has_tickets` (boolean - ticket availability)

**7. Metadata (7 fields):**
- `attendance` (integer)
- `neutral_site` (boolean)
- `playoff_game` (boolean)
- `conference_competition` (boolean)
- `recent` (boolean - recently played)
- `created_at`, `updated_at` (timestamps)

**Total:** 4 + 19 + 19 + 6 + 4 + 4 + 7 = **63 fields** (53 user-defined + 10 database-managed)

---

##### üîó Integration with Other Workflows

**Workflow 1: After RDS Database Creation**
```bash
# Step 1: Create RDS instance (PROGRESS.md Phase 2.1)
make create-rds

# Step 2: Create schema
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -f scripts/db_schema.sql

# Step 3: Load historical data (local ETL)
source /Users/ryanranft/nba-sim-credentials.env
python scripts/etl/extract_schedule_local.py --year-range 1993-2025

# Step 4: Verify
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT season, COUNT(*) FROM games GROUP BY season ORDER BY season;"
```

**Workflow 2: Before Setting Up Glue Jobs**
```bash
# Use local ETL to test data quality before investing in Glue setup
python scripts/etl/extract_schedule_local.py --year 2024 --dry-run

# If successful, proceed with Glue setup for production
python scripts/etl/create_glue_etl_job.py
```

**Workflow 3: Incremental Updates (New Season)**
```bash
# At start of new NBA season (October)
python scripts/etl/extract_schedule_local.py --year 2025

# Verify new games
psql -c "SELECT COUNT(*) FROM games WHERE season = '2025-26';"
```

**Workflow 4: Backfill Missing Years**
```bash
# Check which years are missing
psql -c "SELECT season FROM games GROUP BY season ORDER BY season;"

# Backfill gap
python scripts/etl/extract_schedule_local.py --year 2010
```

---

##### üìä Comparison: Local ETL vs. AWS Glue

| Aspect | Local ETL (this script) | AWS Glue ETL |
|--------|------------------------|--------------|
| **Execution** | Runs on local laptop | Runs on AWS infrastructure |
| **Parallelization** | Single-threaded | Multi-threaded (DPUs) |
| **Cost** | Free (uses existing AWS creds) | ~$0.44/hour per DPU |
| **Setup time** | Immediate (no AWS config) | Requires Glue job creation |
| **Scalability** | Limited (1-10 years) | Excellent (33 years in parallel) |
| **Debugging** | Easy (print statements, breakpoints) | Harder (CloudWatch logs) |
| **Production use** | Not recommended | Recommended |
| **Team visibility** | Local only | Shared in AWS console |
| **Data Catalog** | Not integrated | Integrated with Glue Catalog |
| **Best for** | Development, testing, small loads | Production, large-scale, automation |

**Decision matrix:**

| Scenario | Use Local ETL | Use Glue ETL |
|----------|---------------|--------------|
| Load 1-5 years | ‚úÖ Yes (faster, free) | ‚ùå No (overkill) |
| Load 10+ years | ‚ö†Ô∏è Maybe (slow) | ‚úÖ Yes (parallel) |
| Load all 33 years | ‚ùå No (too slow) | ‚úÖ Yes (optimized) |
| Test ETL logic | ‚úÖ Yes (easy debug) | ‚ùå No (hard debug) |
| Production pipeline | ‚ùå No (not scalable) | ‚úÖ Yes (automated) |
| Quick backfill | ‚úÖ Yes (immediate) | ‚ö†Ô∏è Maybe (setup time) |
| Recurring jobs | ‚ùå No (manual) | ‚úÖ Yes (scheduled) |

---

##### ‚úÖ Best Practices

**1. Always dry-run first:**
```bash
# Preview before committing
python scripts/etl/extract_schedule_local.py --year 2024 --dry-run
```

**2. Source credentials before running:**
```bash
# Required for database access
source /Users/ryanranft/nba-sim-credentials.env

# Verify
echo $DB_HOST
```

**3. Start with recent years (better data quality):**
```bash
# 2020-2024 has more complete data
python scripts/etl/extract_schedule_local.py --year-range 2020-2024

# Then backfill older years if needed
python scripts/etl/extract_schedule_local.py --year-range 1993-2019
```

**4. Verify after each load:**
```bash
# Check row count
psql -c "SELECT COUNT(*) FROM games WHERE season = '2024-25';"

# Check for nulls
psql -c "SELECT COUNT(*) FROM games WHERE home_score IS NULL;"

# Check for duplicates
psql -c "SELECT game_id, COUNT(*) FROM games GROUP BY game_id HAVING COUNT(*) > 1;"
```

**5. Monitor progress for large ranges:**
```bash
# In separate terminal, watch database grow
watch -n 5 'psql -c "SELECT COUNT(*) FROM games;"'
```

**6. Use year ranges for efficiency:**
```bash
# Instead of 10 separate commands
python scripts/etl/extract_schedule_local.py --year-range 1993-2003

# vs. 10 separate runs
python scripts/etl/extract_schedule_local.py --year 1993
python scripts/etl/extract_schedule_local.py --year 1994
# ... (slow, manual)
```

**7. Log output for troubleshooting:**
```bash
python scripts/etl/extract_schedule_local.py --year 2024 2>&1 | tee extract_schedule_2024.log
```

---

##### ‚ö†Ô∏è Troubleshooting

**Problem 1: Environment variables not set**

**Symptoms:**
```
‚ùå ERROR: Required environment variables not set:
  - DB_HOST
  - DB_USER
  - DB_PASSWORD
```

**Solution:**
```bash
# Source credentials file
source /Users/ryanranft/nba-sim-credentials.env

# Verify
env | grep DB_
```

**Root cause:** Credentials not loaded in current shell session

---

**Problem 2: S3 access denied**

**Symptoms:**
```
botocore.exceptions.NoCredentialsError: Unable to locate credentials
```

**Solution:**
```bash
# Check AWS credentials
aws sts get-caller-identity

# If missing, configure
aws configure

# Or use environment variables
export AWS_ACCESS_KEY_ID=<key>
export AWS_SECRET_ACCESS_KEY=<secret>
export AWS_DEFAULT_REGION=us-east-1
```

**Root cause:** AWS credentials not configured for S3 access

---

**Problem 3: Database connection failed**

**Symptoms:**
```
psycopg2.OperationalError: could not connect to server: Connection timed out
```

**Solution:**
```bash
# Check RDS endpoint
aws rds describe-db-instances --db-instance-identifier nba-simulator-db

# Check security group (must allow your IP on port 5432)
aws ec2 describe-security-groups --group-ids sg-xxxxx

# Test connectivity
nc -zv $DB_HOST 5432

# Verify credentials
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT 1;"
```

**Root cause:** RDS security group not allowing your IP, or RDS instance stopped

---

**Problem 4: Foreign key constraint violation**

**Symptoms:**
```
psycopg2.errors.ForeignKeyViolation: insert or update on table "games" violates foreign key constraint "games_home_team_id_fkey"
```

**Solution:**
```bash
# Script auto-creates missing teams, but if it fails:

# Manually insert missing team
psql -c "INSERT INTO teams (team_id, team_name, team_abbreviation) VALUES ('16', 'Los Angeles Lakers', 'LAL') ON CONFLICT DO NOTHING;"

# Or drop foreign key constraint temporarily
psql -c "ALTER TABLE games DROP CONSTRAINT games_home_team_id_fkey;"
python scripts/etl/extract_schedule_local.py --year 2024
psql -c "ALTER TABLE games ADD CONSTRAINT games_home_team_id_fkey FOREIGN KEY (home_team_id) REFERENCES teams(team_id);"
```

**Root cause:** Teams table missing referenced team_id

---

**Problem 5: JSON parsing error**

**Symptoms:**
```
KeyError: 'page'
AttributeError: 'NoneType' object has no attribute 'get'
```

**Solution:**
```bash
# Download problematic file for inspection
aws s3 cp s3://nba-sim-raw-data-lake/schedule/20241015.json /tmp/

# Inspect structure
jq '.' /tmp/20241015.json

# Common issues:
# - Missing 'page' key ‚Üí file might be error JSON
# - Empty 'events' array ‚Üí no games scheduled that day
# - Malformed JSON ‚Üí re-download from ESPN

# Skip problematic files (add error handling)
# Script already has try/except blocks
```

**Root cause:** ESPN JSON structure varies by year, some files malformed

---

**Problem 6: Slow performance (>10 minutes per year)**

**Symptoms:**
```
‚è±Ô∏è  Progress: [‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë] 120/365 (33%) - ETA: 15m
```

**Solution:**
```bash
# 1. Check network speed
aws s3 cp s3://nba-sim-raw-data-lake/schedule/20241015.json /tmp/ --debug

# 2. Use parallel processing (modify script)
# Currently single-threaded, could parallelize S3 downloads

# 3. Use Glue instead for large ranges
python scripts/etl/create_glue_etl_job.py

# 4. Increase batch size (currently 1,000 per year)
# Modify script: execute_values(cursor, insert_query, values, page_size=5000)
```

**Root cause:** Network latency, single-threaded processing

---

##### üîß Technical Details

**Database schema (games table):**
```sql
CREATE TABLE games (
    game_id VARCHAR(20) PRIMARY KEY,
    game_date DATE NOT NULL,
    game_time TIME,
    season VARCHAR(10),

    home_team_id VARCHAR(10) REFERENCES teams(team_id),
    home_team_abbrev VARCHAR(5),
    home_team_name VARCHAR(100),
    home_score INTEGER,
    home_team_is_winner BOOLEAN,

    away_team_id VARCHAR(10) REFERENCES teams(team_id),
    away_team_abbrev VARCHAR(5),
    away_team_name VARCHAR(100),
    away_score INTEGER,
    away_team_is_winner BOOLEAN,

    venue_name VARCHAR(200),
    venue_city VARCHAR(100),
    venue_state VARCHAR(50),

    status_completed BOOLEAN,
    status_description VARCHAR(50),

    broadcast_names TEXT,
    has_national_broadcast BOOLEAN,

    attendance INTEGER,
    playoff_game BOOLEAN,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_games_season ON games(season);
CREATE INDEX idx_games_date ON games(game_date);
CREATE INDEX idx_games_teams ON games(home_team_id, away_team_id);
```

**Idempotency guarantee:**
- `ON CONFLICT (game_id) DO UPDATE` ensures script can be re-run safely
- Updates existing games with latest data (e.g., final scores)
- Created_at preserved, updated_at refreshed

**Performance characteristics:**
- **S3 download:** ~0.5s per file (network-bound)
- **JSON parsing:** ~0.01s per file (CPU-bound)
- **Database insert:** ~0.1s per batch of 1,000 (I/O-bound)
- **Total:** ~2-5 minutes per year (365 files √ó 0.5s ‚âà 3 minutes)

**Memory usage:**
- **Peak:** ~200 MB (loads all year's games into memory before insert)
- **Steady:** ~50 MB (Python interpreter + libraries)

**Error handling:**
- ‚úÖ Missing environment variables ‚Üí Exit with helpful message
- ‚úÖ S3 access errors ‚Üí Retry 3 times with exponential backoff
- ‚úÖ Malformed JSON ‚Üí Skip file, log error, continue
- ‚úÖ Database errors ‚Üí Rollback transaction, exit with error
- ‚úÖ Duplicate game_ids ‚Üí Deduplicate before insert

---

##### üöÄ Future Enhancements

**1. Parallel S3 downloads:**
```python
# Use concurrent.futures for faster downloads
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(download_and_parse, s3_key) for s3_key in s3_files]
    games = [f.result() for f in futures]
```

**2. Incremental updates (only new games):**
```python
# Check database for latest game_date, only process newer files
cursor.execute("SELECT MAX(game_date) FROM games WHERE season = %s", (season,))
latest_date = cursor.fetchone()[0]

# Filter S3 files by date
new_files = [f for f in s3_files if parse_date(f) > latest_date]
```

**3. Data quality checks:**
```python
# Validate scores (home + away should be > 0)
invalid_scores = [g for g in games if (g['home_score'] or 0) + (g['away_score'] or 0) == 0]
if invalid_scores:
    print(f"‚ö†Ô∏è  {len(invalid_scores)} games with zero scores")
```

**4. Progress persistence:**
```python
# Save progress to file, resume on failure
with open('extract_progress.json', 'w') as f:
    json.dump({'year': year, 'files_processed': processed_count}, f)
```

**5. CloudWatch metrics:**
```python
# Send metrics to CloudWatch for monitoring
cloudwatch = boto3.client('cloudwatch')
cloudwatch.put_metric_data(
    Namespace='NBA-Simulator/ETL',
    MetricData=[{
        'MetricName': 'GamesExtracted',
        'Value': len(games),
        'Unit': 'Count'
    }]
)
```

---

#### üìã Local ETL Alternative: Play-by-Play Data Extraction (extract_pbp_local.py)

**Script location:** `scripts/etl/extract_pbp_local.py` (311 lines)

##### üéØ Why This Matters

**Purpose:** Local ETL alternative to AWS Glue for extracting play-by-play data from S3 ‚Üí RDS PostgreSQL

**Key difference from schedule extraction:**
- **Schedule:** Extracts game-level metadata (53 fields per game)
- **Play-by-Play:** Extracts individual play events (10 fields per play)
- **Volume:** PBP has 100-300 plays per game (~30K-90K total plays per year)

**When to use this instead of Glue:**
- **Development/testing:** Test PBP ETL logic locally before deploying to AWS Glue
- **Cost savings:** Avoid Glue DPU charges (~$0.44/hour) for small year ranges
- **Debugging:** Easier to debug locally with print statements and breakpoints
- **Quick extractions:** Single year or small range (<5 years) faster locally
- **No AWS Glue setup:** RDS exists but Glue jobs not yet configured

**When NOT to use (use Glue instead):**
- **Full historical load:** 1997-2021 (25 years) better suited for parallel Glue jobs
- **Production pipelines:** Automated recurring jobs should use Glue
- **Large-scale processing:** >10 years at once (local processing slow, ~3M plays)
- **Team collaboration:** Shared Glue jobs provide better visibility

**Key architecture difference:**
- **Game discovery:** Queries `games` table first to get game_ids for year
- **S3 lookup:** Uses game_id to construct S3 key: `pbp/{game_id}.json`
- **Foreign key dependency:** Requires games table populated first (run extract_schedule_local.py)

---

##### üìã What This Does (4-Step Process)

**High-level flow:**
```
games table ‚Üí Get game_ids for year ‚Üí S3 (pbp/{game_id}.json) ‚Üí Parse ESPN JSON ‚Üí Extract 10 fields per play ‚Üí Deduplicate ‚Üí Batch insert to RDS
```

**Sample size:** Varies by year (1997 has ~19,000 plays, 2021 has ~120,000 plays)

**Runtime:** ~5-15 minutes per year (depends on game count and network speed)

**Accuracy:** 100% of plays in valid JSON files (no sampling)

---

###### Step 1: Game ID Discovery from Database

**Check:** Queries `games` table to get all game_ids for specified year

**Why this approach:**
- **Foreign key constraint:** play_by_play.game_id references games.game_id
- **Data consistency:** Only process games that exist in games table
- **Efficient filtering:** Database query faster than S3 list operations

**Query logic:**
```python
def get_game_ids_for_year(year: int, cursor) -> List[str]:
    """Get list of game IDs for a specific year from database"""
    query = """
        SELECT game_id
        FROM games
        WHERE game_date >= %s AND game_date < %s
        ORDER BY game_date
    """
    cursor.execute(query, (f'{year}-01-01', f'{year+1}-01-01'))
    return [row[0] for row in cursor.fetchall()]
```

**Example output (1997):**
```
‚úÖ Connected to database: nba-simulator-db.xxxxxx.us-east-1.rds.amazonaws.com
Fetching game IDs from database...
Found 1,189 games for year 1997
```

**Prerequisites:**
- ‚úÖ `games` table populated (run `extract_schedule_local.py` first)
- ‚úÖ Database connection credentials configured

---

###### Step 2: S3 File Lookup by Game ID

**Check:** Fetches PBP JSON files from S3 using game_id as filename

**S3 structure:**
```
s3://nba-sim-raw-data-lake/pbp/
‚îú‚îÄ‚îÄ 100000001.json (game_id for Nov 5, 1993)
‚îú‚îÄ‚îÄ 100000002.json (game_id for Nov 5, 1993)
‚îú‚îÄ‚îÄ 100000003.json (game_id for Nov 6, 1993)
‚îú‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ 401468215.json (game_id for Oct 24, 2023)
‚îî‚îÄ‚îÄ 401468216.json (game_id for Oct 24, 2023)
```

**File naming:** `{game_id}.json` (ESPN game identifier)

**Lookup logic:**
```python
for i, game_id in enumerate(game_ids, 1):
    s3_key = f"pbp/{game_id}.json"

    try:
        # Download and parse JSON
        response = s3.get_object(Bucket=S3_BUCKET, Key=s3_key)
        json_content = json.loads(response['Body'].read())

        # Extract play-by-play data
        plays_from_game = extract_pbp_data(json_content, game_id)

        if plays_from_game:
            plays_to_insert.extend(plays_from_game)
            stats['processed'] += len(plays_from_game)
        else:
            stats['skipped'] += 1

        # Progress update every 100 games
        if i % 100 == 0:
            print(f"  Progress: {i}/{len(game_ids)} games | {len(plays_to_insert)} plays extracted")

    except Exception as e:
        if "NoSuchKey" not in str(e):
            print(f"  ‚ùå Error processing {s3_key}: {e}")
        stats['errors'] += 1
```

**Error handling:**
- **NoSuchKey:** Game exists in database but no PBP file in S3 (silently skipped)
- **Malformed JSON:** Log error, continue processing other games
- **Network errors:** Retry logic built into boto3

**Example output:**
```
  Progress: 100/1,189 games | 12,345 plays extracted
  Progress: 200/1,189 games | 24,678 plays extracted
  Progress: 300/1,189 games | 37,012 plays extracted
  ...
  Progress: 1,100/1,189 games | 143,250 plays extracted

Extraction complete: 147,891 plays ready to insert
```

---

###### Step 3: JSON Parsing & Play Extraction

**Check:** Parses ESPN JSON structure and extracts play-by-play data (10 fields per play)

**ESPN PBP JSON structure:**
```json
{
  "page": {
    "content": {
      "gamepackage": {
        "pbp": {
          "playGrps": [
            [
              {
                "id": "4014682150001",
                "period": {
                  "number": 1,
                  "displayValue": "1st Quarter"
                },
                "clock": {
                  "displayValue": "12:00"
                },
                "text": "LeBron James makes 25-foot three point jumper (Anthony Davis assists)",
                "homeAway": "home",
                "scoringPlay": true,
                "awayScore": 0,
                "homeScore": 3
              },
              {
                "id": "4014682150002",
                "period": {
                  "number": 1,
                  "displayValue": "1st Quarter"
                },
                "clock": {
                  "displayValue": "11:42"
                },
                "text": "Damian Lillard misses 18-foot pullup jump shot",
                "homeAway": "away",
                "scoringPlay": false,
                "awayScore": 0,
                "homeScore": 3
              }
            ],
            [
              {
                "id": "4014682150101",
                "period": {
                  "number": 2,
                  "displayValue": "2nd Quarter"
                },
                "clock": {
                  "displayValue": "12:00"
                },
                "text": "Start of 2nd Quarter"
              }
            ],
            []
          ]
        }
      }
    }
  }
}
```

**Extraction logic:**
```python
def extract_pbp_data(json_content: dict, game_id: str) -> List[Dict]:
    """
    Extract play-by-play data from ESPN JSON structure

    Returns list of play records
    """
    plays_list = []

    try:
        # Navigate to gamepackage.pbp
        if 'page' not in json_content:
            return plays_list

        page = json_content['page']
        content = page.get('content', {})
        gamepackage = content.get('gamepackage', {})
        pbp = gamepackage.get('pbp', {})

        play_groups = pbp.get('playGrps', [])

        if not play_groups:
            return plays_list

        # Each play_group is a list of plays for a period (quarter)
        for period_plays in play_groups:
            if not isinstance(period_plays, list):
                continue

            for play in period_plays:
                play_id = play.get('id')
                if not play_id:
                    continue

                period = play.get('period', {})
                period_number = period.get('number')
                period_display = period.get('displayValue')

                clock = play.get('clock', {})
                clock_display = clock.get('displayValue')

                play_record = {
                    'play_id': str(play_id),
                    'game_id': str(game_id),
                    'period_number': period_number,
                    'period_display': period_display,
                    'clock_display': clock_display,
                    'play_text': play.get('text'),
                    'home_away': play.get('homeAway'),
                    'scoring_play': play.get('scoringPlay', False),
                    'away_score': play.get('awayScore'),
                    'home_score': play.get('homeScore'),
                }
                plays_list.append(play_record)

    except Exception as e:
        print(f"  Warning: Error extracting PBP data from game {game_id}: {e}")

    return plays_list
```

**Field structure (10 fields per play):**

1. **play_id** (VARCHAR) - Unique ESPN play identifier (e.g., "4014682150001")
2. **game_id** (VARCHAR) - Foreign key to games table (e.g., "401468215")
3. **period_number** (INTEGER) - Quarter number (1-4, or 5+ for OT)
4. **period_display** (VARCHAR) - Human-readable period ("1st Quarter", "OT")
5. **clock_display** (VARCHAR) - Game clock time ("12:00", "5:23")
6. **play_text** (TEXT) - Natural language description of play
7. **home_away** (VARCHAR) - Which team executed play ("home", "away")
8. **scoring_play** (BOOLEAN) - Whether play resulted in points
9. **away_score** (INTEGER) - Away team score after this play
10. **home_score** (INTEGER) - Home team score after this play

**Example extracted plays:**
```python
[
  {
    "play_id": "4014682150001",
    "game_id": "401468215",
    "period_number": 1,
    "period_display": "1st Quarter",
    "clock_display": "12:00",
    "play_text": "LeBron James makes 25-foot three point jumper (Anthony Davis assists)",
    "home_away": "home",
    "scoring_play": True,
    "away_score": 0,
    "home_score": 3
  },
  {
    "play_id": "4014682150002",
    "game_id": "401468215",
    "period_number": 1,
    "period_display": "1st Quarter",
    "clock_display": "11:42",
    "play_text": "Damian Lillard misses 18-foot pullup jump shot",
    "home_away": "away",
    "scoring_play": False,
    "away_score": 0,
    "home_score": 3
  }
]
```

**Output:**
```
üìä Extracted 147,891 plays from 1,189 JSON files (year 1997)
```

---

###### Step 4: Deduplication & Batch Insert to RDS

**Check:** Remove duplicate play_ids and batch insert all plays using `execute_values()`

**Deduplication logic:**
```python
# Deduplicate plays by play_id
seen_ids = set()
unique_plays = []
for play in plays_to_insert:
    if play['play_id'] not in seen_ids:
        seen_ids.add(play['play_id'])
        unique_plays.append(play)

if len(plays_to_insert) != len(unique_plays):
    print(f"‚ö†Ô∏è  Removed {len(plays_to_insert) - len(unique_plays)} duplicate play_ids")
    plays_to_insert = unique_plays
```

**Batch insert logic (idempotent):**
```python
insert_query = """
    INSERT INTO play_by_play (play_id, game_id, period_number, period_display,
                             clock_display, play_text, home_away, scoring_play,
                             away_score, home_score)
    VALUES %s
    ON CONFLICT (play_id) DO UPDATE SET
        period_number = EXCLUDED.period_number,
        period_display = EXCLUDED.period_display,
        clock_display = EXCLUDED.clock_display,
        play_text = EXCLUDED.play_text,
        home_away = EXCLUDED.home_away,
        scoring_play = EXCLUDED.scoring_play,
        away_score = EXCLUDED.away_score,
        home_score = EXCLUDED.home_score,
        updated_at = CURRENT_TIMESTAMP
"""

values = [
    (
        play['play_id'],
        play['game_id'],
        play['period_number'],
        play['period_display'],
        play['clock_display'],
        play['play_text'],
        play['home_away'],
        play['scoring_play'],
        play['away_score'],
        play['home_score']
    )
    for play in plays_to_insert
]

execute_values(cursor, insert_query, values)
conn.commit()

print(f"‚úÖ Inserted {len(plays_to_insert)} plays into database")
```

**Performance optimization:**
- **Batch insert:** Single query for all plays (vs. 147K individual INSERTs)
- **execute_values():** PostgreSQL-optimized batch insertion
- **Idempotent:** Re-running script updates existing plays (safe to retry)

**Output (success):**
```
‚ö†Ô∏è  Removed 234 duplicate play_ids
‚úÖ Inserted 147,657 plays into database
```

---

##### üéÆ How to Use This Script

**Prerequisites:**
1. ‚úÖ RDS PostgreSQL instance running
2. ‚úÖ `play_by_play` table created (run `scripts/db_schema.sql`)
3. ‚úÖ `games` table populated (run `extract_schedule_local.py` first)
4. ‚úÖ Credentials file sourced: `source /Users/ryanranft/nba-sim-credentials.env`
5. ‚úÖ AWS credentials configured (for S3 access)
6. ‚úÖ Python environment activated: `conda activate nba-aws`

---

###### Usage 1: Single Year Extraction

**Command:**
```bash
python scripts/etl/extract_pbp_local.py --year 1997
```

**Output:**
```
================================================================================
Processing Year: 1997
================================================================================

‚úÖ Connected to database: nba-simulator-db.xxxxxx.us-east-1.rds.amazonaws.com
Fetching game IDs from database...
Found 1,189 games for year 1997

  Progress: 100/1,189 games | 12,345 plays extracted
  Progress: 200/1,189 games | 24,678 plays extracted
  Progress: 300/1,189 games | 37,012 plays extracted
  Progress: 400/1,189 games | 49,234 plays extracted
  Progress: 500/1,189 games | 61,567 plays extracted
  Progress: 600/1,189 games | 73,890 plays extracted
  Progress: 700/1,189 games | 86,123 plays extracted
  Progress: 800/1,189 games | 98,456 plays extracted
  Progress: 900/1,189 games | 110,789 plays extracted
  Progress: 1,000/1,189 games | 123,012 plays extracted
  Progress: 1,100/1,189 games | 135,345 plays extracted

Extraction complete: 147,891 plays ready to insert

‚ö†Ô∏è  Removed 234 duplicate play_ids
‚úÖ Inserted 147,657 plays into database

================================================================================
EXTRACTION COMPLETE
================================================================================
Years processed: 1
Plays processed: 147,657
Plays inserted: 147,657
Games skipped: 45
Errors: 12
```

**Use case:** Test ETL logic, load single historical year, backfill missing year

---

###### Usage 2: Year Range Extraction

**Command:**
```bash
python scripts/etl/extract_pbp_local.py --year-range 1997-1999
```

**Output:**
```
================================================================================
Processing Year: 1997
================================================================================

  ... [same as single year] ...
  ‚úÖ Inserted 147,657 plays

================================================================================
Processing Year: 1998
================================================================================

  ... [processing 1998] ...
  ‚úÖ Inserted 156,234 plays

================================================================================
Processing Year: 1999
================================================================================

  ... [processing 1999] ...
  ‚úÖ Inserted 149,012 plays

================================================================================
EXTRACTION COMPLETE
================================================================================
Years processed: 3
Plays processed: 452,903
Plays inserted: 452,903
Games skipped: 127
Errors: 38
```

**Use case:** Load multiple consecutive years, backfill year ranges, incremental loading

---

###### Usage 3: Dry Run (Preview)

**Command:**
```bash
python scripts/etl/extract_pbp_local.py --year 1997 --dry-run
```

**Output:**
```
================================================================================
Processing Year: 1997
================================================================================

Found 1,189 games for year 1997

  Progress: 100/1,189 games | 12,345 plays extracted
  ... [processing] ...

Extraction complete: 147,891 plays ready to insert

‚ö†Ô∏è  Removed 234 duplicate play_ids
üîç DRY RUN: Would insert 147,657 plays

Sample play data:
{
  "play_id": "4014682150001",
  "game_id": "401468215",
  "period_number": 1,
  "period_display": "1st Quarter",
  "clock_display": "12:00",
  "play_text": "LeBron James makes 25-foot three point jumper (Anthony Davis assists)",
  "home_away": "home",
  "scoring_play": true,
  "away_score": 0,
  "home_score": 3
}
```

**Use case:** Preview before committing, test S3 access, validate JSON parsing, estimate runtime

---

##### üîó Integration with Other Workflows

**Workflow 1: After Schedule Data Loaded**
```bash
# Step 1: Load schedule data first (required for game_ids)
source /Users/ryanranft/nba-sim-credentials.env
python scripts/etl/extract_schedule_local.py --year-range 1997-2021

# Step 2: Verify games table
psql -c "SELECT COUNT(*) FROM games WHERE game_date >= '1997-01-01';"

# Step 3: Load play-by-play data
python scripts/etl/extract_pbp_local.py --year-range 1997-2021

# Step 4: Verify
psql -c "SELECT COUNT(*) FROM play_by_play;"
psql -c "SELECT game_id, COUNT(*) FROM play_by_play GROUP BY game_id ORDER BY COUNT(*) DESC LIMIT 10;"
```

**Workflow 2: Incremental Updates (New Season)**
```bash
# At start of new NBA season (October)
# Step 1: Load new season games
python scripts/etl/extract_schedule_local.py --year 2025

# Step 2: Load new season PBP
python scripts/etl/extract_pbp_local.py --year 2025
```

**Workflow 3: Backfill Missing Years**
```bash
# Check which years have PBP data
psql -c "SELECT EXTRACT(YEAR FROM g.game_date) AS year, COUNT(DISTINCT p.game_id) FROM games g LEFT JOIN play_by_play p ON g.game_id = p.game_id GROUP BY year ORDER BY year;"

# Backfill gap
python scripts/etl/extract_pbp_local.py --year 2010
```

**Workflow 4: Data Quality Analysis**
```bash
# Load PBP for a single year
python scripts/etl/extract_pbp_local.py --year 2021

# Analyze play distribution
psql -c "
  SELECT period_number, COUNT(*) AS plays, AVG(home_score + away_score) AS avg_score
  FROM play_by_play
  WHERE game_id IN (SELECT game_id FROM games WHERE game_date >= '2021-01-01')
  GROUP BY period_number
  ORDER BY period_number;
"

# Check scoring plays
psql -c "
  SELECT scoring_play, COUNT(*)
  FROM play_by_play
  WHERE game_id IN (SELECT game_id FROM games WHERE game_date >= '2021-01-01')
  GROUP BY scoring_play;
"
```

---

##### üìä Comparison: PBP vs. Schedule Extraction

| Aspect | extract_pbp_local.py | extract_schedule_local.py |
|--------|---------------------|--------------------------|
| **Data type** | Play-by-play events | Game metadata |
| **Fields extracted** | 10 per play | 53 per game |
| **Volume** | 100-300 plays per game | 1 game per file |
| **Total records** | ~3M plays (1997-2021) | ~40K games (1993-2025) |
| **S3 lookup** | By game_id from database | By year prefix |
| **Foreign key** | Requires games table | Independent |
| **Execution order** | AFTER schedule extraction | First |
| **Runtime** | ~5-15 min per year | ~2-5 min per year |
| **Use case** | Detailed analysis, simulation | Game results, schedules |

**Dependency chain:**
```
extract_schedule_local.py ‚Üí populate games table ‚Üí extract_pbp_local.py
```

---

##### ‚úÖ Best Practices

**1. Always load schedule data first:**
```bash
# REQUIRED: Load games table before PBP
python scripts/etl/extract_schedule_local.py --year-range 1997-2021
python scripts/etl/extract_pbp_local.py --year-range 1997-2021
```

**2. Verify foreign key relationships:**
```bash
# Check for orphaned plays (should be 0)
psql -c "SELECT COUNT(*) FROM play_by_play p WHERE NOT EXISTS (SELECT 1 FROM games g WHERE g.game_id = p.game_id);"
```

**3. Monitor progress for large year ranges:**
```bash
# In separate terminal, watch database grow
watch -n 10 'psql -c "SELECT COUNT(*) FROM play_by_play;"'
```

**4. Verify data quality after load:**
```bash
# Check for missing game_ids
psql -c "SELECT COUNT(*) FROM games g WHERE NOT EXISTS (SELECT 1 FROM play_by_play p WHERE p.game_id = g.game_id);"

# Check for duplicate plays
psql -c "SELECT play_id, COUNT(*) FROM play_by_play GROUP BY play_id HAVING COUNT(*) > 1;"

# Check score progression
psql -c "SELECT game_id, period_number, MAX(home_score + away_score) FROM play_by_play GROUP BY game_id, period_number ORDER BY game_id, period_number LIMIT 50;"
```

**5. Use year ranges for efficiency:**
```bash
# Process multiple years in one run
python scripts/etl/extract_pbp_local.py --year-range 1997-2005
```

**6. Log output for troubleshooting:**
```bash
python scripts/etl/extract_pbp_local.py --year 2021 2>&1 | tee extract_pbp_2021.log
```

---

##### ‚ö†Ô∏è Troubleshooting

**Problem 1: No games found for year**

**Symptoms:**
```
‚ö†Ô∏è  No games found for year 1997
```

**Solution:**
```bash
# Load schedule data first
python scripts/etl/extract_schedule_local.py --year 1997

# Verify
psql -c "SELECT COUNT(*) FROM games WHERE game_date >= '1997-01-01' AND game_date < '1998-01-01';"
```

**Root cause:** Games table not populated for this year

---

**Problem 2: Foreign key constraint violation**

**Symptoms:**
```
psycopg2.errors.ForeignKeyViolation: insert or update on table "play_by_play" violates foreign key constraint "play_by_play_game_id_fkey"
```

**Solution:**
```bash
# Verify games exist
psql -c "SELECT game_id FROM games LIMIT 10;"

# If missing, load schedule data
python scripts/etl/extract_schedule_local.py --year 1997

# Or temporarily drop constraint
psql -c "ALTER TABLE play_by_play DROP CONSTRAINT play_by_play_game_id_fkey;"
python scripts/etl/extract_pbp_local.py --year 1997
psql -c "ALTER TABLE play_by_play ADD CONSTRAINT play_by_play_game_id_fkey FOREIGN KEY (game_id) REFERENCES games(game_id);"
```

**Root cause:** Games table not populated before PBP extraction

---

**Problem 3: High skip rate (>20% games skipped)**

**Symptoms:**
```
Games skipped: 257
Errors: 12
```

**Solution:**
```bash
# Check S3 for missing files
aws s3 ls s3://nba-sim-raw-data-lake/pbp/ | wc -l

# Verify games exist in S3
psql -c "SELECT game_id FROM games WHERE game_date >= '1997-01-01' LIMIT 10;"
aws s3 ls s3://nba-sim-raw-data-lake/pbp/ | grep <game_id>

# Run comprehensive_data_analysis.py to check PBP availability
python scripts/analysis/comprehensive_data_analysis.py
```

**Root cause:** PBP files missing from S3 for some games (expected for older years)

---

**Problem 4: Slow performance (>30 minutes per year)**

**Symptoms:**
```
Progress: 100/1,189 games | 12,345 plays extracted
... [30 minutes later] ...
Progress: 200/1,189 games | 24,678 plays extracted
```

**Solution:**
```bash
# 1. Check network speed
aws s3 cp s3://nba-sim-raw-data-lake/pbp/401468215.json /tmp/ --debug

# 2. Use Glue for large ranges
python scripts/etl/create_glue_etl_job.py --data-type pbp

# 3. Process smaller year ranges
python scripts/etl/extract_pbp_local.py --year-range 1997-2000  # Instead of 1997-2021
```

**Root cause:** Network latency, large file sizes (PBP files 100-500KB each)

---

**Problem 5: Out of memory error**

**Symptoms:**
```
MemoryError: Unable to allocate array
```

**Solution:**
```bash
# Process smaller year ranges (script loads all plays into memory)
python scripts/etl/extract_pbp_local.py --year 1997  # Instead of --year-range 1997-2021

# Or modify script to batch insert every 10,000 plays instead of all at once
# (requires code change)
```

**Root cause:** Loading 3M+ plays into memory for batch insert

---

##### üîß Technical Details

**Database schema (play_by_play table):**
```sql
CREATE TABLE play_by_play (
    play_id VARCHAR(20) PRIMARY KEY,
    game_id VARCHAR(20) REFERENCES games(game_id),
    period_number INTEGER,
    period_display VARCHAR(50),
    clock_display VARCHAR(10),
    play_text TEXT,
    home_away VARCHAR(10),
    scoring_play BOOLEAN,
    away_score INTEGER,
    home_score INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_pbp_game_id ON play_by_play(game_id);
CREATE INDEX idx_pbp_period ON play_by_play(period_number);
CREATE INDEX idx_pbp_scoring ON play_by_play(scoring_play);
```

**Idempotency guarantee:**
- `ON CONFLICT (play_id) DO UPDATE` ensures script can be re-run safely
- Updates existing plays with latest data
- Created_at preserved, updated_at refreshed

**Performance characteristics:**
- **S3 download:** ~0.3s per file (network-bound)
- **JSON parsing:** ~0.02s per file (CPU-bound)
- **Database insert:** ~0.5s per batch of 10,000 plays (I/O-bound)
- **Total:** ~5-15 minutes per year (1,200 games √ó 0.3s ‚âà 6 minutes)

**Memory usage:**
- **Peak:** ~500 MB (loads all year's plays into memory before insert)
- **Steady:** ~50 MB (Python interpreter + libraries)

**Error handling:**
- ‚úÖ Missing environment variables ‚Üí Exit with helpful message
- ‚úÖ S3 access errors ‚Üí Skip file, continue (NoSuchKey expected for some games)
- ‚úÖ Malformed JSON ‚Üí Log warning, continue
- ‚úÖ Database errors ‚Üí Rollback transaction, exit with error
- ‚úÖ Duplicate play_ids ‚Üí Deduplicate before insert

---

##### üöÄ Future Enhancements

**1. Batch insert during extraction (reduce memory):**
```python
# Insert every 10,000 plays instead of loading all into memory
BATCH_SIZE = 10000
if len(plays_to_insert) >= BATCH_SIZE:
    execute_values(cursor, insert_query, plays_to_insert[:BATCH_SIZE])
    plays_to_insert = plays_to_insert[BATCH_SIZE:]
```

**2. Parallel S3 downloads:**
```python
# Use concurrent.futures for faster downloads
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(download_and_parse_pbp, game_id) for game_id in game_ids]
    all_plays = [play for f in futures for play in f.result()]
```

**3. Enhanced play parsing (extract player names):**
```python
# Parse play_text to extract player names, action types
def parse_play_text(text):
    # "LeBron James makes 25-foot three point jumper (Anthony Davis assists)"
    # ‚Üí player: "LeBron James", action: "makes 3pt", assist: "Anthony Davis"
    pass
```

**4. Progress persistence:**
```python
# Save progress to file, resume on failure
with open('extract_pbp_progress.json', 'w') as f:
    json.dump({'year': year, 'games_processed': processed_count}, f)
```

**5. Data quality metrics:**
```python
# Calculate play distribution stats
scoring_plays = sum(1 for p in plays if p['scoring_play'])
print(f"Scoring plays: {scoring_plays}/{len(plays)} ({scoring_plays/len(plays)*100:.1f}%)")
```

---

#### Overview: Year-Based Approach

**Why year-based partitioning?**
- **Problem:** Single Glue Crawler processing 146K files = timeouts, memory issues, slow performance
- **Solution:** Partition S3 data by year (33 folders: year=1993 to year=2025)
- **Benefit:** Each crawler processes ~4,400 files instead of 146K (manageable chunks)

**Pipeline stages:**
1. **Partition S3 data** by year (one-time operation)
2. **Create year-based crawlers** (33 crawlers per data type)
3. **Run crawlers** to populate Glue Data Catalog
4. **Run ETL jobs** year by year to load RDS

**Data types:** 4 types √ó 33 years = 132 partitions
- `schedule/` - Game schedules (1993-2025)
- `pbp/` - Play-by-play data (1997-2021)
- `box_scores/` - Box score statistics (1997-2021)
- `team_stats/` - Team statistics (1997-2021)

---

#### Workflow #1: Check Partition Status

**Script:** `scripts/shell/check_partition_status.sh`

**Purpose:** Monitor S3 partitioning progress and verify completion

**When to run:**
- During overnight partitioning to check progress
- After `partition_all_overnight.sh` completes
- To verify readiness before creating crawlers

**Usage:**
```bash
bash scripts/shell/check_partition_status.sh
```

**What this checks (4 sections):**

##### Section 1: Process Status
```bash
# Checks if partitioning processes still running
ps aux | grep "[p]artition_by_year.py"
```

**Possible outputs:**
```
‚è≥ Partitioning is STILL RUNNING

Active processes:
ryan   12345  partition_by_year.py --data-types schedule

To monitor progress:
  tail -f partition_overnight.log
```

OR

```
‚úÖ Partitioning processes have COMPLETED
```

##### Section 2: Year Folders Created
```bash
# Counts year= folders in S3 for each data type
for data_type in schedule pbp box_scores team_stats; do
  aws s3 ls s3://nba-sim-raw-data-lake/${data_type}/ | grep "year="
done
```

**Sample output:**
```
Year Folders Created in S3
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
‚úÖ schedule: 33 year folders (COMPLETE)
‚úÖ pbp: 25 year folders (COMPLETE)
‚è≥ box_scores: 18 year folders (IN PROGRESS)
‚è∏Ô∏è  team_stats: 0 year folders (NOT STARTED)
```

**Interpretation:**
- **33 folders** = COMPLETE (schedule: 1993-2025)
- **25 folders** = COMPLETE (pbp/box/team: 1997-2021)
- **0-32 folders** = IN PROGRESS
- **0 folders** = NOT STARTED

##### Section 3: Log File Status
```bash
# Checks partition_overnight.log for completion markers
grep "ALL PARTITIONING COMPLETE" partition_overnight.log
```

**Possible outputs:**
```
‚úÖ Log shows: ALL PARTITIONING COMPLETE
```

OR

```
‚ùå Log shows: Some partitioning FAILED

Failed sections:
  FAILED: box_scores partitioning (error: S3 access denied)
```

OR

```
‚è≥ Partitioning in progress

Last 5 lines:
  Copied 450 files to schedule/year=2015/
  Copied 500 files to schedule/year=2015/
  ...
```

##### Section 4: Summary & Next Steps
```bash
# Calculates total year folders: 132 expected (33√ó4)
total=$((schedule_count + pbp_count + box_count + team_count))
```

**100% Complete (132/132):**
```
Total year folders created: 132/132

üéâ PARTITIONING 100% COMPLETE!

Next steps:
  1. Create year-based crawlers:
     ./scripts/etl/create_year_crawlers.sh --all

  2. Test one crawler:
     aws glue start-crawler --name nba-schedule-1997-crawler
```

**Partial Progress (e.g., 87/132):**
```
Total year folders created: 87/132

‚è≥ PARTITIONING 66% COMPLETE

Monitor progress:
  tail -f partition_overnight.log
```

**Not Started (0/132):**
```
Total year folders created: 0/132

‚ö†Ô∏è  PARTITIONING NOT STARTED

Start partitioning:
  nohup ./scripts/etl/partition_all_overnight.sh > partition_overnight.log 2>&1 &
```

**Integration:** Run this script frequently during overnight partitioning to monitor progress

---

#### Workflow #2: Partition All Data (Overnight)

**Script:** `scripts/etl/partition_all_overnight.sh`

**Purpose:** Automated overnight S3 partitioning for all 4 data types (schedule, pbp, box_scores, team_stats)

**When to run:** ONE-TIME operation before creating crawlers

**Critical:** This is a LONG-RUNNING operation (6-12 hours). Run overnight in background.

**Usage:**
```bash
# Start in background with logging
nohup bash scripts/etl/partition_all_overnight.sh > partition_overnight.log 2>&1 &

# Monitor progress (separate terminal)
tail -f partition_overnight.log

# Check process status
ps aux | grep partition

# Check partition status
bash scripts/shell/check_partition_status.sh
```

**What this does (4 steps):**

**Step 1: Partition Schedule Data**
```bash
# Calls partition_by_year.py for schedule data
echo "yes" | python scripts/etl/partition_by_year.py \
    --data-types "schedule" \
    --execute
```

**Partitions:** `schedule/*.json` ‚Üí `schedule/year=YYYY/*.json` (33 years: 1993-2025)

**Step 2: Partition Play-by-Play Data**
```bash
echo "yes" | python scripts/etl/partition_by_year.py \
    --data-types "pbp" \
    --execute
```

**Partitions:** `pbp/*.json` ‚Üí `pbp/year=YYYY/*.json` (25 years: 1997-2021)

**Step 3: Partition Box Scores**
```bash
echo "yes" | python scripts/etl/partition_by_year.py \
    --data-types "box_scores" \
    --execute
```

**Partitions:** `box_scores/*.json` ‚Üí `box_scores/year=YYYY/*.json` (25 years)

**Step 4: Partition Team Stats**
```bash
echo "yes" | python scripts/etl/partition_by_year.py \
    --data-types "team_stats" \
    --execute
```

**Partitions:** `team_stats/*.json` ‚Üí `team_stats/year=YYYY/*.json` (25 years)

**Script output:**
```
================================================================================
AUTOMATED S3 PARTITIONING - OVERNIGHT RUN
================================================================================
Started: 2025-10-02 23:00:00
Log file: /Users/ryanranft/nba-simulator-aws/partition_overnight.log

================================================================================
Partitioning: schedule
Started: 2025-10-02 23:00:05
================================================================================
Processing 33 years (1993-2025)...
  Year 1993: Copied 1234 files to schedule/year=1993/
  Year 1994: Copied 1189 files to schedule/year=1994/
  ...
  Year 2025: Copied 892 files to schedule/year=2025/

‚úÖ SUCCESS: schedule partitioning completed
Completed: 2025-10-02 23:45:32

================================================================================
Partitioning: pbp
Started: 2025-10-02 23:45:35
================================================================================
[... continues for pbp, box_scores, team_stats ...]

================================================================================
ALL PARTITIONING COMPLETE
================================================================================
Finished: 2025-10-03 05:23:17

Verifying S3 structure...

Schedule years: 33
PBP years: 25
Box scores years: 25
Team stats years: 25

‚úÖ Partitioning complete! Ready to create year-based crawlers.

Next steps:
1. Create year-based crawlers:
   ./scripts/etl/create_year_crawlers.sh --all

2. Run crawlers for a test year:
   aws glue start-crawler --name nba-schedule-1997-crawler
```

**Expected timeline:**
- **Schedule:** ~2-3 hours (most years, 1993-2025)
- **PBP:** ~1.5-2 hours
- **Box Scores:** ~1.5-2 hours
- **Team Stats:** ~1-1.5 hours
- **Total:** 6-9 hours (depends on S3 performance)

**Monitoring:**
```bash
# Check progress
tail -f partition_overnight.log

# Count partitioned files
aws s3 ls s3://nba-sim-raw-data-lake/schedule/ | grep "year=" | wc -l

# Check specific year
aws s3 ls s3://nba-sim-raw-data-lake/schedule/year=2015/ --recursive | wc -l
```

**Troubleshooting:**

**Process killed/stopped:**
```bash
# Check log for errors
grep -i "error\|failed" partition_overnight.log

# Resume from specific data type (if schedule complete, start with pbp)
echo "yes" | python scripts/etl/partition_by_year.py --data-types "pbp" --execute
```

**S3 rate limiting:**
- Script includes automatic retry logic
- Wait 5 minutes, script will resume
- Check CloudWatch S3 metrics for throttling

**Verification after completion:**
```bash
bash scripts/shell/check_partition_status.sh
# Should show: üéâ PARTITIONING 100% COMPLETE!
```

**Cost:** ~$0 (S3 copy operations are free for same-region, same-bucket)

---

#### Workflow #3: Create Year-Based Crawlers

**Script:** `scripts/etl/create_year_crawlers.sh`

**Purpose:** Create Glue Crawlers for each year partition (avoid 146K-file overload)

**Prerequisites:** Partitioning complete (132/132 folders created)

**Usage options:**

**Option 1: Create all crawlers (recommended)**
```bash
bash scripts/etl/create_year_crawlers.sh --all
```

Creates 100+ crawlers:
- 33 crawlers for schedule (1993-2025)
- 25 crawlers for pbp (1997-2021)
- 25 crawlers for box_scores (1997-2021)
- 25 crawlers for team_stats (1997-2021)

**Option 2: Create crawlers for specific data type and years**
```bash
# Schedule data: 1997-2021
bash scripts/etl/create_year_crawlers.sh --data-type schedule --years 1997-2021

# PBP data: 2015-2021 only
bash scripts/etl/create_year_crawlers.sh --data-type pbp --years 2015-2021
```

**Option 3: Dry-run (preview without creating)**
```bash
bash scripts/etl/create_year_crawlers.sh --all --dry-run
```

**What this does:**

**For each year:**
1. Checks if crawler already exists (skips if exists)
2. Creates crawler with naming convention: `nba-{data_type}-{year}-crawler`
3. Configures S3 target: `s3://nba-sim-raw-data-lake/{data_type}/year={year}/`
4. Links to database: `nba_raw_data`
5. Uses IAM role: `AWSGlueServiceRole-NBASimulator`

**Sample output:**
```
================================================================
Year-Based Glue Crawler Creation
================================================================
Bucket: nba-sim-raw-data-lake
Database: nba_raw_data
Role: AWSGlueServiceRole-NBASimulator
Years: 1997-2021
Mode: EXECUTE (crawlers will be created)

================================================================
Creating crawlers for: SCHEDULE
Years: 1997 to 2021
================================================================

‚úì Creating crawler: nba-schedule-1997-crawler
    S3 Path: s3://nba-sim-raw-data-lake/schedule/year=1997/
‚úì Creating crawler: nba-schedule-1998-crawler
    S3 Path: s3://nba-sim-raw-data-lake/schedule/year=1998/
[... continues for all years ...]

‚úì Created 25 crawlers for schedule

================================================================
Creating crawlers for: PBP
Years: 1997 to 2021
================================================================
[... continues for pbp, box_scores, team_stats ...]

================================================================
SUMMARY
================================================================
CRAWLERS CREATED SUCCESSFULLY

Next steps:
1. List all crawlers:
   aws glue list-crawlers --query 'CrawlerNames' --output table

2. Start a specific crawler:
   aws glue start-crawler --name nba-schedule-1997-crawler

3. Start all crawlers for a year (e.g., 1997):
   for crawler in $(aws glue list-crawlers --query 'CrawlerNames[?contains(@, `1997`)]' --output text); do
     aws glue start-crawler --name $crawler
   done

4. Monitor crawler status:
   aws glue get-crawler --name nba-schedule-1997-crawler
```

**Verify crawlers created:**
```bash
# Count total crawlers
aws glue list-crawlers --query 'CrawlerNames' | grep "nba-" | wc -l
# Should show: 108 (33+25+25+25)

# List schedule crawlers
aws glue list-crawlers --query 'CrawlerNames[?starts_with(@, `nba-schedule`)]' --output table

# Get specific crawler details
aws glue get-crawler --name nba-schedule-1997-crawler
```

**Cost:** ~$0 (creating crawlers is free, running them costs ~$0.44/DPU-hour)

---

#### Workflow #4: Run Crawlers (Overnight)

**Script:** `scripts/etl/run_crawlers_overnight.sh`

**Purpose:** Automatically run all year-based crawlers with AWS service limit management (max 10 concurrent)

**Prerequisites:**
- Partitioning complete (132/132)
- Crawlers created (~108 crawlers)

**Usage:**
```bash
# Start in background
nohup bash scripts/etl/run_crawlers_overnight.sh > crawler_overnight.log 2>&1 &

# Monitor progress
tail -f crawler_overnight.log

# Check running crawlers
aws glue list-crawlers --query 'CrawlerNames' | xargs -I {} aws glue get-crawler --name {} --query 'Crawler.{Name:Name,State:State}' --output table
```

**What this does (intelligent crawler orchestration):**

**Strategy:**
1. Wait for each year partition to be ready (files exist in S3)
2. Create crawler if doesn't exist (auto-creates missing crawlers)
3. Start crawler (respects AWS 10-concurrent-crawler limit)
4. Process all years (1993-2025) for all data types

**Concurrency management:**
```bash
# AWS Glue limit: 10 concurrent crawlers per account
# Script automatically waits if 10 running:
wait_for_crawlers() {
  while [ $(running_count) -ge 10 ]; do
    echo "‚è≥ 10 crawlers running, waiting 60 seconds..."
    sleep 60
  done
}
```

**Sample output:**
```
================================================================================
AUTOMATED GLUE CRAWLER EXECUTION - OVERNIGHT RUN
================================================================================
Started: 2025-10-03 18:00:00
Log file: /Users/ryanranft/nba-simulator-aws/crawler_overnight.log

Strategy:
  1. Wait for each year partition to be ready in S3
  2. Create crawler if it doesn't exist
  3. Start crawler (max 10 concurrent)
  4. Repeat for all years (1993-2025) and all data types

Expected timeline:
  - Schedule: 33 years √ó ~5 min = ~2.5 hours
  - PBP: 25 years √ó ~5 min = ~2 hours
  - Box Scores: 25 years √ó ~5 min = ~2 hours
  - Team Stats: 25 years √ó ~5 min = ~2 hours
  Total: ~8-10 hours (with parallelization: ~3-4 hours)

================================================================================
Processing: SCHEDULE
================================================================================

Year 1997:
  ‚úì Crawler exists: nba-schedule-1997-crawler
  ‚ñ∂Ô∏è  Starting: nba-schedule-1997-crawler

Year 1998:
  ‚úì Crawler exists: nba-schedule-1998-crawler
  ‚ñ∂Ô∏è  Starting: nba-schedule-1998-crawler
[... starts 10 crawlers ...]

  ‚è≥ 10 crawlers running, waiting 60 seconds...
[... waits for slot to open ...]

  ‚ñ∂Ô∏è  Starting: nba-schedule-2007-crawler

Summary for schedule:
  Started: 33 crawlers
  Skipped: 0 crawlers

================================================================================
Processing: PBP
================================================================================
[... continues for pbp, box_scores, team_stats ...]

================================================================================
ALL CRAWLERS STARTED
================================================================================
Finished: 2025-10-03 22:15:42

Waiting for all crawlers to complete...

‚è≥ 7 crawlers still running... (checking again in 2 minutes)
‚è≥ 3 crawlers still running... (checking again in 2 minutes)
‚úÖ All crawlers completed!

================================================================================
FINAL SUMMARY
================================================================================

Glue Tables Created:
  schedule: 33 tables
  pbp: 25 tables
  box_scores: 25 tables
  team_stats: 25 tables

‚úÖ Crawler execution complete! Ready for Phase 2.2 (Glue ETL).
```

**Expected timeline:**
- **With 10-crawler concurrency:** ~3-4 hours
- **Sequential (no concurrency):** ~8-10 hours
- **Per crawler:** ~3-5 minutes (depends on file count)

**Monitoring commands:**
```bash
# Count running crawlers
aws glue list-crawlers --query 'CrawlerNames' | xargs -I {} aws glue get-crawler --name {} --query 'Crawler.State' --output text | grep -c "RUNNING"

# List crawler states
aws glue list-crawlers --query 'CrawlerNames' | xargs -I {} aws glue get-crawler --name {} --query 'Crawler.{Name:Name,State:State}'

# Check specific crawler
aws glue get-crawler --name nba-schedule-1997-crawler --query 'Crawler.{State:State,LastCrawl:LastCrawl}'
```

**Verify Glue tables created:**
```bash
# Count tables
aws glue get-tables --database-name nba_raw_data --query 'TableList[*].Name' | wc -l
# Should show: ~108 tables

# List schedule tables
aws glue get-tables --database-name nba_raw_data --query "TableList[?starts_with(Name, 'schedule')].Name"
```

**Cost:** ~$1-2 total (108 crawlers √ó ~5 min √ó $0.44/DPU-hour)

---

#### Workflow #5: Process Year-by-Year (Alternative Sequential Approach)

**Script:** `scripts/etl/process_year_by_year.sh`

**Purpose:** Alternative approach - partition AND crawl one year at a time (more sequential)

**When to use:**
- More predictable resource usage
- Easier to debug year-specific issues
- Prefer sequential over parallel processing
- Want tighter control over execution

**Difference from Workflow #2 + #4:**
- **Workflows #2 & #4:** Partition ALL years first, then crawl ALL years (parallel)
- **Workflow #5:** For each year: partition THEN crawl (sequential)

**Usage:**
```bash
nohup bash scripts/etl/process_year_by_year.sh > year_by_year.log 2>&1 &
```

**What this does:**

**For each year (1993-2025):**
1. **Partition** all 4 data types for this year
2. **Crawl** all 4 data types for this year
3. Move to next year

**Sample output:**
```
================================================================================
YEAR-BY-YEAR S3 PARTITIONING AND CRAWLER EXECUTION
================================================================================
Started: 2025-10-03 18:00:00

Strategy: Process one year at a time
  1. Partition all 4 data types for year N
  2. Crawl all 4 data types for year N
  3. Move to year N+1

Data types: schedule, pbp, box_scores, team_stats
Years: 1993-2025

================================================================================
YEAR 1997
================================================================================

--- Step 1: Partitioning data for year 1997 ---
  Partitioning schedule for year 1997...
    Copied 1234 files...
  ‚úÖ Copied 1234 files for schedule/year=1997

  Partitioning pbp for year 1997...
    Copied 892 files...
  ‚úÖ Copied 892 files for pbp/year=1997

  Partitioning box_scores for year 1997...
    Copied 1456 files...
  ‚úÖ Copied 1456 files for box_scores/year=1997

  Partitioning team_stats for year 1997...
    Copied 234 files...
  ‚úÖ Copied 234 files for team_stats/year=1997

--- Step 2: Crawling data for year 1997 ---
  Creating crawler: nba-schedule-1997-crawler
  Starting crawler: nba-schedule-1997-crawler
  Waiting for crawler to complete...
  ‚úÖ Crawler completed: nba-schedule-1997-crawler

  Creating crawler: nba-pbp-1997-crawler
  Starting crawler: nba-pbp-1997-crawler
  ‚úÖ Crawler completed: nba-pbp-1997-crawler

  Creating crawler: nba-box_scores-1997-crawler
  ‚úÖ Crawler completed: nba-box_scores-1997-crawler

  Creating crawler: nba-team_stats-1997-crawler
  ‚úÖ Crawler completed: nba-team_stats-1997-crawler

‚úÖ Year 1997 complete!

================================================================================
YEAR 1998
================================================================================
[... continues for 1998-2025 ...]

================================================================================
ALL YEARS PROCESSED
================================================================================
Finished: 2025-10-04 06:23:45

Summary:
  Year folders created:
    schedule: 33
    pbp: 25
    box_scores: 25
    team_stats: 25

  Glue tables created:
    schedule: 33
    pbp: 25
    box_scores: 25
    team_stats: 25

‚úÖ Complete! Ready for Phase 2.2 (Glue ETL).
```

**Expected timeline:** ~12-16 hours (fully sequential)

**Trade-offs:**

| Aspect | Year-by-Year (Workflow #5) | Partition All + Crawl All (Workflows #2 & #4) |
|--------|---------------------------|----------------------------------------------|
| **Speed** | Slower (~12-16 hours) | Faster (~6-9 hours) |
| **Concurrency** | Sequential (1 year at a time) | Parallel (10 crawlers at once) |
| **Debugging** | Easy (isolated per year) | Harder (multiple years in flight) |
| **Resource usage** | Predictable | Variable |
| **Best for** | Cautious first run, debugging | Production, speed-focused |

**When to use Workflow #5:**
- First time running pipeline (want to verify each year)
- Debugging year-specific data issues
- Limited resources/prefer sequential
- Want to process specific year range only

---

#### Workflow #6: Run ETL Jobs (All Years)

**Script:** `scripts/etl/run_etl_all_years.sh`

**Purpose:** Execute Glue ETL jobs year-by-year to load data from S3 (via Glue Catalog) into RDS PostgreSQL

**Prerequisites:**
- Partitioning complete (132/132)
- Crawlers run successfully (~108 tables in Glue Catalog)
- RDS instance created and accessible
- Glue ETL job created: `nba-schedule-etl-job`

**Usage:**
```bash
# Ensure DB password is set
source /Users/ryanranft/nba-sim-credentials.env

# Start ETL processing
nohup bash scripts/etl/run_etl_all_years.sh > etl_all_years.log 2>&1 &

# Monitor progress
tail -f etl_all_years.log
```

**What this does:**

**For each year (1993-2025):**
1. Start Glue ETL job with `--year` parameter
2. Job reads from Glue Catalog table (e.g., `schedule_year_1997`)
3. Transform and validate data
4. Load to RDS `games` table
5. Wait for job completion (max 20 minutes per year)
6. Report success/failure
7. Continue to next year

**Sample output:**
```
================================================================================
GLUE ETL JOB EXECUTION - ALL YEARS
================================================================================
Started: 2025-10-04 08:00:00

Job: nba-schedule-etl-job
Years: 1993-2025

================================================================================
PROCESSING YEAR 1997
================================================================================

--- Year 1997 ---
  Starting job run...
  ‚ñ∂Ô∏è  Job started: jr_a1b2c3d4e5f6...

  ‚è≥ Job RUNNING (10s elapsed)...
  ‚è≥ Job RUNNING (20s elapsed)...
  ‚è≥ Job RUNNING (30s elapsed)...
  ‚úÖ Job completed successfully!
     Execution time: 45s

‚úÖ Year 1997 complete!

================================================================================
PROCESSING YEAR 1998
================================================================================
[... continues for 1998-2025 ...]

================================================================================
PROCESSING YEAR 2015
================================================================================

--- Year 2015 ---
  Starting job run...
  ‚ñ∂Ô∏è  Job started: jr_x9y8z7...

  ‚è≥ Job RUNNING (10s elapsed)...
  ‚ùå Job failed!
     Status: FAILED
     Error: Table schedule_year_2015 not found in Glue Catalog

‚ùå Year 2015 failed!
   Continuing to next year...

[... continues despite failures ...]

================================================================================
ETL PROCESSING COMPLETE
================================================================================
Finished: 2025-10-04 09:15:32

Summary:
  ‚úÖ Successful: 32 years
  ‚ùå Failed: 1 years
  üìä Total: 33 years

Checking database for loaded games...

Database Statistics:
 season_year | game_count | first_game | last_game
-------------+------------+------------+-----------
        1997 |       1189 | 1997-10-31 | 1998-06-14
        1998 |       1145 | 1998-10-27 | 1999-06-25
        ...  |        ... | ...        | ...
        2021 |       1080 | 2020-12-22 | 2021-07-20

 total_games | seasons | earliest_game | latest_game
-------------+---------+---------------+-------------
       38234 |      32 |    1997-10-31 | 2021-07-20

‚úÖ All years processed!
```

**Expected timeline:**
- **Per year:** ~30-60 seconds (depends on data volume)
- **Total (33 years):** ~30-45 minutes

**Job monitoring:**
```bash
# List recent job runs
aws glue get-job-runs --job-name nba-schedule-etl-job --max-results 10

# Check specific job run
aws glue get-job-run --job-name nba-schedule-etl-job --run-id jr_xxxxx

# View CloudWatch logs
aws logs tail /aws-glue/jobs/output --follow
```

**Verify data loaded to RDS:**
```bash
# Connect to RDS
psql -h nba-sim-db.xxx.rds.amazonaws.com -U postgres -d nba_simulator

# Check row counts
SELECT season_year, COUNT(*) as game_count
FROM games
GROUP BY season_year
ORDER BY season_year;

# Verify date ranges
SELECT MIN(game_date), MAX(game_date) FROM games;

# Check for missing years
SELECT generate_series(1997, 2021) AS year
EXCEPT
SELECT DISTINCT season_year FROM games
ORDER BY year;
```

**Troubleshooting:**

**Job fails with "Table not found":**
- Crawler didn't run for that year
- Check: `aws glue get-tables --database-name nba_raw_data --query "TableList[?contains(Name, '1997')]"`
- Fix: Re-run crawler for that year

**Job fails with "Access denied to S3":**
- Glue IAM role missing S3 permissions
- Fix: Add S3ReadOnlyAccess to `AWSGlueServiceRole-NBASimulator`

**Job timeout (>20 minutes):**
- Too much data for allocated DPUs
- Fix: Increase DPUs in job definition (2 ‚Üí 5 DPU)

**Cost:** ~$0.30-0.50 per year √ó 33 years = ~$10-15 total (2 DPU √ó ~45 sec/year √ó $0.44/DPU-hour)

---

### Phase 2.2 Complete Workflow Summary

**Sequential execution order:**

1. **Check Partition Status** (pre-flight check)
   ```bash
   bash scripts/shell/check_partition_status.sh
   ```

2. **Partition All Data** (one-time, 6-12 hours)
   ```bash
   nohup bash scripts/etl/partition_all_overnight.sh > partition_overnight.log 2>&1 &
   ```

3. **Create Year Crawlers** (one-time, <5 minutes)
   ```bash
   bash scripts/etl/create_year_crawlers.sh --all
   ```

4. **Run Crawlers** (one-time, 3-4 hours)
   ```bash
   nohup bash scripts/etl/run_crawlers_overnight.sh > crawler_overnight.log 2>&1 &
   ```

5. **Run ETL Jobs** (one-time, 30-45 minutes)
   ```bash
   nohup bash scripts/etl/run_etl_all_years.sh > etl_all_years.log 2>&1 &
   ```

6. **Verify Complete Pipeline**
   ```bash
   # S3 partitions
   bash scripts/shell/check_partition_status.sh
   # Should show: 132/132

   # Glue tables
   aws glue get-tables --database-name nba_raw_data --query 'TableList[*].Name' | wc -l
   # Should show: ~108

   # RDS games
   psql -h <endpoint> -U postgres -d nba_simulator -c "SELECT COUNT(*) FROM games;"
   # Should show: ~38,000 games
   ```

**Total pipeline time:** ~10-16 hours (overnight recommended)

**Total cost:** ~$15-20 (one-time)

**After completion:** Ready for Phase 3 (simulation and analysis)

### Phase 3: RDS PostgreSQL Setup

**See PROGRESS.md Phase 3.1 (lines 606-743)**

1. **Create DB subnet group** (if not exists):
   ```bash
   aws rds create-db-subnet-group \
     --db-subnet-group-name nba-sim-subnet-group \
     --db-subnet-group-description "Subnet group for NBA simulator RDS" \
     --subnet-ids subnet-xxxxx subnet-yyyyy
   ```

2. **Create security group**:
   ```bash
   aws ec2 create-security-group \
     --group-name nba-sim-rds-sg \
     --description "Security group for NBA RDS" \
     --vpc-id vpc-xxxxx

   # Allow PostgreSQL access (port 5432)
   aws ec2 authorize-security-group-ingress \
     --group-id sg-xxxxx \
     --protocol tcp \
     --port 5432 \
     --cidr 0.0.0.0/0  # CAUTION: Restrict in production
   ```

3. **Create RDS instance**:
   ```bash
   aws rds create-db-instance \
     --db-instance-identifier nba-sim-db \
     --db-instance-class db.t3.micro \
     --engine postgres \
     --engine-version 15.3 \
     --master-username admin \
     --master-user-password <STRONG_PASSWORD> \
     --allocated-storage 20 \
     --storage-type gp2 \
     --vpc-security-group-ids sg-xxxxx \
     --db-subnet-group-name nba-sim-subnet-group \
     --publicly-accessible \
     --backup-retention-period 7 \
     --preferred-backup-window "03:00-04:00" \
     --preferred-maintenance-window "mon:04:00-mon:05:00"
   ```

4. **Wait for availability** (10-15 min):
   ```bash
   aws rds wait db-instance-available --db-instance-identifier nba-sim-db
   ```

5. **Get endpoint**:
   ```bash
   aws rds describe-db-instances \
     --db-instance-identifier nba-sim-db \
     --query 'DBInstances[0].Endpoint.Address' \
     --output text
   ```

6. **Test connection**:
   ```bash
   psql -h <endpoint> -U admin -d postgres -c "SELECT version();"
   ```

7. **Create database**:
   ```sql
   CREATE DATABASE nba_sim;
   ```

8. **Document endpoint** in PROGRESS.md and .env file

**Cost:** ~$29/month (db.t3.micro, 20GB storage)
**Rollback:** `aws rds delete-db-instance --db-instance-identifier nba-sim-db --skip-final-snapshot`

### Phase 3: AWS Glue ETL Job Setup

**See PROGRESS.md Phase 3.2 (lines 744-862)**

1. **Upload ETL script to S3**:
   ```bash
   aws s3 cp scripts/glue/etl_job.py s3://nba-sim-raw-data-lake/scripts/
   ```

2. **Create Glue job**:
   ```bash
   aws glue create-job \
     --name nba-etl-job \
     --role AWSGlueServiceRole-NBA \
     --command Name=glueetl,ScriptLocation=s3://nba-sim-raw-data-lake/scripts/etl_job.py \
     --default-arguments '{
       "--TempDir":"s3://nba-sim-raw-data-lake/temp/",
       "--job-bookmark-option":"job-bookmark-enable",
       "--enable-metrics":"true"
     }' \
     --glue-version "4.0" \
     --number-of-workers 2 \
     --worker-type G.1X
   ```

3. **Run job**:
   ```bash
   aws glue start-job-run --job-name nba-etl-job
   ```

4. **Monitor job run**:
   ```bash
   aws glue get-job-run \
     --job-name nba-etl-job \
     --run-id jr_xxxxx \
     --query 'JobRun.{State:JobRunState,Duration:ExecutionTime}'
   ```

5. **Check CloudWatch logs** for errors:
   ```bash
   aws logs tail /aws-glue/jobs/output --follow
   ```

**Cost:** ~$0.44/hour (2 DPU G.1X workers)

### Phase 4: EC2 Instance Setup (Optional)

**See PROGRESS.md Phase 4 (simulation host)**

1. **Launch instance**:
   ```bash
   aws ec2 run-instances \
     --image-id ami-xxxxx \  # Amazon Linux 2023
     --instance-type t2.micro \
     --key-name my-key-pair \
     --security-group-ids sg-xxxxx \
     --subnet-id subnet-xxxxx \
     --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=nba-sim-host}]'
   ```

2. **Wait for running state**:
   ```bash
   aws ec2 wait instance-running --instance-ids i-xxxxx
   ```

3. **Get public IP**:
   ```bash
   aws ec2 describe-instances \
     --instance-ids i-xxxxx \
     --query 'Reservations[0].Instances[0].PublicIpAddress' \
     --output text
   ```

4. **SSH into instance**:
   ```bash
   ssh -i ~/.ssh/my-key-pair.pem ec2-user@<PUBLIC_IP>
   ```

5. **Install dependencies**:
   ```bash
   sudo yum update -y
   sudo yum install -y python3 postgresql
   pip3 install boto3 psycopg2-binary pandas
   ```

6. **Test RDS connection** from EC2:
   ```bash
   psql -h <rds-endpoint> -U admin -d nba_sim -c "SELECT 1;"
   ```

**Cost:** ~$8-15/month (t2.micro, variable hours)
**Stop when not in use:** `aws ec2 stop-instances --instance-ids i-xxxxx`

### Phase 5: SageMaker Notebook Setup

**See PROGRESS.md Phase 5 (ML training)**

1. **Create notebook instance**:
   ```bash
   aws sagemaker create-notebook-instance \
     --notebook-instance-name nba-ml-notebook \
     --instance-type ml.t3.medium \
     --role-arn arn:aws:iam::ACCOUNT_ID:role/SageMakerRole \
     --volume-size-in-gb 10
   ```

2. **Wait for InService status**:
   ```bash
   aws sagemaker describe-notebook-instance \
     --notebook-instance-name nba-ml-notebook \
     --query 'NotebookInstanceStatus'
   ```

3. **Get presigned URL**:
   ```bash
   aws sagemaker create-presigned-notebook-instance-url \
     --notebook-instance-name nba-ml-notebook
   ```

4. **Open Jupyter** in browser (URL from step 3)

5. **Install additional packages**:
   ```bash
   !pip install scikit-learn xgboost lightgbm shap
   ```

6. **Test S3 and RDS connectivity** from notebook

**Cost:** ~$50/month (ml.t3.medium running 24/7)
**Stop when not in use:** `aws sagemaker stop-notebook-instance --notebook-instance-name nba-ml-notebook`

**See PROGRESS.md for complete AWS resource specifications and costs**

---

## üóÑÔ∏è Database Migration Workflow

**For setting up PostgreSQL schema and loading data**

### Initial Schema Setup

1. **Connect to RDS instance**:
   ```bash
   psql -h <rds-endpoint> -U admin -d nba_sim
   ```

2. **Create schema from DDL file**:
   ```sql
   \i scripts/sql/schema.sql
   ```

   **Or run commands individually**:
   ```sql
   CREATE SCHEMA IF NOT EXISTS nba;

   CREATE TABLE nba.games (
     game_id VARCHAR(50) PRIMARY KEY,
     season INTEGER NOT NULL,
     game_date DATE NOT NULL,
     home_team VARCHAR(50) NOT NULL,
     away_team VARCHAR(50) NOT NULL,
     home_score INTEGER,
     away_score INTEGER
   );

   CREATE TABLE nba.player_stats (
     stat_id SERIAL PRIMARY KEY,
     game_id VARCHAR(50) REFERENCES nba.games(game_id),
     player_id VARCHAR(50) NOT NULL,
     player_name VARCHAR(255),
     team VARCHAR(50),
     minutes DECIMAL(5,2),
     points INTEGER,
     rebounds INTEGER,
     assists INTEGER
   );

   -- Add indexes for common queries
   CREATE INDEX idx_games_season ON nba.games(season);
   CREATE INDEX idx_games_date ON nba.games(game_date);
   CREATE INDEX idx_player_stats_game ON nba.player_stats(game_id);
   CREATE INDEX idx_player_stats_player ON nba.player_stats(player_id);
   ```

3. **Verify schema created**:
   ```sql
   \dt nba.*
   \d+ nba.games
   ```

4. **Set permissions** (if multi-user):
   ```sql
   GRANT ALL PRIVILEGES ON SCHEMA nba TO admin;
   GRANT SELECT ON ALL TABLES IN SCHEMA nba TO readonly_user;
   ```

### Data Loading from Glue ETL

**After Glue ETL job completes, data automatically loads to RDS**

**Verify data loaded:**
```sql
-- Check row counts
SELECT 'games' as table_name, COUNT(*) as row_count FROM nba.games
UNION ALL
SELECT 'player_stats', COUNT(*) FROM nba.player_stats;

-- Check date range
SELECT MIN(game_date), MAX(game_date) FROM nba.games;

-- Sample data
SELECT * FROM nba.games ORDER BY game_date DESC LIMIT 10;
```

### Manual Data Loading (Alternative)

**If loading from local CSV files:**

```bash
# Copy CSV to S3
aws s3 cp data/games.csv s3://nba-sim-raw-data-lake/temp/

# Load from S3 to RDS using psql
psql -h <endpoint> -U admin -d nba_sim -c "
  COPY nba.games FROM 's3://nba-sim-raw-data-lake/temp/games.csv'
  WITH (FORMAT csv, HEADER true);
"
```

**Or use Python script:**
```python
import pandas as pd
import psycopg2

# Read CSV
df = pd.read_csv('data/games.csv')

# Connect to RDS
conn = psycopg2.connect(
    host='<endpoint>',
    database='nba_sim',
    user='admin',
    password='<password>'
)

# Load to database
df.to_sql('games', conn, schema='nba', if_exists='append', index=False)
```

### Schema Modifications (Adding Columns/Tables)

1. **Create migration script** (`scripts/sql/migrations/001_add_column.sql`):
   ```sql
   ALTER TABLE nba.games ADD COLUMN playoff_game BOOLEAN DEFAULT FALSE;
   ```

2. **Test on local database first** (if available)

3. **Backup RDS before migration**:
   ```bash
   aws rds create-db-snapshot \
     --db-instance-identifier nba-sim-db \
     --db-snapshot-identifier nba-sim-pre-migration-$(date +%Y%m%d)
   ```

4. **Run migration**:
   ```bash
   psql -h <endpoint> -U admin -d nba_sim -f scripts/sql/migrations/001_add_column.sql
   ```

5. **Verify migration**:
   ```sql
   \d+ nba.games  -- Check column added
   ```

6. **Document migration** in COMMAND_LOG.md:
   - Migration file path
   - What changed (DDL statements)
   - Why changed (business reason)
   - Data impact (rows affected, downtime)

### Database Maintenance

**Regular maintenance tasks:**

```sql
-- Vacuum and analyze (rebuild indexes, update statistics)
VACUUM ANALYZE nba.games;
VACUUM ANALYZE nba.player_stats;

-- Check table sizes
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS size
FROM pg_tables
WHERE schemaname = 'nba'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- Check index usage
SELECT
  schemaname,
  tablename,
  indexname,
  idx_scan AS index_scans
FROM pg_stat_user_indexes
WHERE schemaname = 'nba'
ORDER BY idx_scan ASC;  -- Low scans = unused index

-- Check for bloat
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) AS total_size,
  ROUND(100 * pg_total_relation_size(schemaname||'.'||tablename) / pg_database_size(current_database()), 2) AS percent_of_db
FROM pg_tables
WHERE schemaname = 'nba';
```

**Schedule maintenance:**
- **Weekly:** VACUUM ANALYZE
- **Monthly:** Review index usage, check for bloat
- **Quarterly:** Consider VACUUM FULL (requires downtime)

**See `docs/RDS_CONNECTION.md` for complete database procedures**

---

## üîß Makefile Workflow Quick Reference

**Common Makefile targets available in this project**

### Verification Commands

```bash
make verify-env         # Verify conda environment
make verify-aws         # Verify AWS credentials and connectivity
make verify-files       # Check expected files exist
make verify-all         # Run all verifications
```

### Documentation Commands

```bash
make inventory          # Update FILE_INVENTORY.md
make update-docs        # Weekly maintenance (inventory + checks)
make sync-progress      # Sync PROGRESS.md with actual AWS resources
make stats              # Show project statistics
make describe FILE=...  # Show detailed file info
```

### Cost Management Commands

```bash
make check-costs        # Check current AWS costs
```

### Backup Commands

```bash
make backup             # Create full backup of working tree
```

### Cleanup Commands

```bash
make clean              # Remove temporary files
```

### AWS Resource Setup Commands

```bash
make setup-glue         # Set up AWS Glue Crawler
make setup-rds          # Set up RDS PostgreSQL
make run-etl            # Execute Glue ETL job
```

**See `Makefile` for complete command list and implementation**

---

## üéØ Summary - Priority Order

**Every session follows this order:**

1. ‚úÖ Initialize session (`session_manager.sh start`)
2. ‚úÖ Orient to current state (read PROGRESS.md)
3. ‚úÖ Ask about completed work
4. ‚úÖ Offer time-based maintenance (if applicable)
5. ‚úÖ Wait for user's task request
6. ‚úÖ Follow decision workflow (check plan, prerequisites, costs, risks)
7. ‚úÖ Execute task (following specific workflow for task type)
8. ‚úÖ Document outcome (COMMAND_LOG.md, inventories, updates)
9. ‚úÖ Wait for confirmation before marking complete
10. ‚úÖ Update PROGRESS.md
11. ‚úÖ Follow security protocol for commits (scan, approve, commit)
12. ‚úÖ Follow pre-push protocol for pushes (inspect, approve, push)
13. ‚úÖ Offer documentation updates (if triggered)
14. ‚úÖ Suggest next action
15. ‚úÖ Monitor context usage (auto-save at 75%, warn at 90%)
16. ‚úÖ Session end reminders

**Never deviate from this order unless user explicitly requests a plan change.**