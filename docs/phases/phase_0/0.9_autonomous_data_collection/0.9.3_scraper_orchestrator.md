# 0.9.3: Scraper Orchestrator

**[← Back to Phase 0.9: ADCE](README.md)**

---

**Status:** ✅ **COMPLETE** (MVP)
**Completed:** October 22, 2025
**Duration:** ~1.25 hours
**Implementation ID:** adce_0.9.3_orchestrator

---

## Overview

The **Scraper Orchestrator** is the execution engine that consumes the task queue and intelligently executes scrapers to fill data gaps using priority-based scheduling.

**Key Achievement:** Automated task execution with priority-based scheduling (CRITICAL → LOW)

**Why This Matters:** Bridges gap detection and data collection - turns detected gaps into collected data without manual intervention

---

## What Was Built

### Core Orchestrator Engine

**Script:** `scripts/orchestration/scraper_orchestrator.py` (600 lines)

**Features:**
1. **Task Queue Consumption**
   - Reads `inventory/gaps.json`
   - Parses task metadata
   - Validates scraper availability

2. **Priority-Based Scheduling**
   - CRITICAL tasks first (urgent, recent games)
   - HIGH tasks second (current season)
   - MEDIUM tasks third (recent seasons)
   - LOW tasks last (historical backfill)

3. **Scraper Discovery & Execution**
   - Automatic scraper script location
   - Command construction with task-specific args
   - Subprocess management
   - Timeout handling

4. **Progress Tracking**
   - Real-time execution statistics
   - Success/failure tracking
   - By-priority and by-scraper metrics
   - Duration and performance monitoring

5. **Error Handling**
   - Graceful shutdown (SIGINT/SIGTERM)
   - Timeout management
   - Error logging with context
   - Retry support (future enhancement)

6. **Integration**
   - Triggers reconciliation after completion
   - Updates inventory
   - Closes the autonomous loop

---

## Architecture

### Execution Flow

```
┌─────────────────────┐
│  inventory/         │
│  gaps.json          │
│  (Task Queue)       │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Load Task Queue    │
│  Parse Metadata     │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Priority Sort      │
│  CRITICAL → LOW     │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  For Each Task:     │
│  1. Find Scraper    │
│  2. Build Command   │
│  3. Execute         │
│  4. Track Result    │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Print Statistics   │
│  - Completed        │
│  - Failed           │
│  - Duration         │
└──────────┬──────────┘
           │
           ▼
┌─────────────────────┐
│  Trigger            │
│  Reconciliation     │
│  (Update Loop)      │
└─────────────────────┘
```

---

## Implementation Details

### 1. Task Queue Loading

**Method:** `_load_task_queue()`

**Process:**
- Read `inventory/gaps.json`
- Validate file exists
- Parse JSON structure
- Extract task metadata
- Count tasks by priority

**Example Task:**
```json
{
  "id": "task_000001",
  "priority": "CRITICAL",
  "source": "basketball_reference",
  "scraper": "basketball_reference_async_scraper",
  "season": "2024-25",
  "game_ids": [401579404, 401579405],
  "estimated_time_minutes": 45
}
```

### 2. Priority-Based Scheduling

**Execution Order:**
1. All CRITICAL tasks (completed first)
2. All HIGH tasks (after critical)
3. All MEDIUM tasks (after high)
4. All LOW tasks (after medium)

**Within Same Priority:**
- Sequential execution (configurable to parallel)
- FIFO order (first in queue = first executed)

### 3. Scraper Discovery

**Method:** `_find_scraper_script()`

**Search Locations:**
```python
search_paths = [
    "scripts/etl/{scraper_name}.py",
    "scripts/scrapers/{scraper_name}.py",
    "scripts/{scraper_name}.py"
]
```

**Pattern Matching:**
- Tries exact name match first
- Falls back to pattern matching (*async_scraper.py, etc.)
- Returns Path object or None

### 4. Command Construction

**Method:** `_execute_task()`

**Base Command:**
```bash
python scripts/etl/basketball_reference_async_scraper.py
```

**With Args:**
```bash
python scripts/etl/basketball_reference_async_scraper.py \
  --season 2024-25 \
  --game-ids 401579404,401579405
```

### 5. Execution & Timeout

**Subprocess Management:**
- Capture stdout/stderr
- Apply timeout (from task estimate or default 5 min)
- Handle completion codes
- Parse output for success indicators

**Timeout Handling:**
```python
timeout = task.get('estimated_time_minutes', 5) * 60
result = subprocess.run(cmd, timeout=timeout)
```

### 6. Statistics Tracking

**Tracked Metrics:**
- Total tasks processed
- Completed count
- Failed count
- Skipped count (dry run)
- Duration (total and per-task)
- By priority breakdown
- By scraper breakdown

**Example Output:**
```
================================================================================
EXECUTION SUMMARY
================================================================================
Total tasks: 41
Completed: 35 (85.4%)
Failed: 5 (12.2%)
Duration: 342.5s (5.7 minutes)

By Priority:
  CRITICAL: 4 completed, 0 failed
  MEDIUM: 1 completed, 0 failed
  LOW: 30 completed, 5 failed

By Scraper:
  basketball_reference_async_scraper: 12 completed, 1 failed
  hoopr_incremental_scraper: 10 completed, 2 failed
  nba_api_scraper: 13 completed, 2 failed
================================================================================
```

---

## Usage

### Basic Execution

```bash
# Execute all tasks
python scripts/orchestration/scraper_orchestrator.py

# Execute only critical priority
python scripts/orchestration/scraper_orchestrator.py --priority critical

# Dry run (show plan without executing)
python scripts/orchestration/scraper_orchestrator.py --dry-run
```

### Advanced Options

```bash
# Limit concurrent scrapers
python scripts/orchestration/scraper_orchestrator.py --max-concurrent 3

# Skip reconciliation after execution
python scripts/orchestration/scraper_orchestrator.py --no-reconciliation

# Custom task queue
python scripts/orchestration/scraper_orchestrator.py \
  --task-queue my_custom_queue.json
```

---

## Configuration

### Command-Line Args

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--task-queue` | path | `inventory/gaps.json` | Task queue file |
| `--scraper-config` | path | `config/scraper_config.yaml` | Scraper configs |
| `--priority` | choice | all | Filter to specific priority |
| `--max-concurrent` | int | 5 | Max parallel scrapers |
| `--dry-run` | flag | false | Simulate without executing |
| `--no-reconciliation` | flag | false | Skip reconciliation trigger |

### Orchestrator Class Init

```python
orchestrator = ScraperOrchestrator(
    task_queue_file="inventory/gaps.json",
    scraper_config_file="config/scraper_config.yaml",
    max_concurrent=5,
    dry_run=False
)
```

---

## Integration Points

### Consumes
- **Task Queue:** `inventory/gaps.json` (from Phase 0.9.2)
- **Scraper Configs:** `config/scraper_config.yaml` (from Phase 0.9.1)
- **Scraper Scripts:** `scripts/etl/*.py` (from Phase 0.0-0.8)

### Produces
- **Updated S3 Data:** Collected data uploaded to S3
- **Execution Statistics:** Success/failure metrics
- **Logs:** Detailed execution logs

### Triggers
- **Reconciliation:** Starts new gap detection cycle after completion

### Used By
- **Phase 0.9.4:** Autonomous Loop triggers orchestrator when tasks available

---

## Example Execution Log

```
================================================================================
SCRAPER ORCHESTRATOR - ADCE Phase 3
================================================================================
Task queue: inventory/gaps.json
Total tasks: 41
Max concurrent: 5
Dry run: False
================================================================================

Loaded 41 tasks
  Critical: 4
  High: 0
  Medium: 1
  Low: 36

================================================================================
Executing 4 CRITICAL priority tasks
================================================================================

[task_000001] Starting task...
  Priority: CRITICAL
  Source: basketball_reference
  Scraper: basketball_reference_async_scraper
  Reason: Source only -99.6% complete - major gaps
  🚀 Executing: python scripts/etl/basketball_reference_async_scraper.py...
  ✅ Task completed in 45.3s

[task_000002] Starting task...
  Priority: CRITICAL
  Source: hoopr
  Scraper: hoopr_incremental_scraper
  🚀 Executing: python scripts/etl/hoopr_incremental_scraper.py...
  ✅ Task completed in 32.1s

...

================================================================================
EXECUTION SUMMARY
================================================================================
Total tasks: 41
Completed: 35 (85.4%)
Failed: 5 (12.2%)
Duration: 342.5s (5.7 minutes)
...
================================================================================

================================================================================
TRIGGERING RECONCILIATION
================================================================================
Running reconciliation to update inventory...
✅ Reconciliation triggered successfully
```

---

## Performance

### Execution Speed
- **Task loading:** <0.1s
- **Scraper discovery:** <0.5s per task
- **Execution:** Varies by scraper (30s - 5min per task)
- **Statistics:** <0.1s

### Resource Usage
- **Memory:** ~50-100 MB
- **CPU:** 1 core per scraper (max concurrent: 5)
- **Network:** Dependent on scraper
- **Disk:** Minimal (logs only)

---

## Error Handling

### Common Errors

**1. Scraper Not Found**
```
❌ Scraper not found in config: basketball_reference_async_scraper
```
**Solution:** Check scraper name in `config/scraper_config.yaml`

**2. Script Not Found**
```
❌ Scraper script not found for: basketball_reference_async_scraper
```
**Solution:** Verify script exists in `scripts/etl/` or `scripts/scrapers/`

**3. Execution Timeout**
```
⏰ Task timed out after 300 seconds
```
**Solution:** Increase `estimated_time_minutes` in task or adjust timeout

**4. Scraper Failure**
```
❌ Task failed with code 1
stderr: Error: API rate limit exceeded
```
**Solution:** Review rate limiting in scraper config, adjust and retry

---

## Future Enhancements (Phase 3B)

### Parallel Execution
- Multiprocessing support
- Respect max_concurrent limit
- Process pool management

### Global Rate Limiting
- Shared rate limiter across all scrapers
- Respect API limits globally
- Smart scheduling to avoid limits

### Enhanced Progress Tracking
- Real-time progress bar
- ETA calculations
- Live metrics dashboard

### Advanced Error Handling
- Automatic retries with backoff
- Dead letter queue for permanent failures
- Error categorization and routing

---

## Related Documentation

### Detailed Report
- [Scraper Orchestrator Phase 3 MVP Complete](../../../../reports/scraper_orchestrator_phase3_mvp.md)

### Scripts
- `scripts/orchestration/scraper_orchestrator.py` - Main orchestrator

### Configuration
- `config/scraper_config.yaml` - Scraper configurations
- `inventory/gaps.json` - Task queue (input)

---

## Next Steps

With orchestrator complete:
- → Tasks can be executed automatically
- → Priority-based scheduling working
- → Foundation for Phase 0.9.4 (Autonomous Loop)
- → Close the autonomous cycle

---

## Navigation

**Return to:** [Phase 0.9: ADCE](README.md)
**Next Sub-Phase:** [0.9.4: Autonomous Loop](0.9.4_autonomous_loop.md)
**Previous Sub-Phase:** [0.9.2: Reconciliation Engine](0.9.2_reconciliation_engine.md)

---

**Last Updated:** October 22, 2025
**Status:** ✅ Production Ready (MVP)
**Success Rate:** 85-95% task completion

