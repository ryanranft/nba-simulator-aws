# Sub-Phase 8.0: Recursive Data Discovery

**Parent Phase:** [Phase 8: Data Audit & Inventory](../PHASE_8_INDEX.md)
**Status:** ‚úÖ COMPLETE
**Time Estimate:** 1-2 hours
**Cost:** $0 (local searches + minimal S3 LIST requests)
**Completed:** October 11, 2025

---

## Overview

Systematically search ALL possible storage locations to discover every data file related to the project. This sub-phase uses a recursive discovery approach: search ‚Üí find ‚Üí search again until no new data is discovered in repeated searches.

**Goal:** Create complete inventory of data holdings with zero blind spots.

**Approach:**
1. Search internal project directories
2. Search external locations (repos, downloads, desktop)
3. Discover all databases (cloud + local)
4. Inventory cloud storage (S3, etc.)
5. Repeat searches until no new data found

---

## Step-by-Step Execution

### Phase 1: Internal Project Directory Search

**Purpose:** Find all data files within the project directory.

**Commands:**
```bash
# Count all data files
find /Users/ryanranft/nba-simulator-aws -type f \( -name "*.json" -o -name "*.csv" -o -name "*.db" -o -name "*.sqlite" -o -name "*.parquet" \) ! -path "*/node_modules/*" ! -path "*/.git/*" 2>/dev/null | wc -l

# Break down by type
echo "JSON: $(find data/ -name "*.json" | wc -l)"
echo "CSV: $(find data/ -name "*.csv" | wc -l)"
echo "Databases: $(find data/ -name "*.db" -o -name "*.sqlite" | wc -l)"

# Break down by subdirectory
for dir in data/*/; do
    echo "$(basename $dir): $(find $dir -name "*.json" | wc -l) JSON files"
done
```

**Results (October 11, 2025):**
- Total files: 146,150
- ESPN data: 146,115 JSON files
  - nba_pbp/: 44,826
  - nba_box_score/: 44,828
  - nba_team_stats/: 44,828
  - nba_schedule_json/: 11,633
- Kaggle data: 35 files (CSV + 1 SQLite database)
- Backups: 2 unified database backups

**Documentation:**
- Log findings in audit notes
- Record file counts per directory
- Note any unexpected file types or locations

---

### Phase 2: External Location Discovery

**Purpose:** Find data in external repositories, downloads, and other locations outside the project.

**Commands:**
```bash
# Search external multi-sport repo
find /Users/ryanranft/0espn -type f \( -name "*.json" -o -name "*.db" -o -name "*.sqlite" -o -name "*.csv" \) 2>/dev/null | wc -l

# Break down by sport
for sport in nba nfl nhl cfb ncaam ncaaw wnba; do
    count=$(find /Users/ryanranft/0espn/data/$sport -type f -name "*.json" 2>/dev/null | wc -l)
    echo "$sport: $count files"
done

# Search archives
find ~/sports-simulator-archives/nba -type f \( -name "*.json" -o -name "*.db" \) 2>/dev/null | wc -l

# Search downloads/desktop for NBA-related files
find ~/Downloads -name "*nba*" -o -name "*espn*" -o -name "*basketball*" 2>/dev/null | head -20
find ~/Desktop -name "*nba*" -o -name "*espn*" -o -name "*basketball*" 2>/dev/null | head -20
```

**Results (October 11, 2025):**
- 0espn repo: 1,223,071 files total (multi-sport)
  - NBA: 146,266 files (mostly duplicate + 151 unique raw files)
  - NCAAM: 410,637 files
  - NCAAW: 397,017 files
  - NHL: 141,298 files
  - CFB: 60,109 files
  - WNBA: 29,240 files
  - NFL: 26,462 files
- Archives: 0 data files (only session logs)
- Downloads: No NBA data files
- Desktop: No NBA data files

**Key Finding:** 0espn repository contains 151 raw box score files not in main project (2013 season, game IDs 131105xxx).

---

### Phase 3: Database Discovery

**Purpose:** Find all SQLite databases and RDS instances containing project data.

#### SQLite Databases

**Commands:**
```bash
# Find all SQLite databases in project
find /Users/ryanranft/nba-simulator-aws -type f \( -name "*.db" -o -name "*.sqlite" -o -name "*.sqlite3" \) 2>/dev/null

# Get database info
for db in $(find . -name "*.db" -o -name "*.sqlite"); do
    echo "Database: $db"
    echo "Size: $(ls -lh $db | awk '{print $5}')"
    echo "Tables: $(sqlite3 $db '.tables')"
    echo "---"
done
```

**Results (October 11, 2025):**

| Database | Size | Location | Tables | Status |
|----------|------|----------|--------|--------|
| nba.sqlite | 2.2 GB | data/kaggle/ | 16 | ‚úÖ Historical reference (2023) |
| unified_nba.db | 21 MB | backups/20251010/ | 5 | ‚úÖ Quality analysis (Oct 10) |
| unified_nba.db | 21 MB | backups/20251009/ | 5 | ‚ö†Ô∏è Older backup (Oct 9) |

**Kaggle Database Tables:**
- common_player_info: 3,632 rows
- draft_combine_stats: 1,633 rows
- draft_history: 8,257 rows
- game: 65,698 rows
- game_info: 58,053 rows
- game_summary: 58,110 rows
- inactive_players: 110,191 rows
- line_score: 58,053 rows
- officials: 70,971 rows
- other_stats: 28,271 rows
- play_by_play: 13,592,899 rows ‚≠ê (13.6M rows!)
- player: 4,815 rows
- team: 30 rows
- team_details: 27 rows
- team_history: 50 rows
- team_info_common: 0 rows (empty)

**Unified Database Tables:**
- source_coverage: 31,243 rows (game coverage by source)
- data_quality_discrepancies: 50,947 rows (cross-source issues)
- quality_scores: 31,243 rows (source quality rankings)
- unified_play_by_play: 0 rows (empty - metadata only)
- unified_schedule: 0 rows (empty - metadata only)

#### AWS RDS PostgreSQL

**Commands:**
```bash
# List RDS instances
aws rds describe-db-instances --query 'DBInstances[*].[DBInstanceIdentifier,DBInstanceStatus,Engine,DBInstanceClass,AllocatedStorage]' --output table

# Get endpoint
aws rds describe-db-instances --db-instance-identifier nba-sim-db --query 'DBInstances[0].Endpoint.Address' --output text

# Connect and list tables (Python)
python3 << 'EOF'
import psycopg2
conn = psycopg2.connect(
    host="nba-sim-db.ck96ciigs7fy.us-east-1.rds.amazonaws.com",
    port=5432,
    database="nba_simulator",
    user="postgres",
    password="<from-credentials-file>",
    sslmode="require"
)
cur = conn.cursor()
cur.execute("SELECT table_name FROM information_schema.tables WHERE table_schema='public' ORDER BY table_name;")
print(f"Tables: {len(cur.fetchall())}")

# Get row counts
for table in tables:
    cur.execute(f"SELECT COUNT(*) FROM {table};")
    count = cur.fetchone()[0]
    print(f"{table}: {count:,} rows")
conn.close()
EOF
```

**Results (October 11, 2025):**
- **Instance:** nba-sim-db (db.t3.small, 20 GB, PostgreSQL)
- **Status:** Available
- **Tables:** 23 tables
- **Total Rows:** 48,436,280 rows ‚≠ê

**Populated Tables (15 with data):**
| Table | Rows | Purpose |
|-------|------|---------|
| temporal_events | 14,114,617 | Kaggle temporal PBP |
| unified_play_by_play | 13,074,829 | Multi-source PBP (hoopR) |
| hoopr_play_by_play | 13,074,829 | hoopR source data |
| play_by_play | 6,781,155 | ESPN play-by-play |
| hoopr_player_box | 785,505 | hoopR player stats |
| box_score_players | 408,833 | Player box scores |
| hoopr_team_box | 59,670 | hoopR team stats |
| games | 44,828 | Game master table |
| unified_schedule | 40,652 | Multi-source schedule |
| hoopr_schedule | 30,758 | hoopR schedule |
| box_score_teams | 15,900 | Team box scores |
| player_biographical | 3,632 | Player info |
| team_seasons | 952 | Team season records |
| teams | 87 | Team master |
| data_source_coverage | 33 | Source metadata |

**Empty Tables (8 need population):**
- game_states (0 rows) - üî¥ HIGH priority
- player_game_stats (0 rows) - üî¥ HIGH priority
- player_snapshots (0 rows) - üü° MEDIUM priority
- players (0 rows) - üî¥ HIGH priority
- plays (0 rows) - üü° MEDIUM priority
- possession_panel (0 rows) - üü¢ LOW priority
- possession_panel_pbpstats (0 rows) - üü¢ LOW priority
- team_game_stats (0 rows) - üî¥ HIGH priority

---

### Phase 4: S3 Complete Inventory

**Purpose:** Catalog all data in the S3 data lake with complete breakdown by source.

**Commands:**
```bash
# Total file count
aws s3 ls s3://nba-sim-raw-data-lake/ --recursive | wc -l

# List top-level prefixes
aws s3 ls s3://nba-sim-raw-data-lake/ | grep PRE

# Count files per data source
for prefix in pbp box_scores team_stats schedule nba_api_comprehensive nba_api_playbyplay hoopr_phase1 hoopr_parquet basketball_reference sportsdataverse ml-features ml-models ml-predictions athena-results scripts; do
    count=$(aws s3 ls s3://nba-sim-raw-data-lake/${prefix}/ --recursive 2>/dev/null | wc -l | tr -d ' ')
    echo "${prefix}: ${count} files"
done
```

**Results (October 11, 2025):**
- **Total S3 Files:** 172,597

**ESPN Data (147,380 files):**
| Data Type | Files | Prefix | Date Range |
|-----------|-------|--------|------------|
| Play-by-Play | 44,826 | pbp/ | 1993-2025 |
| Box Scores | 44,828 | box_scores/ | 1993-2025 |
| Team Stats | 46,093 | team_stats/ | 1993-2025 |
| Schedule | 11,633 | schedule/ | 1993-2025 |

‚ö†Ô∏è **Sync Issue:** S3 team_stats (46,093) has 1,265 MORE files than local (44,828)

**NBA API Data (24,419 files):**
| Data Type | Files | Prefix | Date Range |
|-----------|-------|--------|------------|
| Comprehensive | 22,256 | nba_api_comprehensive/ | 1995-2006 |
| Play-by-Play | 2,163 | nba_api_playbyplay/ | 1995-2006 |

Breakdown of comprehensive:
- Boxscores Advanced: 13,940 files (player box scores 1995-2005)
- Play-by-Play: 6,973 files
- League Dashboards: 77 files (includes lineup data 1996-2006)
- Player Info: 453 files
- Tracking: 86 files

**hoopR Data (314 files):**
| Data Type | Files | Prefix | Format |
|-----------|-------|--------|--------|
| Phase 1 | 218 | hoopr_phase1/ | JSON |
| Parquet | 96 | hoopr_parquet/ | Parquet |

**Other Sources:**
| Source | Files | Prefix |
|--------|-------|--------|
| Basketball Reference | 444 | basketball_reference/ |
| Sportsdataverse | 12 | sportsdataverse/ |
| ML Features | 3 | ml-features/ |
| ML Models | 7 | ml-models/ |
| ML Predictions | 10 | ml-predictions/ |
| Athena Results | 7 | athena-results/ |
| Scripts | 1 | scripts/ |

---

## Phase 5: Verification Pass

**Purpose:** Verify no new data discovered in repeated searches (recursive completion).

**Commands:**
```bash
# Re-run Phase 1 count
find /Users/ryanranft/nba-simulator-aws -type f \( -name "*.json" -o -name "*.csv" -o -name "*.db" \) 2>/dev/null | wc -l

# Compare to Phase 1 result - should match exactly
# If different, investigate new files and repeat search
```

**Results (October 11, 2025):**
- Phase 1 count: 146,150 files
- Phase 5 verification count: 146,150 files ‚úÖ
- **Match confirmed** - no new data discovered

**Recursive discovery complete:** Repeated search found zero new files.

---

## Summary of Findings

### Data Holdings Inventory

| Location | Type | Count | Status |
|----------|------|-------|--------|
| **Local Project** | JSON | 146,115 | ‚úÖ Synced with S3 (except team_stats) |
| **Local Project** | SQLite | 3 databases | ‚úÖ 2.2 GB + 42 MB |
| **AWS S3** | JSON/Parquet | 172,597 | ‚úÖ Complete data lake |
| **AWS RDS** | PostgreSQL | 48.4M rows | ‚úÖ 23 tables (15 populated) |
| **External (0espn)** | JSON | 1,223,071 | ‚ö†Ô∏è Mostly duplicate |
| **Archives** | Logs | Session logs | ‚úÖ No data files |

### Critical Findings

**Data Gaps:**
1. üî¥ **CRITICAL:** Box score players missing 2006-2025 (19 seasons)
   - Have: 1995-2005 in NBA API
   - Need: 2006-2025

2. üî¥ **CRITICAL:** Lineup data missing 2007-2025 (18 seasons)
   - Have: 1996-2006 in NBA API
   - Need: 2007-2025

**Sync Issues:**
1. üü° **MEDIUM:** S3 team_stats (+1,265 files vs local)
   - S3: 46,093 files
   - Local: 44,828 files
   - Action: Download missing 1,265 files

**Database Issues:**
1. üü° **MEDIUM:** 8 RDS tables empty
   - Can be populated from existing data
   - Need ETL scripts to load

**External Data:**
1. üì¶ 0espn repository has 151 unique raw box score files (2013 season)
   - Game IDs: 131105xxx
   - Not present in main project
   - Decision needed: integrate or ignore?

---

## Outputs Created

1. **MASTER_DATA_INVENTORY.md** (docs/)
   - 12 sections covering all storage locations
   - Complete file counts and row counts
   - Data gap analysis
   - Action items with priorities
   - 500+ lines of comprehensive documentation

2. **Audit Notes** (session logs)
   - All commands executed
   - All counts recorded
   - Findings documented
   - Decision points logged

---

## Next Steps

After completing this sub-phase:
1. ‚úÖ All storage locations searched
2. ‚úÖ Complete inventory documented
3. ‚Üí Proceed to Sub-Phase 8.1: Deep Content Analysis
4. ‚Üí Or proceed to Phase 4: Integrate External Data

---

## Reusable Workflow

**To audit a new sport or project:**

1. **Copy this file** as template
2. **Update paths** for new sport:
   ```bash
   # Replace "nba-simulator-aws" with new project
   # Replace "nba" with sport name (nfl, mlb, nhl)
   ```
3. **Execute each phase** in order
4. **Document findings** in new MASTER_DATA_INVENTORY_<SPORT>.md
5. **Identify gaps** specific to that sport
6. **Verify recursively** until no new data found

**Time to complete:** 1-2 hours per sport/project

---

## Troubleshooting

**Issue:** S3 LIST commands timing out
**Solution:**
```bash
aws s3 ls s3://bucket/ --recursive --page-size 1000 | wc -l
```

**Issue:** Database connection refused
**Solution:**
- Check security group allows current IP
- Verify credentials loaded: `echo $DB_HOST`
- Test connection: `psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT 1;"`

**Issue:** File counts differ between runs
**Solution:**
- Files being added/modified during audit
- Run during quiet period or take snapshot first

**Issue:** Unknown external locations
**Solution:**
- Search common locations: Downloads, Desktop, Documents
- Check git repos for related projects
- Ask team members about data locations

---

## Key Learnings

**What worked:**
- ‚úÖ Systematic location-by-location search
- ‚úÖ Recursive verification (search until no new data)
- ‚úÖ Breaking down by data source and type
- ‚úÖ Documenting counts immediately

**What was surprising:**
- RDS had 48.4M rows (way more than expected!)
- 0espn had 1.2M files across 7 sports
- S3 had 1,265 MORE team_stats files than local
- Kaggle DB had 13.6M play-by-play rows

**Recommendations:**
- Always run recursive verification
- Document sync issues immediately
- Check external repos before scraping duplicates
- Audit regularly (quarterly) to catch drift

---

## Navigation

**Return to:** [Phase 8 Index](../PHASE_8_INDEX.md) | [PROGRESS.md](../../../PROGRESS.md)
**Next:** [Sub-Phase 8.1: Deep Content Analysis](8.1_deep_content_analysis.md)

---

*This sub-phase discovered ALL data holdings across all storage locations and created a complete inventory with zero blind spots.*

---

**Completed:** October 11, 2025
**Duration:** ~1.5 hours
**Files discovered:** 1,541,818 total across all locations
**Documentation created:** MASTER_DATA_INVENTORY.md (500+ lines)