# Phase 1: Data Quality & Gap Analysis

**Status:** ⏸️ READY TO IMPLEMENT (multi-source integration planned - 209 features)
**Prerequisites:** Phase 0 complete (data in S3)
**Estimated Time:** 28 hours over 4 weeks
**Estimated Cost:** $5-8/month (additional sources)

---

> **📌 NOTE - Phase Reorganization:**
>
> This is the NEW Phase 1 as of October 4, 2025 (ADR-008).
> Previously, this content was in Phase 0 (Data Source Definition & Verification).
>
> **Old structure:** Phase 0 = Data Verification → Phase 1 = S3 Upload
> **New structure:** Phase 0 = Data Collection → Phase 1 = Quality Analysis
>
> See `docs/adr/008-phase-reorganization.md` for rationale.

---

> **🚀 ENHANCEMENT PLANNING COMPLETE (October 6, 2025):**
>
> This phase now has comprehensive multi-source integration planning:
> - **209 features** from 5 data sources (ESPN, Basketball Reference, NBA.com Stats, Kaggle, Derived)
> - **28-hour implementation roadmap** over 4 weeks
> - **Complete documentation:** See links in PROGRESS.md Phase Details section
>
> Key documents:
> - [ML_FEATURE_CATALOG.md](../ML_FEATURE_CATALOG.md) - All 209 features cataloged
> - [IMPLEMENTATION_CHECKLIST.md](../IMPLEMENTATION_CHECKLIST.md) - Week-by-week tasks
> - [QUICK_START_MULTI_SOURCE.md](../QUICK_START_MULTI_SOURCE.md) - Quick start guide
> - [PHASE_1_MULTI_SOURCE_PLAN.md](PHASE_1_MULTI_SOURCE_PLAN.md) - Detailed roadmap

---

## Overview

Integrate multiple data sources to maximize ML granularity. Originally planned as simple data quality verification, this phase has evolved into comprehensive multi-source data integration to extract **209 features** for machine learning.

**This phase delivers:**
- **ESPN data (58 features):** Already in S3 - basic box scores, play-by-play
- **Basketball Reference (47 features):** Advanced metrics - TS%, PER, BPM, Win Shares, Four Factors
- **NBA.com Stats (92 features):** Player tracking - movement, touches, shot quality, hustle, defense
- **Kaggle (12 features):** Historical data - fill 1946-1998 gap
- **Derived features (20+ features):** Efficiency, momentum, contextual metrics

**Why multi-source integration matters:**
- ML accuracy boost: 63% → 75-80% (estimated +15-20%)
- Enables defensive impact metrics (not in ESPN)
- Historical coverage: 1946-2025 (vs 1993-2025)
- Confidence scoring for data quality

---

## Prerequisites

Before starting this phase:
- [x] Phase 0 complete (data uploaded to S3)
- [x] S3 bucket accessible
- [ ] Data structure documented (`docs/DATA_STRUCTURE_GUIDE.md`)
- [ ] User has chosen verification sources (see options below)

---

## Implementation Steps

###Sub-Phase 1.1: Analyze S3 Data Coverage

**Status:** ⏸️ PENDING
**Time Estimate:** 1 hour

**Purpose:** Understand what data exists in S3, identify quality issues

**Follow these workflows:**
- Workflow #21 ([Data Validation](../claude_workflows/workflow_descriptions/21_data_validation.md))
  - **When to run:** First step after Phase 0 completes
  - **Purpose:** Sample S3 files, check for valid JSON, identify empty files

**Tasks:**

1. **Count total files in S3:**
```bash
aws s3 ls s3://nba-sim-raw-data-lake/ --recursive | grep "\.json$" | wc -l
```

2. **Check S3 storage size:**
```bash
aws s3 ls s3://nba-sim-raw-data-lake/ --recursive --summarize | grep "Total Size"
```

3. **Find date range:**
```bash
# First file (earliest date)
aws s3 ls s3://nba-sim-raw-data-lake/schedule/ --recursive | grep "\.json$" | head -1

# Last file (latest date)
aws s3 ls s3://nba-sim-raw-data-lake/schedule/ --recursive | grep "\.json$" | tail -1
```

4. **Sample files for quality:**
```bash
# Download 100 random files
for i in {1..100}; do
  FILE=$(aws s3 ls s3://nba-sim-raw-data-lake/schedule/ --recursive | grep "\.json$" | shuf -n 1 | awk '{print $4}')
  aws s3 cp "s3://nba-sim-raw-data-lake/$FILE" "/tmp/sample_$i.json"

  # Check if valid JSON and non-empty
  if python -m json.tool "/tmp/sample_$i.json" > /dev/null 2>&1; then
    SIZE=$(stat -f%z "/tmp/sample_$i.json")
    if [ $SIZE -gt 100 ]; then
      echo "Valid: $FILE ($SIZE bytes)"
    else
      echo "Empty: $FILE"
    fi
  else
    echo "Invalid JSON: $FILE"
  fi
done
```

**Output:** `docs/DATA_QUALITY_BASELINE.md` with:
- Total files count
- Date range coverage
- % valid vs empty files
- Average file size
- Known issues

---

### Sub-Phase 1.2: Identify Coverage Gaps

**Status:** ⏸️ PENDING
**Time Estimate:** 1 hour

**Purpose:** Find missing dates and data holes

**Tasks:**

1. **Extract all dates from S3 filenames:**
```bash
aws s3 ls s3://nba-sim-raw-data-lake/schedule/ --recursive | \
  grep "\.json$" | \
  grep -oE '[0-9]{8}' | \
  sort | uniq > /tmp/s3_dates.txt

echo "Total unique dates: $(wc -l < /tmp/s3_dates.txt)"
```

2. **Detect gaps > 7 days:**
```python
# scripts/analysis/detect_date_gaps.py
from datetime import datetime, timedelta

with open('/tmp/s3_dates.txt') as f:
    dates = [datetime.strptime(line.strip(), '%Y%m%d') for line in f]

dates.sort()
gaps = []

for i in range(len(dates) - 1):
    diff = (dates[i+1] - dates[i]).days
    if diff > 7:
        gaps.append({
            'start': dates[i],
            'end': dates[i+1],
            'days_missing': diff
        })

print(f"Found {len(gaps)} gaps:")
for gap in gaps:
    print(f"  {gap['start'].date()} to {gap['end'].date()}: {gap['days_missing']} days")
```

3. **Identify today's gap:**
```bash
LAST_DATE=$(tail -1 /tmp/s3_dates.txt)
TODAY=$(date +%Y%m%d)

echo "Last date in S3: $LAST_DATE"
echo "Today's date: $TODAY"
echo "Gap: $(( ($(date -j -f "%Y%m%d" "$TODAY" "+%s") - $(date -j -f "%Y%m%d" "$LAST_DATE" "+%s")) / 86400 )) days"
```

**Output:** List of coverage gaps, prioritized by severity

---

### Sub-Phase 1.3: Upload Local ESPN Data to Fill Known Gaps (If Applicable)

**Status:** ⏸️ PENDING (NBA: may already be complete from Phase 0)
**Time Estimate:** 30 minutes

**When to run:** If you have local ESPN data that's more recent than S3 data

**Purpose:** Sync any local ESPN files not yet in S3

**Commands:**
```bash
# Check if local data is newer than S3
LOCAL_LAST=$(ls -1 /Users/ryanranft/0espn/data/nba/*.json | tail -1 | grep -oE '[0-9]{8}')
S3_LAST=$(aws s3 ls s3://nba-sim-raw-data-lake/schedule/ --recursive | grep "\.json$" | tail -1 | grep -oE '[0-9]{8}')

echo "Local last date: $LOCAL_LAST"
echo "S3 last date: $S3_LAST"

# If local is newer, sync
if [ "$LOCAL_LAST" -gt "$S3_LAST" ]; then
  echo "Uploading newer local files to S3..."
  aws s3 sync /Users/ryanranft/0espn/data/nba/ \
    s3://nba-sim-raw-data-lake/schedule/ \
    --exclude "*" --include "*.json" --size-only
else
  echo "S3 is up to date with local files"
fi
```

**Validation:**
- [ ] S3 last date >= local last date
- [ ] File counts match

---

### Sub-Phase 1.4: Run Automated Gap Filling (Workflow #38)

**Status:** ⏸️ PENDING
**Time Estimate:** 10-60 minutes (depending on gap size)

**Purpose:** Automatically scrape ESPN for missing dates up to today

**Follow Workflow #38:** [Auto-Update ESPN Data](../claude_workflows/workflow_descriptions/38_auto_update_espn_data.md)

**Quick run:**
```bash
bash scripts/etl/auto_update_espn_data.sh
```

**This workflow:**
1. ✓ Checks today's date
2. ✓ Finds last date with data in S3
3. ✓ Calculates gap (days missing)
4. ✓ Updates ESPN scraper end_date automatically
5. ✓ Runs ESPN scraper for missing dates
6. ✓ Uploads new files to S3
7. ✓ Extracts to RDS database (if Phase 3 complete)
8. ✓ Verifies results

**Validation:**
- [ ] Gap filled: S3 last date == today
- [ ] New files uploaded successfully
- [ ] No errors in scraping process

---

### Sub-Phase 1.5: Establish Data Quality Baseline

**Status:** ⏸️ PENDING
**Time Estimate:** 2 hours

**Purpose:** Calculate and document baseline quality metrics

**Quality metrics to track:**

#### 1. Completeness
```python
# Expected games per season: ~82 games/team * 30 teams = ~2,460 games
# Actual: Check S3 files vs expected

import boto3
from datetime import datetime

s3 = boto3.client('s3')
bucket = 'nba-sim-raw-data-lake'

# Count files per season
seasons = {}
response = s3.list_objects_v2(Bucket=bucket, Prefix='schedule/')

for obj in response.get('Contents', []):
    key = obj['Key']
    if key.endswith('.json'):
        # Extract year from YYYYMMDD
        date_str = key.split('/')[-1].replace('.json', '')
        year = int(date_str[:4])
        month = int(date_str[4:6])

        # NBA season spans Oct-Jun
        season = year if month >= 10 else year - 1
        seasons[season] = seasons.get(season, 0) + 1

for season, count in sorted(seasons.items()):
    expected = 1230  # ~82 games/team * 30 teams / 2
    completeness = (count / expected) * 100
    print(f"Season {season}-{season+1}: {count} games ({completeness:.1f}% complete)")
```

#### 2. Accuracy
```python
# Verify against NBA.com Stats API (if chosen as verification source)
# Sample 100 random games, compare scores
# Track: matches / total_checked * 100
```

#### 3. Freshness
```python
from datetime import datetime

last_date = "2025-06-30"  # From S3 analysis
today = datetime.today()
freshness_days = (today - datetime.strptime(last_date, '%Y-%m-%d')).days

print(f"Data freshness: {freshness_days} days old")
# NBA offseason: acceptable if < 180 days
# NBA season: should be < 1 day
```

#### 4. Consistency
```python
# Check for team name variations, player name inconsistencies
# Ensure canonical IDs exist for all entities
```

**Output:** `docs/DATA_QUALITY_BASELINE.md` with all 4 metrics

---

### Sub-Phase 1.6: Data Source Verification Setup (Optional)

**Status:** ⏸️ PENDING USER INPUT
**Time Estimate:** 3-4 hours

**Purpose:** Set up secondary data sources for validation

**User must choose verification source:**

| Source | Type | Cost | Pros | Cons |
|--------|------|------|------|------|
| **NBA.com Stats** | API | Free | Official, authoritative | Rate limits, reverse-engineering |
| **Basketball Reference** | HTML | Free | Most comprehensive | Scraping required, TOS restrictions |
| **SportsData.io** | API | $19/mo | Clean API, reliable | Cost |
| **balldontlie** | API | Free | Simple REST, unlimited | Community-maintained, less reliable |

**Steps for NBA.com Stats API (example):**

1. **Test API access:**
```python
import requests

url = "https://stats.nba.com/stats/scoreboardV2"
params = {'GameDate': '2023-04-10'}
headers = {'User-Agent': 'Mozilla/5.0'}

response = requests.get(url, params=params, headers=headers)
if response.status_code == 200:
    print("✓ NBA.com Stats API accessible")
    print(f"Games on 2023-04-10: {len(response.json()['resultSets'][0]['rowSet'])}")
else:
    print(f"✗ API error: {response.status_code}")
```

2. **Document rate limits:**
- NBA.com Stats: ~10-20 requests/minute (unofficial)
- Implement exponential backoff

3. **Create verification script:**
```python
# scripts/etl/verify_nba_games.py
# Compare ESPN data vs NBA.com Stats
# Log discrepancies to database table
```

4. **Add credentials to environment:**
```bash
# No API key needed for NBA.com Stats
# But add User-Agent to ~/nba-sim-credentials.env
echo 'NBA_STATS_USER_AGENT="Mozilla/5.0"' >> ~/nba-sim-credentials.env
```

**Output:**
- Verification source chosen and documented
- Test script working
- Credentials configured
- Rate limits documented

---

## Multi-Sport Replication Framework

**When adding a new sport (NFL, MLB, NHL, Soccer):**

### Sport-Specific Implementation Registry

**Current implementations:**

#### NBA (Basketball)
**Status:** ⏸️ PENDING (verification sources not yet defined)
**Primary Source:** ESPN JSON files (146,115 files, 119GB, S3)
- Coverage: 1993-2025
- Data quality: 83% valid files
- Missing: 17% empty files (~24,507)
- Fields extracted: 53 (games), play-by-play, player stats, team stats

**Verification Sources:** (User must specify - see options in Sub-Phase 1.6)
- [ ] NBA.com Stats API
- [ ] Basketball Reference
- [ ] SportsData.io
- [ ] balldontlie API
- [ ] Other: ________________

**Critical Fields to Verify:**
- [ ] Game scores (home_score, away_score)
- [ ] Game dates/times
- [ ] Team IDs/names
- [ ] Player stats

**Started:** September 29, 2025
**Primary ETL Complete:** October 2, 2025
**Verification Status:** Pending user input

---

#### NFL (American Football)
**Status:** ⏸️ NOT STARTED
**Primary Sources:** (To be defined by user)
**Verification Sources:** (To be defined by user)

**Note:** ESPN scraper code exists at `/Users/ryanranft/0espn/espn/nfl/` - ready to activate

---

#### MLB (Baseball)
**Status:** ⏸️ NOT STARTED
**Primary Sources:** (To be defined by user)
**Verification Sources:** (To be defined by user)

---

#### NHL (Hockey)
**Status:** ⏸️ NOT STARTED
**Primary Sources:** (To be defined by user)
**Verification Sources:** (To be defined by user)

**Note:** ESPN scraper code exists at `/Users/ryanranft/0espn/espn/nhl/` - ready to activate

---

#### Soccer (Football)
**Status:** ⏸️ NOT STARTED
**Primary Sources:** (To be defined by user)
**Verification Sources:** (To be defined by user)

---

### Step 1: Ask User for Sport-Specific Data Sources

**Claude must ask:**
```
Great! Let's set up data quality analysis for [SPORT]. I need to know:

1. Do you already have data in S3 for [SPORT]?
   (If not, complete Phase 0 first)

2. Primary data source details:
   - What format is the data? (JSON files, API, CSV, database, HTML scraping)
   - How much data? (file count, GB, date range)
   - What's the quality? (completeness %, known gaps)
   - What fields are included? (list of all available fields)
   - What's missing? (fields you need but don't have)

3. What verification/validation source do you prefer?
   Options for [SPORT]:
   - Official league API ([SPORT].com)
   - Sports Reference sites
   - Commercial API (SportsData.io, etc.)
   - Community API
   - Multiple sources (recommended)

4. What fields are most critical to verify?
   - Game scores (always)
   - Player stats
   - Team stats
   - Betting odds
   - Venue information
   - Other: ___________

5. Auto-fix or manual review?
   - Auto-fix everything (fast, risky)
   - Auto-fix low severity, review high/critical (recommended)
   - Manual review all (slow, safe)
```

### Step 2: Run Sub-Phases 1.1-1.6 for New Sport

Same process as NBA:
1. **Sub-Phase 1.1:** Analyze S3 coverage
2. **Sub-Phase 1.2:** Identify gaps
3. **Sub-Phase 1.3:** Upload local data (if applicable)
4. **Sub-Phase 1.4:** Run automated gap filling (adapt Workflow #38 for sport)
5. **Sub-Phase 1.5:** Establish quality baseline
6. **Sub-Phase 1.6:** Set up verification sources

### Step 3: Document Sport-Specific Findings

Create `docs/sports/{sport}/QUALITY_BASELINE.md` with:
- Total files count
- Date range coverage
- % valid vs empty files
- Quality metrics (completeness, accuracy, freshness, consistency)
- Known issues and gaps
- Verification source details

---

### Data Quality Metrics (Sport-Agnostic Formulas)

**Every sport implementation should track:**

#### 1. Completeness
```python
completeness = (records_with_data / total_expected_records) * 100

# Example (NBA):
# Expected: ~82 games/team/season * 30 teams = ~2,460 games/season
# Actual: Check S3 files vs expected

# Example (NFL):
# Expected: ~17 games/team/season * 32 teams / 2 = ~272 games/season
```

#### 2. Accuracy
```python
accuracy = (matching_records / verified_records) * 100

# Example (NBA):
# Verified 1,000 random games against NBA.com
# 987 scores matched exactly
# Accuracy = 98.7%
```

#### 3. Freshness
```python
freshness = days_since_last_update

# Example (NBA):
# Last game added: 2025-04-10
# Today: 2025-10-04
# Freshness: 177 days (offseason, acceptable)

# NBA offseason: acceptable if < 180 days
# NBA season: should be < 1 day
```

#### 4. Consistency
```python
consistency = (records_without_conflicts / total_records) * 100

# Example (NBA):
# Team name variations: "LA Lakers" vs "Los Angeles Lakers"
# Player name variations: "LeBron James" vs "Lebron James"
# Consistency check: canonical names exist for all
```

---

### Database Schema for Verification (Sport-Agnostic)

**These tables work for ANY sport:**

#### verification_runs
```sql
CREATE TABLE IF NOT EXISTS verification_runs (
    run_id SERIAL PRIMARY KEY,
    sport VARCHAR(20) NOT NULL,          -- 'NBA', 'NFL', 'MLB', etc.
    run_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    data_source VARCHAR(50) NOT NULL,    -- 'nba_stats', 'nfl_api', etc.
    records_checked INTEGER DEFAULT 0,
    discrepancies_found INTEGER DEFAULT 0,
    status VARCHAR(20) DEFAULT 'running',
    notes TEXT
);
```

#### {sport}_discrepancies
```sql
-- Example: game_discrepancies (NBA), match_discrepancies (Soccer)
CREATE TABLE IF NOT EXISTS game_discrepancies (
    discrepancy_id SERIAL PRIMARY KEY,
    run_id INTEGER REFERENCES verification_runs(run_id),
    record_id VARCHAR(50) NOT NULL,      -- game_id, match_id, etc.
    record_type VARCHAR(20),             -- 'game', 'player_stat', 'team', etc.

    field_name VARCHAR(50) NOT NULL,
    primary_value TEXT,                  -- Value from primary source
    verification_value TEXT,             -- Value from verification source

    discrepancy_type VARCHAR(30),        -- 'mismatch', 'missing_primary', 'missing_verification'
    severity VARCHAR(10),                -- 'low', 'medium', 'high', 'critical'

    resolved BOOLEAN DEFAULT FALSE,
    resolution_action VARCHAR(100),
    resolved_at TIMESTAMP,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

#### data_quality_metrics
```sql
CREATE TABLE IF NOT EXISTS data_quality_metrics (
    metric_id SERIAL PRIMARY KEY,
    sport VARCHAR(20) NOT NULL,
    run_id INTEGER REFERENCES verification_runs(run_id),

    metric_name VARCHAR(50) NOT NULL,    -- 'completeness', 'accuracy', 'consistency'
    metric_value DECIMAL(5,2),           -- 0.00 to 100.00
    metric_details JSONB,                -- Flexible JSON for sport-specific details

    total_records INTEGER,
    valid_records INTEGER,
    invalid_records INTEGER,

    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

## Cost Analysis

**Per sport costs (monthly):**

| Verification Source | Cost | Notes |
|---------------------|------|-------|
| **NBA.com Stats** | $0 | Free, rate-limited |
| **Basketball Reference** | $0 | Free, scraping |
| **SportsData.io** | $19/mo | Paid API |
| **balldontlie** | $0 | Free, community |

**AWS costs (this phase):**
- S3 GET requests: ~$0.01/month (checking files)
- Lambda (if automated): ~$0.20/month
- **Total: $0-20/month depending on verification source choice**

---

## Success Criteria

- [ ] Data coverage analyzed (file counts, date ranges)
- [ ] Coverage gaps identified and documented
- [ ] Automated gap-filling workflow tested (Workflow #38)
- [ ] Quality baseline metrics calculated:
  - [ ] Completeness > 95%
  - [ ] Freshness < 7 days (in-season)
  - [ ] Consistency checked (canonical IDs)
- [ ] Verification source chosen (optional for Phase 2-3)

---

## Next Steps

After completing this phase:
1. ✅ Data quality baseline established
2. ✅ Coverage gaps filled (S3 up to date)
3. ✅ Update PROGRESS.md status
4. → Proceed to [Phase 2: ETL Development](PHASE_2_AWS_GLUE.md)

**Phase 1 establishes the quality foundation for extraction and database loading.**

---

## Navigation

**Return to:** [PROGRESS.md](../../PROGRESS.md) | **Workflows:** [Workflow Index](../claude_workflows/CLAUDE_WORKFLOW_ORDER.md)

**Related phases:**
- Previous: [Phase 0: Data Collection & Initial Upload](PHASE_0_DATA_COLLECTION.md)
- Next: [Phase 2: ETL Development](PHASE_2_AWS_GLUE.md)
- Formerly: This was Phase 0 before ADR-008 reorganization

---

## Data Source Options Reference

**Use this table to choose verification sources for any sport:**

### Basketball (NBA)

| Source | Type | Cost | Coverage | Quality | Notes |
|--------|------|------|----------|---------|-------|
| **ESPN** | JSON/API | Free | 1999-present | ⭐⭐⭐⭐ | Primary (your current source) |
| **NBA.com Stats** | API | Free | 1996-present | ⭐⭐⭐⭐⭐ | Official, rate-limited |
| **Basketball Reference** | HTML | Free | 1946-present | ⭐⭐⭐⭐⭐ | Most comprehensive |
| **SportsData.io** | API | $19/mo | 1999-present | ⭐⭐⭐⭐ | Clean API |
| **balldontlie** | API | Free | 1979-present | ⭐⭐⭐ | Community |

### Football (NFL)

| Source | Type | Cost | Coverage | Quality | Notes |
|--------|------|------|----------|---------|-------|
| **ESPN** | JSON/API | Free | 2000-present | ⭐⭐⭐⭐ | Similar to NBA |
| **NFL.com** | API | Free | 2009-present | ⭐⭐⭐⭐⭐ | Official |
| **Pro Football Reference** | HTML | Free | 1920-present | ⭐⭐⭐⭐⭐ | Comprehensive |
| **SportsData.io** | API | $29/mo | 2000-present | ⭐⭐⭐⭐ | |

### Baseball (MLB)

| Source | Type | Cost | Coverage | Quality | Notes |
|--------|------|------|----------|---------|-------|
| **ESPN** | JSON/API | Free | 2000-present | ⭐⭐⭐⭐ | |
| **MLB Stats API** | API | Free | 2008-present | ⭐⭐⭐⭐⭐ | Official |
| **Baseball Reference** | HTML | Free | 1871-present | ⭐⭐⭐⭐⭐ | Historical gold mine |
| **SportsData.io** | API | $19/mo | 2000-present | ⭐⭐⭐⭐ | |

### Hockey (NHL)

| Source | Type | Cost | Coverage | Quality | Notes |
|--------|------|------|----------|---------|-------|
| **ESPN** | JSON/API | Free | 2000-present | ⭐⭐⭐⭐ | |
| **NHL.com API** | API | Free | 2010-present | ⭐⭐⭐⭐⭐ | Official |
| **Hockey Reference** | HTML | Free | 1917-present | ⭐⭐⭐⭐⭐ | |
| **SportsData.io** | API | $29/mo | 2000-present | ⭐⭐⭐⭐ | |

### Soccer (Multiple Leagues)

| Source | Type | Cost | Coverage | Quality | Notes |
|--------|------|------|----------|---------|-------|
| **ESPN** | JSON/API | Free | 2010-present | ⭐⭐⭐⭐ | Multiple leagues |
| **API-Football** | API | $0-30/mo | 2010-present | ⭐⭐⭐⭐⭐ | 900+ leagues |
| **TheSportsDB** | API | Free | 2000-present | ⭐⭐⭐ | Community |
| **FBref** | HTML | Free | 2010-present | ⭐⭐⭐⭐ | Advanced stats |

---

## Additional Resources

**Multi-sport data quality templates available for:**
- NFL (American Football) - ESPN scraper code exists at `/Users/ryanranft/0espn/espn/nfl/`
- MLB (Baseball)
- NHL (Hockey) - ESPN scraper code exists at `/Users/ryanranft/0espn/espn/nhl/`
- Soccer (Multiple Leagues)
- NCAAM, NCAAW, CFB, WNBA (ESPN scraper code exists)

---

*For Claude Code: See CLAUDE.md for navigation instructions and context management strategies.*

---

*Last updated: 2025-10-04 (reorganized per ADR-008)*
*Status: NBA pending verification source selection*