# NBA Data Inventory Management System (DIMS) Configuration
# Version: 1.0.0
# Created: 2025-10-21

version: "1.0.0"
system_name: "DIMS"
project_root: "/Users/ryanranft/nba-simulator-aws"

# Feature Toggles
features:
  database_backend: true         # ✅ PHASE 2: Enable PostgreSQL history
  event_driven: true            # ✅ PHASE 2: Enable event hooks
  live_mode: false              # Lazy evaluation (slow but fresh)
  approval_workflow: true       # ✅ PHASE 2: 3-tier verification
  smart_cache: true             # TTL-based caching
  jupyter_output: true          # ✅ PHASE 3: Interactive Jupyter notebook
  html_dashboard: false         # Generate HTML dashboard

# Cache Configuration
cache:
  enabled: true
  backend: "file"               # file | redis | memory
  default_ttl_hours: 24
  cache_dir: "inventory/cache"

  # Per-metric TTL overrides (in hours)
  ttl_overrides:
    s3_objects: 24              # Daily refresh
    test_files: 1               # Hourly refresh
    git_commits: 168            # Weekly refresh
    plus_minus_total: 24
    documentation_files: 24

# Verification Settings
verification:
  enabled: true
  schedule: "0 9 * * 1"         # Weekly Monday 9 AM

  thresholds:
    minor_drift_pct: 5          # Log only
    moderate_drift_pct: 15      # Notify
    major_drift_pct: 25         # Alert

  # Metric categorization for alerting
  stable_metrics:               # High-severity alerts if changed
    - git_commits
    - book_recommendations_total
    - plus_minus_total_lines

  volatile_metrics:             # Low-severity alerts
    - s3_objects
    - s3_size_gb
    - documentation_files
    - python_files

# Event Triggers (✅ PHASE 2: ENABLED, ✅ PHASE 3: WORKFLOW INTEGRATION)
events:
  enabled: true                                  # ✅ PHASE 2: Event-driven updates enabled
  hooks_dir: "inventory/events"
  cooldown_seconds: 60                          # Prevent spam (60 second cooldown)
  hooks:
    - name: "git_post_commit"
      trigger: "post-commit"
      metrics: ["code_base.*", "documentation.*", "git.*", "file_inventory.*"]

    - name: "s3_upload_complete"
      trigger: "s3_event"
      metrics: ["s3_storage.*", "sync_status.*"]

    - name: "scraper_complete"
      trigger: "scraper_complete"
      metrics: ["data_gaps.*", "sync_status.*", "aws_inventory.*"]
      description: "Triggered when scraper finishes - runs gap analysis and sync check"

    - name: "espn_scraper_complete"
      trigger: "scraper_complete"
      metrics: ["espn_data.*", "data_gaps.*", "sync_status.*"]
      description: "Triggered when ESPN scraper completes - updates ESPN metrics, gap detection, and sync status"

    - name: "monthly_audit"
      trigger: "cron"
      schedule: "0 0 1 * *"  # Monthly on 1st at midnight
      metrics: ["file_inventory.*", "local_data.*", "aws_inventory.*", "data_gaps.*", "sync_status.*"]
      description: "Monthly comprehensive audit of all workflow metrics"

    - name: "daily_sync_check"
      trigger: "cron"
      schedule: "0 9 * * *"  # Daily at 9 AM
      metrics: ["sync_status.*"]
      description: "Daily sync status verification"

# Database Configuration (✅ PHASE 2: ENABLED)
database:
  enabled: true                                  # ✅ PHASE 2: PostgreSQL backend enabled
  connection:
    host: "${DB_HOST}"
    port: 5432
    database: "nba_simulator"
    user: "${DB_USER}"
    password: "${DB_PASSWORD}"

  retention:
    history_days: 365           # Keep 1 year of history
    snapshots_daily: 90         # Keep 90 daily snapshots

# Approval Workflow (✅ PHASE 2: ENABLED, ✅ PHASE 3: WORKFLOW INTEGRATION)
approval:
  enabled: true                                  # ✅ PHASE 2: Approval workflow enabled
  critical_metrics:                              # Metrics requiring approval
    # Original critical metrics
    - "s3_storage.total_objects"
    - "s3_storage.total_size_gb"
    - "prediction_system.total_lines"
    - "plus_minus_system.total_lines"
    - "git.book_recommendation_commits"

    # Workflow integration critical metrics (Phase 3)
    - "aws_inventory.monthly_cost_estimate_usd"  # Cost changes need approval
    - "data_gaps.missing_games_count"            # Data gaps need review
    - "data_gaps.games_without_pbp"              # PBP gaps need review
    - "sync_status.local_s3_sync_status"         # Sync issues need approval
    - "local_data.archives_size_gb"              # Large storage changes

    # ESPN autonomous collection critical metrics (Phase 0.0001)
    - "espn_data.last_update_hours"              # ESPN data freshness monitoring
    - "espn_data.completeness_pct"               # ESPN data completeness
    - "espn_data.gap_days_count"                 # ESPN gap detection

  require_approval_threshold: 15                 # % drift requiring approval

# Output Configuration
outputs:
  markdown:
    enabled: true
    files:
      - path: "docs/MASTER_DATA_INVENTORY.md"
        template: "master_inventory"
      - path: "docs/DATA_COLLECTION_INVENTORY.md"
        template: "collection_inventory"

  json:
    enabled: true
    files:
      - path: "inventory/metrics.json"

  html:
    enabled: false
    file: "docs/inventory_dashboard.html"

  jupyter:
    enabled: true
    file: "notebooks/dims_explorer.ipynb"
    auto_execute: false           # Auto-execute on generation
    export_html: true            # Export to HTML after generation

# Logging Configuration
logging:
  enabled: true
  log_dir: "inventory/logs"
  log_level: "INFO"           # DEBUG | INFO | WARNING | ERROR
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Separate log files
  verification_log: "verification.log"
  update_log: "update.log"
  error_log: "error.log"

# Metrics Definitions
# Each metric has a command to calculate its current value
metrics:

  # S3 Storage Metrics
  s3_storage:
    total_objects:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); paginator = s3.get_paginator('list_objects_v2'); pages = paginator.paginate(Bucket='nba-sim-raw-data-lake'); print(sum(len(page.get('Contents', [])) for page in pages))\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Total number of objects in S3 bucket"

    total_size_gb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); paginator = s3.get_paginator('list_objects_v2'); pages = paginator.paginate(Bucket='nba-sim-raw-data-lake'); print(round(sum(obj['Size'] for page in pages for obj in page.get('Contents', [])) / 1024 / 1024 / 1024, 2))\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "Total size of S3 bucket in GB"

    hoopr_files:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_parquet/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "stable"
      description: "Number of hoopR parquet files in S3"

  # Prediction System Metrics
  prediction_system:
    total_lines:
      command: "wc -l scripts/ml/fetch_upcoming_games.py scripts/ml/fetch_recent_player_data.py scripts/ml/prepare_upcoming_game_features.py scripts/ml/predict_upcoming_games.py scripts/ml/train_model_for_predictions.py scripts/ml/demo_predictions.py scripts/ml/daily_predictions.sh 2>/dev/null | tail -1 | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"
      description: "Total lines of code in prediction system"

    fetch_upcoming_games_lines:
      command: "wc -l scripts/ml/fetch_upcoming_games.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    fetch_recent_player_data_lines:
      command: "wc -l scripts/ml/fetch_recent_player_data.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    prepare_upcoming_game_features_lines:
      command: "wc -l scripts/ml/prepare_upcoming_game_features.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    predict_upcoming_games_lines:
      command: "wc -l scripts/ml/predict_upcoming_games.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    train_model_for_predictions_lines:
      command: "wc -l scripts/ml/train_model_for_predictions.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    demo_predictions_lines:
      command: "wc -l scripts/ml/demo_predictions.py 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    daily_predictions_sh_lines:
      command: "wc -l scripts/ml/daily_predictions.sh 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

  # Plus/Minus System Metrics
  plus_minus_system:
    total_lines:
      command: "echo $(($(find scripts/pbp_to_boxscore -name '*plus_minus*.py' -exec cat {} \\; 2>/dev/null | wc -l) + $(find sql/plus_minus -name '*.sql' -exec cat {} \\; 2>/dev/null | wc -l) + $(wc -l docs/PLUS_MINUS_ML_INTEGRATION.md docs/PLUS_MINUS_IMPLEMENTATION_SUMMARY.md docs/PLUS_MINUS_OPTIMIZATION_SUMMARY.md docs/REC_11_PLUS_MINUS_COMPLETION_SUMMARY.md docs/PLUS_MINUS_RDS_DEPLOYMENT_SUCCESS.md docs/REC_11_PLUS_MINUS_INTEGRATION.md 2>/dev/null | tail -1 | awk '{print $1}')))"
      parse_type: "integer"
      category: "stable"
      description: "Total lines in Plus/Minus system (Python + SQL + Docs)"

    python_lines:
      command: "wc -l scripts/pbp_to_boxscore/plus_minus_calculator.py scripts/pbp_to_boxscore/populate_plus_minus_tables.py scripts/pbp_to_boxscore/demo_plus_minus_population.py 2>/dev/null | tail -1 | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

    sql_lines:
      command: "find sql/plus_minus -name '*.sql' -exec cat {} \\; 2>/dev/null | wc -l"
      parse_type: "integer"
      category: "stable"

    docs_lines:
      command: "wc -l docs/PLUS_MINUS_ML_INTEGRATION.md docs/PLUS_MINUS_IMPLEMENTATION_SUMMARY.md docs/PLUS_MINUS_OPTIMIZATION_SUMMARY.md docs/REC_11_PLUS_MINUS_COMPLETION_SUMMARY.md docs/PLUS_MINUS_RDS_DEPLOYMENT_SUCCESS.md docs/REC_11_PLUS_MINUS_INTEGRATION.md 2>/dev/null | tail -1 | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"

  # Code Base Metrics
  code_base:
    python_files:
      command: "find . -name '*.py' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total Python files in project"

    ml_scripts:
      command: "find scripts/ml -name '*.py' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"

    phase_2_scripts:
      command: "find scripts/pbp_to_boxscore -name '*.py' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"

    test_files:
      command: "find . -path '*/test*' -name '*.py' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total test files in project"

  # Documentation Metrics
  documentation:
    markdown_files:
      command: "find docs -name '*.md' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total markdown files in docs/"

    total_size_mb:
      command: "du -sm docs 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "volatile"

  # Git Metrics
  git:
    book_recommendation_commits:
      command: "git log --since='2025-10-01' --oneline --all 2>/dev/null | grep -i 'rec_' | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "stable"
      description: "Number of book recommendation commits"

  # Workflow Metrics
  workflows:
    total:
      command: "find docs/claude_workflows/workflow_descriptions -name '*.md' -type f 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "stable"
      description: "Total workflow description files"

  # SQL Schema Metrics
  sql_schemas:
    total_lines:
      command: "wc -l sql/master_schema.sql sql/phase9_box_score_snapshots.sql 2>/dev/null | tail -1 | awk '{print $1}'"
      parse_type: "integer"
      category: "stable"
      description: "Total lines in master SQL schemas"

  # Local Data Metrics (optional - can be slow)
  local_data:
    espn_size_gb:
      command: "du -sg /Users/ryanranft/0espn 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "volatile"
      description: "Size of local ESPN data in GB"
      enabled: false  # Disabled by default (slow operation)

    archives_size_gb:
      command: "du -sg ~/sports-simulator-archives/nba 2>/dev/null | awk '{print $1}'"
      parse_type: "integer"
      category: "volatile"
      description: "Size of NBA archives directory in GB"

    temp_data_size_gb:
      command: "[ -d ~/nba-sim-temp ] && du -sg ~/nba-sim-temp 2>/dev/null | awk '{print $1}' || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Size of temporary NBA data in GB"

  # File Inventory Metrics (Workflow #13)
  file_inventory:
    total_files_documented:
      command: "[ -f docs/FILE_INVENTORY.md ] && grep -c '^###' docs/FILE_INVENTORY.md 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "stable"
      description: "Total files documented in FILE_INVENTORY.md"

    last_inventory_age_days:
      command: "[ -f docs/FILE_INVENTORY.md ] && echo $(( ($(date +%s) - $(stat -f %m docs/FILE_INVENTORY.md)) / 86400 )) || echo 999"
      parse_type: "integer"
      category: "volatile"
      description: "Days since last file inventory update"

  # AWS Inventory Metrics (Workflow #47)
  aws_inventory:
    rds_database_size_gb:
      command: "PGPASSWORD=$RDS_PASSWORD psql -h $RDS_HOST -U $RDS_USERNAME -d $RDS_DATABASE -t -c \"SELECT ROUND(pg_database_size('nba_simulator')::numeric / 1024 / 1024 / 1024, 2)\" 2>/dev/null | tr -d ' ' || echo 0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "RDS database size in GB"

    rds_allocated_storage_gb:
      command: "aws rds describe-db-instances --db-instance-identifier nba-simulator --query 'DBInstances[0].AllocatedStorage' --output text 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "stable"
      description: "RDS allocated storage in GB"

    monthly_cost_estimate_usd:
      command: "python3 -c \"import boto3; s3_size_gb = round(sum(obj['Size'] for page in boto3.client('s3').get_paginator('list_objects_v2').paginate(Bucket='nba-sim-raw-data-lake') for obj in page.get('Contents', [])) / 1024 / 1024 / 1024, 2); rds_cost = 29; s3_cost = s3_size_gb * 0.023; print(f'{(s3_cost + rds_cost):.2f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "Estimated monthly AWS cost in USD"

  # Data Gap Analysis Metrics (Workflow #46)
  data_gaps:
    missing_games_count:
      command: "PGPASSWORD=$RDS_PASSWORD psql -h $RDS_HOST -U $RDS_USERNAME -d $RDS_DATABASE -t -c \"SELECT COUNT(*) FROM (SELECT DISTINCT game_id FROM games EXCEPT SELECT DISTINCT game_id FROM box_score_players) AS gaps\" 2>/dev/null | tr -d ' ' || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of games missing box score player data"

    games_without_pbp:
      command: "PGPASSWORD=$RDS_PASSWORD psql -h $RDS_HOST -U $RDS_USERNAME -d $RDS_DATABASE -t -c \"SELECT COUNT(*) FROM (SELECT DISTINCT game_id FROM games EXCEPT SELECT DISTINCT game_id::varchar FROM hoopr_play_by_play) AS gaps\" 2>/dev/null | tr -d ' ' || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of games without play-by-play data"

  # Sync Status Metrics (Workflow #49)
  sync_status:
    local_s3_sync_status:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/ --max-items 1 >/dev/null 2>&1 && echo 'accessible' || echo 'unavailable'"
      parse_type: "string"
      category: "volatile"
      description: "S3 bucket accessibility status (accessible/unavailable)"

  # ESPN Data Metrics (Phase 0.0001 - Autonomous Data Collection)
  espn_data:
    # File Count Metrics
    pbp_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_play_by_play/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total ESPN play-by-play files in S3"

    boxscore_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_box_scores/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total ESPN box score files in S3"

    schedule_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_schedules/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total ESPN schedule files in S3"

    team_stats_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_team_stats/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total ESPN team stats files in S3"

    # Data Freshness Metrics
    last_update_hours:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_play_by_play/ --recursive 2>/dev/null | tail -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 3600)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Hours since last ESPN data upload"

    oldest_data_days:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/espn_play_by_play/ --recursive 2>/dev/null | head -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 86400)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Days since oldest ESPN data file"

    # Data Size Metrics
    total_size_gb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); folders = ['espn_play_by_play/', 'espn_box_scores/', 'espn_schedules/', 'espn_team_stats/']; size = 0; [size := size + sum(obj.get('Size', 0) for obj in s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix=folder).get('Contents', [])) for folder in folders]; print(f'{size / 1024 / 1024 / 1024:.2f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "Total ESPN data size across all folders (GB)"

    avg_file_size_kb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='espn_play_by_play/').get('Contents', []); sizes = [obj['Size'] for obj in objs]; print(f'{(sum(sizes) / len(sizes) / 1024):.1f}' if sizes else '0.0')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "Average ESPN play-by-play file size (KB)"

    # Data Quality Metrics
    small_files_count:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='espn_play_by_play/').get('Contents', []); print(sum(1 for obj in objs if obj.get('Size', 0) < 1024))\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of ESPN files smaller than 1KB (potentially incomplete)"

    completeness_pct:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); schedules = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='espn_schedules/').get('Contents', [])); pbp = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='espn_play_by_play/').get('Contents', [])); pct = (pbp / schedules * 100) if schedules > 0 else 0; print(f'{pct:.1f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "ESPN data completeness percentage (pbp files / schedules * 100)"

    # Gap Detection Metrics
    gap_days_count:
      command: "python3 -c \"import boto3; from datetime import datetime, timedelta; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='espn_schedules/').get('Contents', []); dates = sorted([datetime.strptime(obj['Key'].split('/')[-1].replace('.json', ''), '%Y%m%d') for obj in objs if obj['Key'].endswith('.json')]); gaps = sum(1 for i in range(len(dates)-1) if (dates[i+1] - dates[i]).days > 1); print(gaps)\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of gaps (missing days) in ESPN schedule data"

  # hoopR Data Metrics (3-source cross-validation)
  hoopr_data:
    # File Count Metrics
    pbp_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_parquet/ --recursive 2>/dev/null | grep -c '.parquet' || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Total hoopR play-by-play parquet files in S3"

    phase1_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_phase1/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total hoopR Phase 1 CSV files in S3"

    schedule_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_schedules/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total hoopR schedule files in S3"

    team_stats_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_team_stats/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total hoopR team stats files in S3"

    # Data Freshness Metrics
    last_update_hours:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_parquet/ --recursive 2>/dev/null | tail -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 3600)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Hours since last hoopR data upload"

    oldest_data_days:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/hoopr_parquet/ --recursive 2>/dev/null | head -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 86400)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Days since oldest hoopR data file"

    # Data Size Metrics
    total_size_gb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); folders = ['hoopr_parquet/', 'hoopr_phase1/', 'hoopr_schedules/', 'hoopr_team_stats/']; size = 0; [size := size + sum(obj.get('Size', 0) for obj in s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix=folder).get('Contents', [])) for folder in folders]; print(f'{size / 1024 / 1024 / 1024:.2f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "Total hoopR data size across all folders (GB)"

    avg_file_size_mb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='hoopr_parquet/').get('Contents', []); sizes = [obj['Size'] for obj in objs]; print(f'{(sum(sizes) / len(sizes) / 1024 / 1024):.1f}' if sizes else '0.0')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "Average hoopR parquet file size (MB)"

    # Data Quality Metrics
    small_files_count:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='hoopr_parquet/').get('Contents', []); print(sum(1 for obj in objs if obj.get('Size', 0) < 10240))\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of hoopR files smaller than 10KB (potentially incomplete)"

    completeness_pct:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); schedules = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='hoopr_schedules/').get('Contents', [])); pbp = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='hoopr_parquet/').get('Contents', [])); pct = (pbp / schedules * 100) if schedules > 0 else 0; print(f'{pct:.1f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "hoopR data completeness percentage (pbp files / schedules * 100)"

    # Gap Detection Metrics
    gap_days_count:
      command: "python3 -c \"import boto3; from datetime import datetime, timedelta; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='hoopr_schedules/').get('Contents', []); dates = sorted([datetime.strptime(obj['Key'].split('/')[-1].replace('.json', '').replace('.csv', ''), '%Y%m%d') for obj in objs if obj['Key'].endswith(('.json', '.csv'))]); gaps = sum(1 for i in range(len(dates)-1) if (dates[i+1] - dates[i]).days > 1); print(gaps)\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of gaps (missing days) in hoopR schedule data"

  # Basketball Reference Data Metrics (historical validation)
  basketball_reference_data:
    # File Count Metrics
    pbp_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/pbp/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total Basketball Reference play-by-play files in S3"

    boxscore_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/box_scores/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total Basketball Reference box score files in S3"

    schedule_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/schedules/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total Basketball Reference schedule files in S3"

    team_stats_file_count:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/team_stats/ --recursive 2>/dev/null | wc -l | tr -d ' '"
      parse_type: "integer"
      category: "volatile"
      description: "Total Basketball Reference team stats files in S3"

    # Data Freshness Metrics
    last_update_hours:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/ --recursive 2>/dev/null | tail -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 3600)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Hours since last Basketball Reference data upload"

    oldest_data_days:
      command: "aws s3 ls s3://nba-sim-raw-data-lake/basketball_reference/ --recursive 2>/dev/null | head -1 | awk '{cmd=\"date +%s\"; cmd | getline now; close(cmd); cmd=\"date -d \\\"\"$1\" \"$2\"\\\" +%s 2>/dev/null || date -j -f \\\"%Y-%m-%d %H:%M:%S\\\" \\\"\"$1\" \"$2\"\\\" +%s\"; cmd | getline then; close(cmd); print int((now - then) / 86400)}'"
      parse_type: "integer"
      category: "volatile"
      description: "Days since oldest Basketball Reference data file"

    # Data Size Metrics
    total_size_gb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); size = sum(obj.get('Size', 0) for obj in s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/').get('Contents', [])); print(f'{size / 1024 / 1024 / 1024:.2f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 2
      category: "volatile"
      description: "Total Basketball Reference data size (GB)"

    avg_file_size_kb:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/pbp/').get('Contents', []); sizes = [obj['Size'] for obj in objs]; print(f'{(sum(sizes) / len(sizes) / 1024):.1f}' if sizes else '0.0')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "Average Basketball Reference file size (KB)"

    # Data Quality Metrics
    small_files_count:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/').get('Contents', []); print(sum(1 for obj in objs if obj.get('Size', 0) < 1024))\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of Basketball Reference files smaller than 1KB (potentially incomplete)"

    completeness_pct:
      command: "python3 -c \"import boto3; s3 = boto3.client('s3'); schedules = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/schedules/').get('Contents', [])); pbp = len(s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/pbp/').get('Contents', [])); pct = (pbp / schedules * 100) if schedules > 0 else 0; print(f'{pct:.1f}')\" 2>/dev/null || echo 0.0"
      parse_type: "float"
      precision: 1
      category: "volatile"
      description: "Basketball Reference data completeness percentage (pbp files / schedules * 100)"

    # Gap Detection Metrics
    gap_days_count:
      command: "python3 -c \"import boto3; from datetime import datetime, timedelta; s3 = boto3.client('s3'); objs = s3.list_objects_v2(Bucket='nba-sim-raw-data-lake', Prefix='basketball_reference/schedules/').get('Contents', []); dates = sorted([datetime.strptime(obj['Key'].split('/')[-1].split('_')[0], '%Y%m%d') for obj in objs if obj['Key'].endswith('.html')]); gaps = sum(1 for i in range(len(dates)-1) if (dates[i+1] - dates[i]).days > 1); print(gaps)\" 2>/dev/null || echo 0"
      parse_type: "integer"
      category: "volatile"
      description: "Number of gaps (missing days) in Basketball Reference schedule data"
